REGULATION
(
EU
)
OF
THE
EUROPEAN
PARLIAMENT
AND
OF
THE
COUNCIL
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intelligence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
THE
EUROPEAN
PARLIAMENT
AND
THE
COUNCIL
OF
THE
EUROPEAN
UNION
,
and
in
particular
Articles
16
and
114
thereof
,
Whereas
:
(
1
)
The
purpose
of
this
Regulation
is
to
imp
rove
the
functioning
of
the
internal
market
by
laying
down
a
uniform
legal
framework
in
particular
for
the
development
,
the
placing
on
the
market
,
the
putting
into
service
and
the
use
of
artificial
intelligence
systems
(
AI
systems
)
in
the
Union
,
in
accordance
with
Union
values
,
to
promote
the
uptak
e
of
human
centr
ic
and
trustworthy
artificial
intellig
ence
(
AI
)
while
ensur
ing
a
high
level
of
protection
of
health
,
safety
,
fundamental
rights
as
enshrined
in
the
Charter
of
Fundamental
Rights
of
the
European
Union
(
the
‘
Charter
’
)
,
including
democracy
,
the
rule
of
law
and
environmental
protection
,
to
protect
against
the
harmful
effects
of
AI
systems
in
the
Union
,
and
to
support
innovation
.
This
Regulation
ensures
the
free
moveme
nt
,
cross
-
border
,
of
AI
-
based
goods
and
services
,
thus
preventing
Member
States
from
imp
osing
restrictions
on
the
development
,
marketing
and
use
of
AI
systems
,
unless
explicitly
authorised
by
this
Regulation
.
(
2
)
This
Regulation
should
be
applied
in
accordance
with
the
values
of
the
Union
enshrined
as
in
the
Charter
,
facilitating
the
protection
of
natural
persons
,
undertakings
,
democracy
,
the
rule
of
law
and
environmental
protection
,
while
boosting
innovation
and
employment
and
making
the
Union
a
leader
in
the
uptake
of
trustwo
rthy
AI
.
(
3
)
AI
systems
can
be
easily
deployed
in
a
large
variety
of
sectors
of
the
economy
and
many
parts
of
society
,
including
across
borders
,
and
can
easily
circulate
throughout
the
Union
.
Certain
Member
States
have
already
explored
the
adoption
of
national
rules
to
ensure
that
AI
is
trustworthy
and
safe
and
is
developed
and
used
in
accordance
with
fundamental
rights
obligations
.
Diverging
national
rules
may
lead
to
the
fragmentation
of
the
inter
nal
market
and
may
decrease
legal
certainty
for
operators
that
develop
,
import
or
use
AI
systems
.
A
consistent
and
high
level
of
protection
throughout
the
Union
should
theref
ore
be
ensured
in
order
to
achi
eve
trustw
orthy
AI
,
while
divergence
s
ham
pering
the
free
circulation
,
innovation
,
deplo
yment
and
the
uptak
e
of
AI
systems
and
related
products
and
services
within
the
inter
nal
market
should
be
prevent
ed
by
laying
down
uniform
obligations
for
operators
and
Offi
cial
Jour
nal
(
1
)
12.2021
,
p.
56
.
(
2
)
3.2022
,
p.
5
.
(
3
)
2.2022
,
p.
60
.
(
4
)
guaranteeing
the
uniform
protect
ion
of
overriding
reasons
of
public
interest
and
of
rights
of
persons
throughout
the
internal
market
on
the
basis
of
Article
114
of
the
Treaty
on
the
Functioning
of
the
European
Union
(
TFEU
)
.
To
the
extent
that
this
Regulation
contains
specific
rules
on
the
protect
ion
of
individuals
with
regard
to
the
processing
of
personal
data
concerning
restrictions
of
the
use
of
AI
systems
for
remote
biometric
identification
for
the
purpose
of
law
enforcement
,
of
the
use
of
AI
systems
for
risk
assessments
of
natural
persons
for
the
purpose
of
law
enforcement
and
of
the
use
of
AI
systems
of
biometric
categorisation
for
the
purpose
of
law
enforcement
,
it
is
appropriate
to
base
this
Regulation
,
in
so
far
as
those
specific
rules
are
concer
ned
,
on
Article
16
TFEU
.
In
light
of
those
specific
rules
and
the
recourse
to
Article
16
TFEU
,
it
is
appropriate
to
consult
the
European
Data
Protection
Board
.
(
4
)
AI
is
a
fast
evolving
family
of
technologies
that
contribut
es
to
a
wide
array
of
economic
,
environmental
and
societal
benefits
across
the
entire
spectrum
of
industr
ies
and
social
activities
.
By
improving
prediction
,
optimising
operations
and
resource
allocation
,
and
personalising
digital
solutions
available
for
individuals
and
organisations
,
the
use
of
AI
can
provide
key
competitive
advantages
to
undertakings
and
support
socially
and
environmentally
beneficial
outcomes
,
for
example
in
healthcare
,
agriculture
,
food
safety
,
education
and
training
,
media
,
spor
ts
,
culture
,
infrastructure
management
,
energy
,
transport
and
logistics
,
public
services
,
security
,
justice
,
resource
and
energy
efficiency
,
environmental
monitoring
,
the
conser
vation
and
restoration
of
biodiversity
and
ecosystems
and
climate
change
mitigation
and
adaptation
.
(
5
)
At
the
same
time
,
depending
on
the
circumstances
regarding
its
specific
application
,
use
,
and
level
of
technological
development
,
AI
may
generate
risks
and
cause
harm
to
public
interests
and
fundamental
rights
that
are
protected
by
Union
law
.
Such
harm
might
be
material
or
immaterial
,
including
physical
,
psychological
,
societal
or
economic
harm
.
(
6
)
Given
the
major
impact
that
AI
can
have
on
society
and
the
need
to
build
trust
,
it
is
vital
for
AI
and
its
regulator
y
framework
to
be
developed
in
accordance
with
Union
values
as
enshrined
in
Article
2
of
the
Treaty
on
European
Union
(
TEU
)
,
the
fundamental
rights
and
freedoms
enshrined
in
the
Treaties
and
,
pursuant
to
Article
6
TEU
,
the
Charter
.
As
a
prerequisite
,
AI
should
be
a
human
-
centr
ic
technology
.
It
should
serve
as
a
tool
for
people
,
with
the
ultimate
aim
of
increasing
human
well
-
being
.
(
7
)
In
order
to
ensure
a
consistent
and
high
level
of
protection
of
public
intere
sts
as
regard
s
health
,
safety
and
fundamental
rights
,
common
rules
for
higher
isk
AI
systems
should
be
established
.
Those
rules
should
be
consiste
nt
with
the
Charter
,
non
-
discr
iminat
ory
and
in
line
with
the
Union
’s
international
trade
commitments
.
They
should
also
take
into
account
the
European
Declaration
on
Digital
Rights
and
Principles
for
the
Digital
Decade
and
the
Ethics
guidelines
for
trustw
orthy
AI
of
the
High
-
Level
Exper
t
Group
on
Artificial
Intellig
ence
(
AI
HLEG
)
.
(
8)
A
Union
lega
l
framework
laying
down
harmonised
rules
on
AI
is
theref
ore
needed
to
foster
the
development
,
use
and
uptake
of
AI
in
the
inter
nal
market
that
at
the
same
time
meets
a
high
level
of
protect
ion
of
public
interests
,
such
as
health
and
safety
and
the
protection
of
fundamental
rights
,
including
democracy
,
the
rule
of
law
and
environmental
protect
ion
as
recognised
and
protected
by
Union
law
.
To
achieve
that
objective
,
rules
regulating
the
placing
on
the
market
,
the
putting
into
service
and
the
use
of
certain
AI
systems
should
be
laid
down
,
thus
ensur
ing
the
smooth
functioning
of
the
internal
market
and
allowi
ng
those
systems
to
benefit
from
the
principle
of
free
movement
of
goods
and
services
.
Those
rules
should
be
clear
and
robust
in
protecting
fundamental
rights
,
supportive
of
new
innovative
solutions
,
enabling
a
European
ecosyste
m
of
public
and
private
actor
s
creating
AI
systems
in
line
with
Union
values
and
unlocking
the
potential
of
the
digital
transf
ormation
across
all
regions
of
the
Union
.
By
laying
down
those
rules
as
well
as
measures
in
support
of
innovation
with
a
particular
focus
on
small
and
medium
enterprises
(
SMEs
)
,
including
startups
,
this
Regulation
supports
the
objective
of
promoting
the
European
human
-
centr
ic
approach
to
AI
and
being
a
global
leader
in
the
development
of
secure
,
trustworthy
and
ethical
AI
as
stated
by
the
European
Council
,
and
it
ensures
the
protection
of
ethical
principles
,
as
specific
ally
request
ed
by
the
European
Parliament
.
European
Council
,
Special
meeting
of
the
European
Council
(
1
and
2
October
2020
)
—
Conclusions
,
EUC
O
,
2020
,
p.
6
.
(
6
)
European
Parliament
resolution
of
20
October
2020
with
recommendations
to
the
Commission
on
a
framework
of
ethical
aspects
of
artificia
l
intellig
ence
,
robotics
and
relat
ed
technologies
,
(
INL).(9
)
Harmonised
rules
applicable
to
the
placing
on
the
mark
et
,
the
putting
into
service
and
the
use
of
higher
isk
AI
systems
should
be
laid
down
consistently
with
Regulation
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
,
Decision
No
/EC
of
the
European
Parliament
and
of
the
Council
and
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
(
New
Legislative
Framewo
rk
)
.
The
harmonised
rules
laid
down
in
this
Regulation
should
apply
across
sectors
and
,
in
line
with
the
New
Legislative
Framework
,
should
be
without
prejudice
to
existing
Union
law
,
in
particular
on
data
protect
ion
,
consumer
protection
,
fundamental
rights
,
employment
,
and
protection
of
work
ers
,
and
product
safety
,
to
which
this
Regulation
is
comp
lementar
y.
As
a
consequence
,
all
rights
and
remedies
provid
ed
for
by
such
Union
law
to
consumers
,
and
other
persons
on
whom
AI
systems
may
have
a
nega
tive
impact
,
including
as
rega
rds
the
compensation
of
possible
damages
pursuant
to
Council
Directive
/EEC
remain
unaffected
and
fully
applicable
.
Further
more
,
in
the
context
of
employment
and
protect
ion
of
workers
,
this
Regulation
should
theref
ore
not
affect
Union
law
on
social
policy
and
national
labour
law
,
in
compliance
with
Union
law
,
concerning
employment
and
working
conditions
,
including
health
and
safety
at
work
and
the
relationship
between
employers
and
workers
.
This
Regulation
should
also
not
affect
the
exercise
of
fundamental
rights
as
recognised
in
the
Member
States
and
at
Union
level
,
including
the
right
or
freedom
to
strike
or
to
take
other
action
covered
by
the
specific
industr
ial
relations
systems
in
Member
States
as
well
as
the
right
to
negotiate
,
to
conclude
and
enforce
collective
agreements
or
to
take
collective
action
in
accordance
with
national
law
.
This
Regulation
should
not
affect
the
provisions
aiming
to
improve
working
conditions
in
platform
work
laid
down
in
a
Directive
of
the
European
Parliament
and
of
the
Council
on
improving
working
conditions
in
platform
work
.
Moreover
,
this
Regulation
aims
to
strengthen
the
effectiveness
of
such
existing
rights
and
remedies
by
establishing
specific
requirements
and
obligations
,
including
in
respect
of
the
transparency
,
technical
documentation
and
record
-
keepi
ng
of
AI
systems
.
Further
more
,
the
obligations
placed
on
various
operators
involved
in
the
AI
value
chain
under
this
Regulation
should
apply
without
prejudice
to
national
law
,
in
compliance
with
Union
law
,
having
the
effect
of
limiting
the
use
of
certain
AI
systems
where
such
law
falls
outside
the
scope
of
this
Regulation
or
pursues
legitimate
public
interest
objectives
other
than
those
pursued
by
this
Regulation
.
For
example
,
national
labour
law
and
law
on
the
protection
of
minors
,
namely
persons
below
the
age
of
environment
,
insofa
r
as
they
are
not
specific
to
AI
systems
and
pursue
other
legitimate
public
interest
objectives
,
should
not
be
affected
by
this
Regulation
.
(
10
)
The
fundamental
right
to
the
protection
of
personal
data
is
safeguarded
in
particular
by
Regulations
(
EU
)
and
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
.
Directive
/EC
of
the
European
Parliament
and
of
the
Council
additionally
prot
ects
private
life
and
the
confi
dentiality
of
communications
,
including
by
way
of
providing
conditions
for
any
storing
of
personal
and
non
-
personal
data
in
,
and
access
from
,
terminal
equipment
.
Those
Union
lega
l
acts
provide
the
basis
for
sustainable
and
responsible
data
processing
,
including
where
data
sets
include
a
mix
of
personal
and
non
-
personal
data
.
This
Regulation
does
not
seek
to
affect
the
application
of
existing
Union
law
governing
the
processing
of
personal
data
,
including
the
tasks
and
powers
of
the
independent
supervisory
authorities
competent
to
monitor
compliance
with
those
instr
uments
.
It
also
does
not
affect
the
obligations
of
providers
and
deplo
yers
of
AI
systems
in
their
role
as
data
controllers
or
processors
stemming
from
Union
or
national
law
on
the
protection
of
personal
data
in
so
far
as
the
design
,
the
development
or
the
use
of
AI
systems
involves
the
processing
of
personal
data
.
It
is
also
appropriate
to
clarify
that
data
subjects
continue
to
enjo
y
all
the
(
7
)
Regulation
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
of
9
July
2008
setting
out
the
requirements
for
accreditation
and
repealing
Regulation
(
EEC
)
No
(
8.2008
,
p.
30
)
.
(
8)
Decision
No
/EC
of
the
European
Parliament
and
of
the
Council
of
9
July
2008
on
a
common
framework
for
the
marketing
of
products
,
and
repealing
Council
Decision
/EEC
(
8.2008
,
p.
82
)
.
(
9
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
20
June
2019
on
market
surveillance
and
compliance
of
products
and
amending
Directive
/EC
and
Regulations
(
EC
)
No
and
(
EU
)
No
(
6.2019
,
p.
1
)
.
(
10
)
Council
Directive
/EEC
of
25
July
1985
on
the
appro
ximation
of
the
laws
,
regulations
and
administrative
provisions
of
the
Member
States
concerning
liability
for
defe
ctive
products
(
8.1985
,
p.
29
)
.
(
11
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
27
Apr
il
2016
on
the
protection
of
natural
persons
with
regard
to
the
processing
of
personal
data
and
on
the
free
movement
of
such
data
,
and
repealing
Directive
/EC
(
General
Data
Protection
Regulation
)
(
5.2016
,
p.
1
)
.
(
12
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
23
October
2018
on
the
protection
of
natural
persons
with
regard
to
the
processing
of
personal
data
by
the
Union
institutions
,
bodies
,
offices
and
agencies
and
on
the
free
movement
of
such
data
,
and
repealing
Regulation
(
EC
)
No
and
Decision
No
/EC
(
11.2018
,
p.
39
)
.
(
13
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
27
Apr
il
2016
on
the
protection
of
natural
persons
with
regard
to
the
processing
of
personal
data
by
competent
authorities
for
the
purposes
of
the
prevention
,
investig
ation
,
detection
or
prosecution
of
criminal
offences
or
the
execution
of
criminal
penalties
,
and
on
the
free
movement
of
such
data
,
and
repealing
Council
Framework
Decision
/JHA
(
5.2016
,
p.
89
)
.
(
14
)
Directive
/EC
of
the
European
Parliament
and
of
the
Council
of
12
July
2002
concerning
the
processing
of
personal
data
and
the
protection
of
privacy
in
the
electronic
communications
sector
(
Directive
on
privacy
and
electronic
communications
)
(
7.2002
,
p.
37).rights
and
guarantees
awarded
to
them
by
such
Union
law
,
including
the
rights
related
to
solely
automat
ed
individual
decision
-
making
,
including
prof
iling
.
Harmonised
rules
for
the
placing
on
the
market
,
the
putting
into
service
and
the
use
of
AI
systems
established
under
this
Regulation
should
facilitate
the
effective
imp
lementation
and
enable
the
exercise
of
the
data
subjects
’
rights
and
other
remedies
guaranteed
under
Union
law
on
the
protect
ion
of
personal
data
and
of
other
fundamental
rights
.
(
11
)
This
Regulation
should
be
without
prejudice
to
the
provisions
rega
rding
the
liability
of
provider
s
of
intermediar
y
services
as
set
out
in
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
.
(
12
)
The
notion
of
‘
AI
system
’
in
this
Regulation
should
be
clearly
defined
and
should
be
closely
aligned
with
the
work
of
international
organisations
working
on
AI
to
ensure
legal
certainty
,
facilitate
inter
national
conve
rgence
and
wide
accep
tance
,
while
providing
the
flexibility
to
accommodate
the
rapid
technological
developments
in
this
field
.
Moreo
ver
,
the
definition
should
be
based
on
key
char
acter
istics
of
AI
systems
that
distinguish
it
from
simpler
traditional
software
systems
or
programming
approac
he
s
and
should
not
cover
systems
that
are
based
on
the
rules
defined
solely
by
natural
persons
to
auto
matically
execute
operations
.
A
key
character
istic
of
AI
systems
is
their
capability
to
infer
.
This
capability
to
infer
refers
to
the
process
of
obtaining
the
outputs
,
such
as
predictions
,
cont
ent
,
recommendations
,
or
decisions
,
which
can
influence
physica
l
and
virtual
environments
,
and
to
a
capability
of
AI
systems
to
derive
models
or
algor
ithms
,
or
both
,
from
inputs
or
data
.
The
techniques
that
enable
inference
while
building
an
AI
syste
m
include
machi
ne
learning
approaches
that
learn
from
data
how
to
achieve
certain
objectives
,
and
logic-
and
kno
wledge
-
based
approac
he
s
that
infer
from
encoded
knowle
dge
or
symbolic
representation
of
the
task
to
be
solved
.
The
capacity
of
an
AI
syste
m
to
infer
transcends
basic
data
processing
by
enabling
learning
,
reasoning
or
modelling
.
The
term
‘
mach
ine
-
based
’
refers
to
the
fact
that
AI
systems
run
on
machi
nes
.
The
reference
to
explicit
or
implicit
objectives
underscores
that
AI
systems
can
operat
e
according
to
explicit
defined
objectives
or
to
imp
licit
objectives
.
The
objectives
of
the
AI
system
may
be
different
from
the
intended
purpose
of
the
AI
system
in
a
specific
cont
ext
.
For
the
purposes
of
this
Regulation
,
environments
should
be
understood
to
be
the
contexts
in
which
the
AI
systems
operate
,
whereas
outputs
generated
by
the
AI
syste
m
reflect
diffe
rent
functions
perfo
rmed
by
AI
systems
and
include
predictions
,
cont
ent
,
recommendations
or
decisions
.
AI
systems
are
designed
to
operate
with
varying
levels
of
autonom
y
,
meaning
that
they
have
some
degree
of
independence
of
actions
from
human
involvement
and
of
capabilities
to
operat
e
without
human
intervention
.
The
adap
tiveness
that
an
AI
syste
m
could
exhibit
after
deplo
yment
,
refers
to
self
-
lear
ning
capabilities
,
allowi
ng
the
syste
m
to
change
while
in
use
.
AI
systems
can
be
used
on
a
stand
-
alone
basis
or
as
a
compo
nent
of
a
product
,
irrespective
of
whether
the
syste
m
is
phys
ically
integrated
into
the
product
(
embedded
)
or
serves
the
functionality
of
the
product
without
being
integrat
ed
therein
(
non
-
embedded
)
.
(
13
)
The
notion
of
‘
deplo
yer
’
referred
to
in
this
Regulation
should
be
interpreted
as
any
natural
or
lega
l
person
,
including
a
public
author
ity
,
agency
or
other
body
,
using
an
AI
system
under
its
author
ity
,
excep
t
where
the
AI
syste
m
is
used
in
the
course
of
a
personal
non
-
profess
ional
activity
.
Depending
on
the
type
of
AI
syste
m
,
the
use
of
the
system
may
affect
persons
other
than
the
deplo
yer
.
(
14
)
The
notion
of
‘
biometric
data
’
used
in
this
Regulation
should
be
inter
preted
in
light
of
the
notion
of
biometric
data
as
defined
in
Article
4
,
point
of
Regulation
(
EU
)
,
Article
3
,
point
of
Regulation
(
EU
)
and
Article
3
,
point
of
Directive
(
EU
)
.
biometric
data
can
allow
for
the
authentication
,
identification
or
categorisation
of
natural
persons
and
for
the
recognition
of
emotions
of
natural
persons
.
(
15
)
The
notion
of
‘
biometric
identifi
cation
’
referred
to
in
this
Regulation
should
be
defined
as
the
auto
mated
recognition
of
physical
,
physiological
and
behavi
oural
human
features
such
as
the
face
,
eye
movement
,
body
shape
,
voice
,
prosody
,
gait
,
posture
,
hear
t
rate
,
blood
pressure
,
odour
,
keystrok
es
character
istics
,
for
the
purpose
of
establishing
an
individual
’s
identity
by
compari
ng
biometric
data
of
that
individual
to
stored
biometric
data
of
individuals
in
a
refere
nce
database
,
irrespective
of
whether
the
individual
has
given
its
consent
or
not
.
This
excludes
AI
systems
intended
to
be
used
for
biometric
verificati
on
,
which
includes
authentication
,
whose
sole
purpose
is
to
confi
rm
that
a
specific
natural
person
is
the
person
he
or
she
claims
to
be
and
to
confi
rm
the
identity
of
a
natural
person
for
the
sole
purpose
of
having
access
to
a
service
,
unlocking
a
device
or
having
security
access
to
premises
.
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
19
October
2022
on
a
Sing
le
Market
For
Digital
Services
and
amending
Directive
/EC
(
Digital
Services
Act
)
(
10.2022
,
p.
1).(16
)
The
notion
of
‘
biometric
catego
risation
’
refer
red
to
in
this
Regulation
should
be
defined
as
assigning
natural
persons
to
specific
cate
gories
on
the
basis
of
their
biometric
data
.
Such
specific
catego
ries
can
relate
to
aspects
such
as
sex
,
age
,
hair
colour
,
eye
colour
,
tattoos
,
behavio
ural
or
personality
traits
,
languag
e
,
religion
,
membership
of
a
national
minor
ity
,
sexual
or
political
orientation
.
This
does
not
include
biometric
catego
risation
systems
that
are
a
purely
ancillar
y
feature
intrinsically
linked
to
another
commercial
service
,
meaning
that
the
feature
can
not
,
for
objective
technical
reasons
,
be
used
without
the
principal
service
,
and
the
integration
of
that
feature
or
functionality
is
not
a
means
to
circum
vent
the
applicability
of
the
rules
of
this
Regulation
.
For
example
,
filters
cate
gorising
facial
or
body
features
used
on
online
marketplaces
could
constitute
such
an
ancillar
y
feature
as
they
can
be
used
only
in
relation
to
the
principal
service
which
consists
in
selling
a
product
by
allowing
the
consumer
to
preview
the
displa
y
of
the
product
on
him
or
herself
and
help
the
consumer
to
mak
e
a
purcha
se
decision
.
Filte
rs
used
on
online
social
netw
ork
services
which
cate
gorise
facial
or
body
features
to
allow
users
to
add
or
modify
pictures
or
videos
could
also
be
considered
to
be
ancillar
y
feature
as
such
filter
can
not
be
used
without
the
principal
service
of
the
social
netw
ork
services
consisting
in
the
shar
ing
of
cont
ent
online
.
(
17
)
The
notion
of
‘
remot
e
biometric
identifica
tion
syste
m
’
referred
to
in
this
Regulation
should
be
defined
functionally
,
as
an
AI
syste
m
intended
for
the
identifica
tion
of
natural
persons
without
their
active
involvement
,
typically
at
a
distance
,
through
the
comp
arison
of
a
person
’s
biometric
data
with
the
biometric
data
contained
in
a
reference
database
,
irrespectively
of
the
particular
technology
,
processes
or
types
of
biometric
data
used
.
Such
remote
biometric
identifica
tion
systems
are
typically
used
to
perceive
multiple
persons
or
their
behavio
ur
simultaneously
in
order
to
facilitate
signif
icantly
the
identification
of
natural
persons
without
their
active
involvement
.
This
excludes
AI
systems
intended
to
be
used
for
biometric
verification
,
which
includes
authentication
,
the
sole
purpose
of
which
is
to
confi
rm
that
a
specific
natural
person
is
the
person
he
or
she
claims
to
be
and
to
confi
rm
the
identity
of
a
natural
person
for
the
sole
purpose
of
having
access
to
a
service
,
unloc
king
a
device
or
having
security
access
to
premises
.
That
exclusion
is
justified
by
the
fact
that
such
systems
are
likely
to
have
a
minor
imp
act
on
fundamental
rights
of
natural
persons
comp
ared
to
the
remote
biometric
identifi
cation
systems
which
may
be
used
for
the
processing
of
the
biometric
data
of
a
large
number
of
persons
without
their
active
involvement
.
In
the
case
of
‘
real
-
time
’
systems
,
the
captu
ring
of
the
biometric
data
,
the
comp
arison
and
the
identifi
cation
occur
all
instantaneously
,
near
-instantaneously
or
in
any
event
without
a
significant
dela
y.
In
this
regard
,
there
should
be
no
scope
for
circum
venting
the
rules
of
this
Regulation
on
the
‘
real
-
time
’
use
of
the
AI
systems
concer
ned
by
providing
for
minor
dela
ys
.
‘
Real
-
time
’
systems
involve
the
use
of
‘
live
’
or
‘
near
-live
’
material
,
such
as
video
footage
,
generat
ed
by
a
camera
or
other
device
with
similar
functionality
.
In
the
case
of
‘
post
’
systems
,
in
contrast
,
the
biometric
data
has
already
been
captured
and
the
comp
arison
and
identifica
tion
occur
only
after
a
significant
dela
y.
This
involves
mat
erial
,
such
as
pictures
or
video
footage
generated
by
closed
circuit
television
cameras
or
private
devices
,
which
has
been
generated
before
the
use
of
the
syste
m
in
respect
of
the
natural
persons
concer
ned
.
(
18
)
The
notion
of
‘
emotion
recognition
system
’
referred
to
in
this
Regulation
should
be
defined
as
an
AI
syste
m
for
the
purpose
of
identifying
or
inferr
ing
emotions
or
intentions
of
natural
persons
on
the
basis
of
their
biometric
data
.
The
notion
refers
to
emotions
or
intent
ions
such
as
happiness
,
sadness
,
anger
,
surprise
,
disgust
,
embar
rassment
,
excitement
,
shame
,
contem
pt
,
satisfaction
and
amusement
.
It
does
not
include
physical
states
,
such
as
pain
or
fatigue
,
including
,
for
example
,
systems
used
in
detecting
the
state
of
fatigue
of
profe
ssional
pilots
or
drivers
for
the
purpose
of
preventing
accidents
.
This
does
also
not
include
the
mere
detection
of
readily
apparent
expressions
,
gestures
or
movements
,
unless
they
are
used
for
identifying
or
inferr
ing
emotions
.
Those
expressions
can
be
basic
facial
expressions
,
such
as
a
frown
or
a
smile
,
or
gestures
such
as
the
moveme
nt
of
hands
,
arms
or
head
,
or
characteristics
of
a
person
’s
voice
,
such
as
a
raised
voice
or
whisper
ing
.
(
19
)
For
the
purposes
of
this
Regulation
the
notion
of
‘
publicly
accessible
space
’
should
be
understood
as
referring
to
any
phys
ical
space
that
is
accessible
to
an
undeter
mined
number
of
natural
persons
,
and
irrespective
of
whether
the
space
in
question
is
privately
or
publicly
owned
,
irrespective
of
the
activity
for
which
the
space
may
be
used
,
such
as
for
commerce
,
for
example
,
shops
,
restaurants
,
cafés
;
for
services
,
for
example
,
banks
,
professional
activities
,
hospitality
;
for
spor
t
,
for
example
,
swimming
pools
,
gyms
,
stadiums
;
for
transport
,
for
example
,
bus
,
metro
and
railwa
y
stations
,
airports
,
means
of
transport
;
for
entertainment
,
for
example
,
cinemas
,
theatres
,
museums
,
concer
t
and
confe
rence
halls
;
or
for
leisure
or
other
wise
,
for
example
,
public
roads
and
squares
,
park
s
,
forests
,
playgrounds
.
A
space
should
also
be
classified
as
being
publicly
accessible
if
,
regard
less
of
pote
ntial
capacity
or
security
restrictions
,
access
is
subject
to
certain
predete
rmined
conditions
which
can
be
fulfilled
by
an
undeter
mined
number
of
persons
,
such
as
the
purc
hase
of
a
ticket
or
title
of
transport
,
prior
registration
or
having
a
certain
age
.
In
contrast
,
a
space
should
not
be
considered
to
be
publicly
accessible
if
access
is
limited
to
specific
and
defined
natural
persons
through
either
Union
or
national
law
directly
related
to
public
safety
or
security
or
through
the
clear
manif
estation
unloc
ked
door
or
an
open
gate
in
a
fence
,
does
not
imp
ly
that
the
space
is
publicly
accessible
in
the
presence
of
indications
or
circumstances
suggesting
the
contrar
y
,
such
as
.
signs
prohibiting
or
restr
icting
access
.
Compan
y
and
factory
premises
,
as
well
as
offices
and
workplaces
that
are
intende
d
to
be
accessed
only
by
relevant
emp
loyees
and
service
providers
,
are
spaces
that
are
not
publicly
accessible
.
Publicly
accessible
spaces
should
not
include
prisons
or
border
control
.
Some
other
spaces
may
compr
ise
both
publicly
accessible
and
non
-
publicly
accessible
spaces
,
such
as
the
hallwa
y
of
a
private
residential
building
necessary
to
access
a
docto
r
’s
office
or
an
airport
.
Online
spaces
are
not
covered
,
as
they
are
not
physical
spaces
.
Whether
a
given
space
is
accessible
to
the
public
should
howe
ver
be
determined
on
a
case
-
by
-
c
ase
basis
,
(
20
)
In
order
to
obtain
the
great
est
benefits
from
AI
systems
while
prot
ecting
fundamental
rights
,
health
and
safety
and
to
enable
democratic
control
,
AI
literacy
should
equip
providers
,
deplo
yers
and
affect
ed
persons
with
the
necessary
notions
to
mak
e
informed
decisions
regarding
AI
systems
.
Those
notions
may
vary
with
regard
to
the
relevant
cont
ext
and
can
include
understanding
the
correct
application
of
technical
elements
during
the
AI
system
’s
development
phase
,
the
measures
to
be
applied
during
its
use
,
the
suitable
ways
in
which
to
inter
pret
the
AI
system
’s
output
,
and
,
in
the
case
of
affected
persons
,
the
kno
wledge
necessary
to
understand
how
decisions
taken
with
the
assistance
of
AI
will
have
an
imp
act
on
them
.
In
the
context
of
the
application
this
Regulation
,
AI
literacy
should
provide
all
relevant
actors
in
the
AI
value
chain
with
the
insights
required
to
ensure
the
appropriate
compliance
and
its
correct
enforcement
.
Further
more
,
the
wide
implementation
of
AI
literac
y
measures
and
the
introduction
of
appropriate
follow
-
up
actions
could
contribut
e
to
impro
ving
work
ing
conditions
and
ultimately
sustain
the
consolidation
,
and
innovation
path
of
trustworthy
AI
in
the
Union
.
The
European
Artificial
Intellig
ence
Board
(
the
‘
Board
’
)
should
support
the
Commission
,
to
promot
e
AI
litera
cy
tools
,
public
awareness
and
understanding
of
the
benefits
,
risks
,
safegua
rds
,
rights
and
obligations
in
relation
to
the
use
of
AI
systems
.
In
cooperation
with
the
relevant
stak
eholders
,
the
Commission
and
the
Member
States
should
facilitate
the
drawing
up
of
voluntar
y
codes
of
conduct
to
advance
AI
literacy
among
persons
dealing
with
the
development
,
operation
and
use
of
AI
.
(
21
)
In
order
to
ensure
a
level
playing
field
and
an
effective
protect
ion
of
rights
and
freedoms
of
individuals
across
the
Union
,
the
rules
established
by
this
Regulation
should
apply
to
provid
ers
of
AI
systems
in
a
non
-
discr
iminat
ory
manner
,
irrespective
of
whether
they
are
established
within
the
Union
or
in
a
third
countr
y
,
and
to
deplo
yers
of
AI
systems
established
within
the
Union
.
(
22
)
In
light
of
their
digital
nature
,
certain
AI
systems
should
fall
within
the
scope
of
this
Regulation
even
when
they
are
not
placed
on
the
market
,
put
into
service
,
or
used
in
the
Union
.
This
is
the
case
,
for
example
,
where
an
operator
established
in
the
Union
contracts
certain
services
to
an
operator
established
in
a
third
countr
y
in
relation
to
an
activity
to
be
perform
ed
by
an
AI
system
that
would
qualify
as
higher
isk
.
In
those
circumstances
,
the
AI
syste
m
used
in
a
third
countr
y
by
the
operator
could
process
data
lawfully
collected
in
and
transferre
d
from
the
Union
,
and
provide
to
the
contracting
operato
r
in
the
Union
the
output
of
that
AI
system
resulting
from
that
processing
,
without
that
AI
syste
m
being
placed
on
the
market
,
put
into
service
or
used
in
the
Union
.
To
prevent
the
circum
vention
of
this
Regulation
and
to
ensure
an
effective
protection
of
natural
persons
located
in
the
Union
,
this
Regulation
should
also
apply
to
providers
and
deplo
yers
of
AI
systems
that
are
established
in
a
third
countr
y
,
to
the
extent
the
output
produced
by
those
systems
is
intende
d
to
be
used
in
the
Union
.
Nonetheless
,
to
take
into
account
existing
arrang
ements
and
special
needs
for
future
cooperation
with
foreign
partners
with
whom
information
and
evidence
is
exchang
ed
,
this
Regulation
should
not
apply
to
public
authorities
of
a
third
countr
y
and
inter
national
organisations
when
acting
in
the
framework
of
cooperation
or
inter
national
agreements
concluded
at
Union
or
national
level
for
law
enforcement
and
judicial
cooperation
with
the
Union
or
the
Member
States
,
provided
that
the
relevant
third
countr
y
or
international
organisation
provides
adequate
safegua
rds
with
respect
to
the
protection
of
fundamental
rights
and
freedoms
of
individuals
.
Where
relevant
,
this
may
cover
activities
of
entities
entr
usted
by
the
third
countr
ies
to
carry
out
specific
tasks
in
support
of
such
law
enforcement
and
judicial
cooperation
.
Such
framework
for
cooperation
or
agreements
have
been
established
bilat
erally
between
Member
States
and
third
countr
ies
or
between
the
European
Union
,
Europol
and
other
Union
agencies
and
third
countr
ies
and
international
organisations
.
The
authorities
competent
for
super
vision
of
the
law
enforcement
and
judicial
authorities
under
this
Regulation
should
assess
whether
those
frameworks
for
cooperation
or
inter
national
agreements
include
adequate
safeguards
with
respect
to
the
protect
ion
of
fundamental
rights
and
freedoms
of
individuals
.
Recipient
national
and
Union
institutions
,
bodies
,
offices
and
agencies
making
use
of
such
outputs
in
the
Union
remain
accountable
to
ensure
their
use
comp
lies
with
Union
law
.
When
those
inter
national
agreements
are
revised
or
new
ones
are
concluded
in
the
future
,
the
contracting
parties
should
make
utmost
efforts
to
align
those
agreements
with
the
requirements
of
this
Regulation
.
(
23
)
This
Regulation
should
also
apply
to
Union
institutions
,
bodies
,
offices
and
agencies
when
acting
as
a
provid
er
or
deplo
yer
of
an
AI
syste
m.
(
24
)
If
,
and
insofa
r
as
,
AI
systems
are
placed
on
the
market
,
put
into
service
,
or
used
with
or
without
modification
of
such
systems
for
military
,
defe
nce
or
national
security
purposes
,
those
should
be
excluded
from
the
scope
of
this
Regulation
regard
less
of
which
type
of
entity
is
carrying
out
those
activities
,
such
as
whether
it
is
a
public
or
private
entity
.
As
regard
s
military
and
defe
nce
purposes
,
such
exclusion
is
justif
ied
both
by
Article
4(2
)
TEU
and
by
the
specificities
of
the
Member
States
’
and
the
common
Union
defence
policy
covered
by
Chapt
er
2
of
Title
V
TEU
that
are
subject
to
public
inter
national
law
,
which
is
theref
ore
the
more
appropriate
legal
framework
for
the
regulation
of
AI
systems
in
the
cont
ext
of
the
use
of
lethal
force
and
other
AI
systems
in
the
context
of
military
and
defence
activities
.
As
rega
rds
national
security
purposes
,
the
exclusion
is
justified
both
by
the
fact
that
national
security
remains
the
sole
responsibility
of
Member
States
in
accordance
with
Article
4(2
)
TEU
and
by
the
specific
nature
and
operational
needs
of
national
security
activities
and
specific
national
rules
applicable
to
those
activities
.
Nonetheless
,
if
an
AI
system
developed
,
placed
on
the
market
,
put
into
service
or
used
for
military
,
defence
or
national
security
purposes
is
used
outside
those
temporar
ily
or
permanently
for
other
purposes
,
for
example
,
civilian
or
humanitar
ian
purposes
,
law
enforcement
or
public
security
purposes
,
such
a
system
would
fall
within
the
scope
of
this
Regulation
.
In
that
case
,
the
entity
using
the
AI
syste
m
for
other
than
military
,
defe
nce
or
national
security
purposes
should
ensure
the
compliance
of
the
AI
syste
m
with
this
Regulation
,
unless
the
syste
m
is
already
compliant
with
this
Regulation
.
AI
systems
placed
on
the
market
or
put
into
service
for
an
excluded
purpose
,
namely
military
,
defe
nce
or
national
security
,
and
one
or
more
non
-
exclu
ded
purposes
,
such
as
civilian
purposes
or
law
enforcement
,
fall
within
the
scope
of
this
Regulation
and
providers
of
those
systems
should
ensure
compliance
with
this
Regulation
.
In
those
cases
,
the
fact
that
an
AI
system
may
fall
within
the
scope
of
this
Regulation
should
not
affect
the
possibility
of
entities
carrying
out
national
security
,
defence
and
military
activities
,
rega
rdless
of
the
type
of
entity
carrying
out
those
activities
,
to
use
AI
systems
for
national
security
,
military
and
defe
nce
purposes
,
the
use
of
which
is
excluded
from
the
scope
of
this
Regulation
.
An
AI
system
placed
on
the
market
for
civilian
or
law
enforcement
purposes
which
is
used
with
or
without
modifi
cation
for
military
,
defence
or
national
security
purposes
should
not
fall
within
the
scope
of
this
Regulation
,
rega
rdless
of
the
type
of
entity
carrying
out
those
activities
.
(
25
)
This
Regulation
should
support
innovation
,
should
respect
freedom
of
science
,
and
should
not
under
mine
research
and
development
activity
.
It
is
theref
ore
necessary
to
exclude
from
its
scope
AI
systems
and
models
specifically
developed
and
put
into
service
for
the
sole
purpose
of
scientifi
c
research
and
development
.
Moreove
r
,
it
is
necessary
to
ensure
that
this
Regulation
does
not
other
wise
affect
scientific
research
and
development
activity
on
AI
systems
or
models
prior
to
being
placed
on
the
market
or
put
into
service
.
As
regards
product
-
or
iented
research
,
testing
and
development
activity
rega
rding
AI
systems
or
models
,
the
provisions
of
this
Regulation
should
also
not
apply
prior
to
those
systems
and
models
being
put
into
service
or
placed
on
the
market
.
That
exclusion
is
without
prejudice
to
the
obligation
to
comp
ly
with
this
Regulation
where
an
AI
system
falling
into
the
scope
of
this
Regulation
is
placed
on
the
market
or
put
into
service
as
a
result
of
such
research
and
development
activity
and
to
the
application
of
provisions
on
AI
regulato
ry
sandbo
xes
and
testing
in
real
world
conditions
.
Further
more
,
without
prejudice
to
the
exclusion
of
AI
systems
specifically
developed
and
put
into
service
for
the
sole
purpose
of
scientifi
c
researc
h
and
development
,
any
other
AI
system
that
may
be
used
for
the
conduct
of
any
research
and
development
activity
should
remain
subject
to
the
provisions
of
this
Regulation
.
In
any
event
,
any
research
and
development
activity
should
be
carried
out
in
accordance
with
recognised
ethical
and
profe
ssional
standards
for
scientific
research
and
should
be
conducte
d
in
accordance
with
applicable
Union
law
.
(
26
)
In
order
to
introduce
a
propor
tionate
and
effective
set
of
binding
rules
for
AI
systems
,
a
clearly
defined
risk
-
based
approac
h
should
be
follo
we
d.
That
approac
h
should
tailor
the
type
and
content
of
such
rules
to
the
intensity
and
scope
of
the
risks
that
AI
systems
can
generate
.
It
is
theref
ore
necessary
to
prohibit
certain
unaccept
able
AI
practices
,
to
lay
down
requirements
for
higher
isk
AI
systems
and
obligati
ons
for
the
relevant
operators
,
and
to
lay
down
transparency
obligations
for
certain
AI
systems
.
(
27
)
While
the
risk
-
based
approac
h
is
the
basis
for
a
propor
tionate
and
effective
set
of
binding
rules
,
it
is
imp
ortant
to
recall
the
2019
Ethics
guidelines
for
trustwo
rthy
AI
developed
by
the
independent
AI
HLEG
appoint
ed
by
the
Commission
.
In
those
guidelines
,
the
AI
HLEG
developed
seven
non
-
binding
ethical
principles
for
AI
which
are
intended
to
help
ensure
that
AI
is
trustworthy
and
ethically
sound
.
The
seven
principles
include
human
agency
and
oversight
;
technical
robustness
and
safety
;
privacy
and
data
gover
nance
;
transparency
;
diversity
,
non
-
discr
imination
and
fairness
;
societal
and
environmental
well
-
being
and
accountability
.
Without
prejudice
to
the
legally
binding
requirements
of
this
Regulation
and
any
other
applicable
Union
law
,
those
guidelines
contribut
e
to
the
design
of
coherent
,
trustworthy
and
human
-
centr
ic
AI
,
in
line
with
the
Charter
and
with
the
values
on
which
the
Union
is
founded
.
According
to
the
guidelines
of
the
AI
HLEG
,
human
agency
and
oversight
means
that
AI
systems
are
developed
and
used
as
a
tool
that
serves
people
,
respects
human
dignity
and
personal
autonom
y
,
and
that
is
functioning
in
a
way
that
can
be
appropriately
controlled
and
overseen
by
humans
.
Technical
robustness
and
safety
means
that
AI
systems
are
developed
and
used
in
a
way
that
allows
robustness
in
the
case
of
problems
and
resilience
against
attempts
to
alter
the
use
or
perform
ance
of
the
AI
syste
m
so
as
to
allow
unla
wful
use
by
third
parties
,
and
minimise
uninte
nded
harm
.
Privacy
and
data
gove
rnance
means
that
AI
systems
are
developed
and
used
in
accordance
with
privacy
and
data
protection
rules
,
while
processing
data
that
meets
high
standards
in
terms
of
quality
and
integrity
.
Transparency
means
that
AI
systems
are
developed
and
used
in
a
way
that
allows
appropriate
traceability
and
explainability
,
while
making
humans
aware
that
they
communicate
or
interact
with
an
AI
syste
m
,
as
well
as
duly
informing
deplo
yers
of
the
capabilities
and
limitations
of
that
AI
system
and
affected
persons
about
their
rights
.
Diversity
,
non
-
discr
imination
and
fairne
ss
means
that
AI
systems
are
developed
and
used
in
a
way
that
includes
diverse
actor
s
and
promotes
equal
access
,
gender
equality
and
cultural
diversity
,
while
avoiding
discr
iminatory
imp
acts
and
unfa
ir
biases
that
are
prohibite
d
by
Union
or
national
law
.
Social
and
environmental
well
-
being
means
that
AI
systems
are
developed
and
used
in
a
sustainable
and
environmentally
friendly
manner
as
well
as
in
a
way
to
benefi
t
all
human
beings
,
while
monitori
ng
and
assessing
the
long
-
term
imp
acts
on
the
individual
,
society
and
democracy
.
The
application
of
those
principles
should
be
translat
ed
,
when
possible
,
in
the
design
and
use
of
AI
models
.
They
should
in
any
case
serve
as
a
basis
for
the
drafting
of
codes
of
conduct
under
this
Regulation
.
All
stakeholders
,
including
industr
y
,
academia
,
civil
society
and
standardisation
organisations
,
are
encourag
ed
to
take
into
account
,
as
appropriate
,
the
ethical
principles
for
the
development
of
voluntary
best
practices
and
standards
.
(
28
)
Aside
from
the
many
beneficial
uses
of
AI
,
it
can
also
be
misused
and
provide
novel
and
powerful
tools
for
manipulative
,
exploitative
and
social
control
practices
.
Such
practices
are
particularly
harmful
and
abusive
and
should
be
prohibite
d
because
they
contradict
Union
values
of
respect
for
human
dignity
,
freedom
,
equality
,
democracy
and
the
rule
of
law
and
fundamental
rights
enshrined
in
the
Charter
,
including
the
right
to
non
-
discr
imination
,
to
data
protection
and
to
privacy
and
the
rights
of
the
child
.
(
29
)
AI
-
enabled
manipulative
techniques
can
be
used
to
persuade
persons
to
engage
in
unwant
ed
behaviours
,
or
to
deceive
them
by
nudging
them
into
decisions
in
a
way
that
subver
ts
and
imp
airs
their
autonom
y
,
decision
-
making
and
free
choices
.
The
placing
on
the
market
,
the
putting
into
service
or
the
use
of
certain
AI
systems
with
the
objective
to
or
the
effect
of
mat
erially
distor
ting
human
behavio
ur
,
whereb
y
signifi
ca
nt
harms
,
in
particular
having
sufficiently
imp
ortant
adverse
impacts
on
phys
ical
,
psychological
health
or
financ
ial
intere
sts
are
likely
to
occur
,
are
particularly
dangerous
and
should
theref
ore
be
prohibite
d.
Such
AI
systems
deplo
y
subliminal
comp
onents
such
as
audio
,
image
,
video
stimuli
that
persons
can
not
perceive
,
as
those
stimuli
are
beyond
human
percep
tion
,
or
other
manipulative
or
decept
i
ve
techniques
that
subver
t
or
imp
air
person
’s
auto
nomy
,
decision
-
making
or
free
choice
in
ways
that
people
are
not
consciously
aware
of
those
techniques
or
,
where
they
are
aware
of
them
,
can
still
be
deceived
or
are
not
able
to
control
or
resist
them
.
This
could
be
facilitated
,
for
example
,
by
machine
-
brain
interfaces
or
virtual
reality
as
they
allow
for
a
higher
degree
of
control
of
what
stimuli
are
presented
to
persons
,
insofa
r
as
they
may
materially
distor
t
their
behavio
ur
in
a
significantly
harmful
manner
.
In
addition
,
AI
systems
may
also
other
wise
exploit
the
vulnerabilities
of
a
person
or
a
specific
group
of
persons
due
to
their
age
,
disability
within
the
meaning
of
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
or
a
specific
social
or
economic
situation
that
is
likely
to
mak
e
those
persons
more
vulnerable
to
exploitation
such
as
persons
living
in
extreme
poverty
,
ethnic
or
religious
minor
ities
.
Such
AI
systems
can
be
placed
on
the
market
,
put
into
service
or
used
with
the
objective
to
or
the
effect
of
materially
distor
ting
the
behavi
our
of
a
person
and
in
a
manner
that
causes
or
is
reasonably
likely
to
cause
signifi
ca
nt
harm
to
that
or
another
person
or
groups
of
persons
,
including
harms
that
may
be
accumulated
over
time
and
should
theref
ore
be
prohibite
d.
It
may
not
be
possible
to
assume
that
there
is
an
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
17
Apr
il
2019
on
the
accessibility
requirements
for
products
and
services
(
6.2019
,
p.
70).intention
to
distor
t
behavio
ur
where
the
distor
tion
results
from
factors
exter
nal
to
the
AI
syste
m
which
are
outside
the
control
of
the
provid
er
or
the
deplo
yer
,
namely
factors
that
may
not
be
reasonably
foreseeable
and
theref
ore
not
possible
for
the
provider
or
the
deplo
yer
of
the
AI
syste
m
to
mitigat
e.
In
any
case
,
it
is
not
necessary
for
the
provid
er
or
the
deplo
yer
to
have
the
intention
to
cause
signifi
ca
nt
harm
,
provided
that
such
harm
results
from
the
manipulative
or
exploitative
AI
-
enabled
practices
.
The
prohibitions
for
such
AI
practices
are
complement
ary
to
the
provisions
contained
in
Directive
/EC
of
the
European
Parliame
nt
and
of
the
Council
,
in
particular
unfa
ir
commercial
practices
leading
to
economic
or
financial
harms
to
consumers
are
prohibited
under
all
circumstances
,
irrespective
of
whether
they
are
put
in
place
through
AI
systems
or
other
wise
.
The
prohibitions
of
manipulative
and
exploitative
practices
in
this
Regulation
should
not
affect
lawful
practices
in
the
context
of
medical
treatment
such
as
psychological
treatment
of
a
mental
disease
or
phys
ical
rehabilitation
,
when
those
practices
are
carried
out
in
accordance
with
the
applicable
law
and
medical
standards
,
for
example
explicit
consent
of
the
individuals
or
their
legal
representatives
.
In
addition
,
common
and
legitimate
commercial
practices
,
for
example
in
the
field
of
adver
tising
,
that
comply
with
the
applicable
law
should
not
,
in
themselves
,
be
rega
rded
as
constituting
harmful
manipulative
AI
-
enabled
practices
.
biometric
categor
isation
systems
that
are
based
on
natural
persons
’
biometric
data
,
such
as
an
individual
person
’s
face
or
finger
print
,
to
deduce
or
infer
an
individuals
’
political
opinions
,
trade
union
membership
,
religious
or
philosophical
beliefs
,
race
,
sex
life
or
sexual
orientation
should
be
prohibite
d.
That
prohibition
should
not
cover
the
lawful
labelling
,
filter
ing
or
categori
sation
of
biometric
data
sets
acquired
in
line
with
Union
or
national
law
according
to
biometric
data
,
such
as
the
sorting
of
imag
es
according
to
hair
colour
or
eye
colour
,
which
can
for
example
be
used
in
the
area
of
law
enforcement
.
(
31
)
AI
systems
provid
ing
social
scor
ing
of
natural
persons
by
public
or
private
actors
may
lead
to
discr
iminat
ory
outcomes
and
the
exclusion
of
certain
groups
.
They
may
violate
the
right
to
dignity
and
non
-
discr
imination
and
the
values
of
equality
and
justice
.
Such
AI
systems
evaluate
or
classify
natural
persons
or
groups
thereof
on
the
basis
of
multiple
data
points
related
to
their
social
behavio
ur
in
multiple
contexts
or
known
,
inferred
or
predicted
personal
or
personality
character
istics
over
certain
periods
of
time
.
The
social
score
obtained
from
such
AI
systems
may
lead
to
the
detr
imental
or
unfa
vourable
treatment
of
natural
persons
or
whole
groups
thereof
in
social
cont
exts
,
which
are
unrelate
d
to
the
context
in
which
the
data
was
originally
generat
ed
or
collect
ed
or
to
a
detr
imental
treatment
that
is
dispropor
tionate
or
unjustifie
d
to
the
gravity
of
their
social
behavio
ur
.
AI
systems
entailing
such
unaccept
able
scor
ing
practices
and
leading
to
such
detr
imental
or
unfa
vourable
outcomes
should
theref
ore
be
prohibited
.
That
prohibition
should
not
affect
lawful
evaluation
practices
of
natural
persons
that
are
carried
out
for
a
specific
purpose
in
accordance
with
Union
and
national
law
.
(
32
)
The
use
of
AI
systems
for
‘
real
-
time
’
remote
biometric
identification
of
natural
persons
in
publicly
accessible
spaces
for
the
purpose
of
law
enforcement
is
particularly
intrusive
to
the
rights
and
freedoms
of
the
concer
ned
persons
,
to
the
extent
that
it
may
affect
the
private
life
of
a
large
part
of
the
population
,
evok
e
a
feeling
of
constant
surveillance
and
indirectly
dissuade
the
exercise
of
the
freedom
of
assembly
and
other
fundamental
rights
.
Technical
inaccuracies
of
AI
systems
intende
d
for
the
remote
biometric
identifi
cation
of
natural
persons
can
lead
to
biased
results
and
entail
discr
iminatory
effects
.
Such
possible
biased
results
and
discr
iminatory
effects
are
particularly
relevant
with
regard
to
age
,
ethnicity
,
race
,
sex
or
disabilities
.
In
addition
,
the
immediacy
of
the
impact
and
the
limited
oppor
tunities
for
further
chec
ks
or
corrections
in
relation
to
the
use
of
such
systems
operating
in
real
-
time
carry
height
ened
risks
for
the
rights
and
freedoms
of
the
persons
concer
ned
in
the
cont
ext
of
,
or
impacted
by
,
law
enforcement
activities
.
(
33
)
The
use
of
those
systems
for
the
purpose
of
law
enforcement
should
theref
ore
be
prohibited
,
except
in
exhaustively
listed
and
narrowly
defined
situations
,
where
the
use
is
strictly
necessary
to
achieve
a
substantial
public
interest
,
the
importance
of
which
outweighs
the
risks
.
Those
situations
involve
the
search
for
certain
victims
of
crime
including
missing
persons
;
certain
threats
to
the
life
or
to
the
phys
ical
safety
of
natural
persons
or
of
a
terrorist
attac
k
;
and
the
localisation
or
identification
of
perpetrat
ors
or
suspects
of
the
criminal
offences
listed
in
an
annex
to
this
Regulation
,
where
those
criminal
offences
are
punishable
in
the
Member
State
concer
ned
by
a
custo
dial
sentence
or
a
detention
(
17
)
Directive
/EC
of
the
European
Parliament
and
of
the
Council
of
11
May
2005
concerning
unfair
business
-
to
-
consumer
commercial
practices
in
the
inter
nal
market
and
amending
Council
Directive
/EEC
,
Directives
/EC
,
/EC
and
/EC
of
the
European
Parliament
and
of
the
Council
and
Regulation
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
(
‘
Unf
air
Commercial
Practices
Directive
’
)
(
6.2005
,
p.
22).order
for
a
maximum
period
of
at
least
four
years
and
as
they
are
defined
in
the
law
of
that
Member
State
.
Such
a
threshold
for
the
custo
dial
sent
ence
or
detention
order
in
accordance
with
national
law
contribut
es
to
ensur
ing
that
the
offence
should
be
serious
enough
to
potentially
justify
the
use
of
‘
real
-
time
’
remot
e
biometric
identification
systems
.
Moreover
,
the
list
of
criminal
offences
provided
in
an
annex
to
this
Regulation
is
based
on
the
32
criminal
offences
listed
in
the
Council
Framework
Decision
/JHA
,
taking
into
account
that
some
of
those
offences
are
,
in
practice
,
likel
y
to
be
more
relevant
than
others
,
in
that
the
recourse
to
‘
real
-
time
’
remote
biometric
identification
could
,
foreseeably
,
be
necessary
and
propor
tionate
to
highly
varying
degrees
for
the
practical
pursuit
of
the
localisation
or
identification
of
a
perpetrat
or
or
suspect
of
the
different
criminal
offences
listed
and
having
rega
rd
to
the
likely
differences
in
the
seriousness
,
probability
and
scale
of
the
harm
or
possible
negati
ve
consequences
.
An
imminent
threat
to
life
or
the
physical
safety
of
natural
persons
could
also
result
from
a
serious
disruption
of
critical
infrastructure
,
as
defined
in
Article
2
,
point
of
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
where
the
disruption
or
destr
uction
of
such
critical
infrastructure
would
result
in
an
imminent
threat
to
life
or
the
physical
safety
of
a
person
,
including
through
serious
harm
to
the
provision
of
basic
supplies
to
the
population
or
to
the
exercise
of
the
core
function
of
the
State
.
In
addition
,
this
Regulation
should
preser
ve
the
ability
for
law
enforcement
,
border
control
,
immigration
or
asylum
authorities
to
carry
out
identity
checks
in
the
presence
of
the
person
concer
ned
in
accordance
with
the
conditions
set
out
in
Union
and
national
law
for
such
chec
ks
.
In
particular
,
law
enforcement
,
border
control
,
immigration
or
asylum
authorities
should
be
able
to
use
information
systems
,
in
accordance
with
Union
or
national
law
,
to
identify
persons
who
,
during
an
identity
check
,
either
refuse
to
be
identified
or
are
unable
to
state
or
prove
their
identity
,
without
being
required
by
this
Regulation
to
obtain
prior
author
isation
.
This
could
be
,
for
example
,
a
person
involved
in
a
crime
,
being
unwilling
,
or
unable
due
to
an
accident
or
a
medical
condition
,
to
disclose
their
identity
to
law
enforcement
authorities
.
(
34
)
In
order
to
ensure
that
those
systems
are
used
in
a
responsible
and
propor
tionate
manner
,
it
is
also
impor
tant
to
establish
that
,
in
each
of
those
exhaustively
listed
and
narrowly
defined
situations
,
certain
elements
should
be
take
n
into
account
,
in
particular
as
regards
the
nature
of
the
situation
giving
rise
to
the
request
and
the
consequences
of
the
use
for
the
rights
and
freedoms
of
all
persons
concer
ned
and
the
safeguards
and
conditions
provid
ed
for
with
the
use
.
In
addition
,
the
use
of
‘
real
-
time
’
remot
e
biometric
identifi
cation
systems
in
publicly
accessible
spaces
for
the
purpose
of
law
enforcement
should
be
deployed
only
to
confir
m
the
specifically
targe
ted
individual
’s
identity
and
should
be
limit
ed
to
what
is
strictly
necessary
concerning
the
period
of
time
,
as
well
as
the
geographic
and
personal
scope
,
having
rega
rd
in
particular
to
the
evidence
or
indications
rega
rding
the
threats
,
the
victims
or
perpetrat
or
.
The
use
of
the
real
-
time
remote
biometric
identifica
tion
system
in
publicly
accessible
spaces
should
be
authorised
only
if
the
relevant
law
enforcement
author
ity
has
comp
leted
a
fundamental
rights
impact
assessment
and
,
unless
provid
ed
other
wise
in
this
Regulation
,
has
register
ed
the
syste
m
in
the
database
as
set
out
in
this
Regulation
.
The
reference
database
of
persons
should
be
appropriate
for
each
use
case
in
each
of
the
situations
mentioned
above
.
(
35
)
Each
use
of
a
‘
real
-
time
’
remote
biometric
identification
system
in
publicly
accessible
spaces
for
the
purpose
of
law
enforcement
should
be
subject
to
an
express
and
specific
author
isation
by
a
judicial
author
ity
or
by
an
independent
administrative
author
ity
of
a
Member
State
whose
decision
is
binding
.
Such
author
isation
should
,
in
principle
,
be
obtained
prior
to
the
use
of
the
AI
system
with
a
view
to
identifying
a
person
or
persons
.
Exceptions
to
that
rule
should
be
allowed
in
duly
justifi
ed
situations
on
grounds
of
urgency
,
namely
in
situations
where
the
need
to
use
the
systems
concer
ned
is
such
as
to
mak
e
it
effectively
and
objectively
impossible
to
obtain
an
author
isation
before
commencing
the
use
of
the
AI
system
.
In
such
situations
of
urgency
,
the
use
of
the
AI
syste
m
should
be
restr
icted
to
the
absolute
minimum
necessary
and
should
be
subject
to
appropriate
safeguards
and
conditions
,
as
determined
in
national
law
and
specif
ied
in
the
cont
ext
of
each
individual
urgent
use
case
by
the
law
enforcement
author
ity
itself
.
In
addition
,
the
law
enforcement
author
ity
should
in
such
situations
request
such
author
isation
while
provid
ing
the
reasons
for
not
having
been
able
to
request
it
earlier
,
without
undue
dela
y
and
at
the
latest
within
24
hours
.
If
such
an
author
isation
is
rejected
,
the
use
of
real
-
time
biometric
identification
systems
linked
to
that
author
isation
should
cease
with
immediate
effect
and
all
the
data
related
to
such
use
should
be
discarded
and
delete
d.
Such
data
includes
Council
Framework
Decision
/JHA
of
13
June
2002
on
the
European
arrest
warrant
and
the
surrender
procedures
between
Member
States
(
7.2002
,
p.
1
)
.
(
19
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
14
December
2022
on
the
resilience
of
critical
entities
and
repealing
Council
Directive
/EC
(
12.2022
,
p.
164).input
data
directly
acquired
by
an
AI
syste
m
in
the
course
of
the
use
of
such
system
as
well
as
the
results
and
outputs
of
the
use
linked
to
that
author
isation
.
It
should
not
include
input
that
is
lega
lly
acquired
in
accordance
with
another
Union
or
national
law
.
In
any
case
,
no
decision
producing
an
adverse
lega
l
effect
on
a
person
should
be
taken
based
solely
on
the
output
of
the
remot
e
biometric
identification
system
.
(
36
)
In
order
to
carry
out
their
tasks
in
accordance
with
the
requirements
set
out
in
this
Regulation
as
well
as
in
national
rules
,
the
relevant
market
surveillance
author
ity
and
the
national
data
protection
author
ity
should
be
notif
ied
of
each
use
of
the
real
-
time
biometric
identification
system
.
Market
surveillance
authorities
and
the
national
data
protection
authorities
that
have
been
notif
ied
should
submit
to
the
Commission
an
annual
repor
t
on
the
use
of
real
-
time
biometric
identifi
cation
systems
.
(
37
)
Further
more
,
it
is
appropriate
to
provid
e
,
within
the
exhaustive
framework
set
by
this
Regulation
that
such
use
in
the
territory
of
a
Member
State
in
accordance
with
this
Regulation
should
only
be
possible
where
and
in
as
far
as
the
Member
State
concer
ned
has
decided
to
expressly
provide
for
the
possibility
to
author
ise
such
use
in
its
detailed
rules
of
national
law
.
Consequently
,
Member
States
remain
free
under
this
Regulation
not
to
provide
for
such
a
possibility
at
all
or
to
only
provid
e
for
such
a
possibility
in
respect
of
some
of
the
objectives
capable
of
justifying
authorised
use
identified
in
this
Regulation
.
Such
national
rules
should
be
notified
to
the
Commission
within
30
days
of
their
adoption
.
(
38
)
The
use
of
AI
systems
for
real
-
time
remote
biometric
identification
of
natural
persons
in
publicly
accessible
spaces
for
the
purpose
of
law
enforcement
necessar
ily
involves
the
processing
of
biometric
data
.
The
rules
of
this
Regulation
that
prohibit
,
subject
to
certain
exceptions
,
such
use
,
which
are
based
on
Article
16
TFEU
,
should
apply
as
lex
specialis
in
respect
of
the
rules
on
the
processing
of
biometric
data
contained
in
Article
10
of
Directive
(
EU
)
,
thus
regulating
such
use
and
the
processing
of
biometric
data
involved
in
an
exhaustive
manner
.
Theref
ore
,
such
use
and
processing
should
be
possible
only
in
as
far
as
it
is
compatible
with
the
framework
set
by
this
Regulation
,
without
there
being
scope
,
outside
that
framework
,
for
the
comp
etent
authorities
,
where
they
act
for
purpose
of
law
enforcement
,
to
use
such
systems
and
process
such
data
in
connection
thereto
on
the
grounds
listed
in
Article
10
of
Directive
(
EU
)
.
In
that
cont
ext
,
this
Regulation
is
not
intended
to
provide
the
legal
basis
for
the
processing
of
personal
data
under
Article
8
of
Directive
(
EU
)
.
However
,
the
use
of
real
-
time
remote
biometric
identifica
tion
systems
in
publicly
accessible
spaces
for
purposes
other
than
law
enforcement
,
including
by
com
petent
authorities
,
should
not
be
covered
by
the
specific
framework
regard
ing
such
use
for
the
purpose
of
law
enforcement
set
by
this
Regulation
.
Such
use
for
purposes
other
than
law
enforcement
should
theref
ore
not
be
subject
to
the
requirement
of
an
author
isation
under
this
Regulation
and
the
applicable
detailed
rules
of
national
law
that
may
give
effect
to
that
author
isation
.
(
39
)
Any
processing
of
biometric
data
and
other
personal
data
involved
in
the
use
of
AI
systems
for
biometric
identification
,
other
than
in
connection
to
the
use
of
real
-
time
remote
biometric
identification
systems
in
publicly
accessible
spaces
for
the
purpose
of
law
enforcement
as
regulated
by
this
Regulation
,
should
continue
to
comply
with
all
requirements
resulting
from
Article
10
of
Directive
(
EU
)
.
For
purposes
other
than
law
enforcement
,
Article
9(1
)
of
Regulation
(
EU
)
and
Article
10(1
)
of
Regulation
(
EU
)
prohibit
the
processing
of
biometric
data
subject
to
limit
ed
exceptions
as
provided
in
those
Articles
.
In
the
application
of
Article
enforcement
has
already
been
subject
to
prohibition
decisions
by
national
data
protection
authorities
.
(
40
)
In
accordance
with
Article
6a
of
Protocol
No
21
on
the
position
of
the
United
Kingdom
and
Ireland
in
respect
of
the
area
of
freedom
,
security
and
justice
,
as
annexe
d
to
the
TEU
and
to
the
TFEU
,
Ireland
is
not
bound
by
the
rules
laid
down
in
Article
5(1
)
,
first
subparagraph
,
point
(
g
)
,
to
the
extent
it
applies
to
the
use
of
biometric
categor
isation
systems
for
activities
in
the
field
of
police
cooperation
and
judicial
cooperation
in
criminal
matt
ers
,
Article
5(1
)
,
first
subparagraph
,
point
(
d
)
,
to
the
extent
it
applies
to
the
use
of
AI
systems
covered
by
that
provision
,
Article
5(1
)
,
first
subparagraph
,
point
(
h
)
,
Article
5(2
)
to
and
Article
26(10
)
of
this
Regulation
adop
ted
on
the
basis
of
Article
16
TFEU
which
relate
to
the
processing
of
personal
data
by
the
Member
States
when
carrying
out
activities
falling
within
the
scope
of
Chapt
er
4
or
Chapt
er
5
of
Title
V
of
Part
Three
of
the
TFEU
,
where
Ireland
is
not
bound
by
the
rules
governing
the
forms
of
judicial
cooperation
in
criminal
matt
ers
or
police
cooperation
which
require
compliance
with
the
provisions
laid
down
on
the
basis
of
Article
16
TFEU
.
(
41
)
In
accordance
with
Articles
2
and
2a
of
Proto
col
No
22
on
the
position
of
Denmark
,
annexe
d
to
the
TEU
and
to
the
TFEU
,
Denmark
is
not
bound
by
rules
laid
down
in
Article
5(1
)
,
first
subparagraph
,
point
(
g
)
,
to
the
extent
it
applies
to
the
use
of
biometric
categor
isation
systems
for
activities
in
the
field
of
police
cooperation
and
judicial
cooperation
in
criminal
matt
ers
,
Article
5(1
)
,
first
subparagraph
,
point
(
d
)
,
to
the
extent
it
applies
to
the
use
of
AI
systems
covered
by
that
provision
,
Article
5(1
)
,
first
subparagraph
,
point
(
h
)
,
to
and
Article
26(10
)
of
this
Regulation
data
by
the
Member
States
when
carrying
out
activities
falling
within
the
scope
of
Chap
ter
4
or
Chapt
er
5
of
Title
V
of
Part
Three
of
the
TFEU
.
(
42
)
In
line
with
the
presump
tion
of
innocence
,
natural
persons
in
the
Union
should
alwa
ys
be
judged
on
their
actual
behavi
our
.
Natural
persons
should
never
be
judg
ed
on
AI
-
predicted
behavio
ur
based
solely
on
their
prof
iling
,
personality
traits
or
char
acter
istics
,
such
as
nationality
,
place
of
birth
,
place
of
residence
,
number
of
children
,
level
of
debt
or
type
of
car
,
without
a
reasonable
suspicion
of
that
person
being
involved
in
a
criminal
activity
based
on
objective
verifiable
facts
and
without
human
assessment
thereof
.
Theref
ore
,
risk
assessments
carried
out
with
rega
rd
to
natural
persons
in
order
to
assess
the
likelihood
of
their
offending
or
to
predict
the
occur
rence
of
an
actual
or
pote
ntial
criminal
offence
based
solely
on
prof
iling
them
or
on
assessing
their
personality
traits
and
character
istics
should
be
prohibited
.
In
any
case
,
that
prohibition
does
not
refer
to
or
touch
upon
risk
analytics
that
are
not
based
on
the
profil
ing
of
individuals
or
on
the
personality
traits
and
charact
eristics
of
individuals
,
such
as
AI
systems
using
risk
analytics
to
assess
the
likelihood
of
financ
ial
fraud
by
undertakings
on
the
basis
of
suspicious
transactions
or
risk
analytic
tools
to
predict
the
likelihood
of
the
localisation
of
narcotics
or
illicit
goods
by
customs
authorities
,
for
example
on
the
basis
of
known
traff
icking
routes
.
(
43
)
The
placing
on
the
mark
et
,
the
putting
into
service
for
that
specific
purpose
,
or
the
use
of
AI
systems
that
create
or
expand
facial
recognition
databases
through
the
untarg
eted
scraping
of
facial
images
from
the
inter
net
or
CCT
V
footage
,
should
be
prohibite
d
because
that
practice
adds
to
the
feeling
of
mass
surveillance
and
can
lead
to
gross
violations
of
fundamental
rights
,
including
the
right
to
privacy
.
(
44
)
There
are
serious
concer
ns
about
the
scientific
basis
of
AI
systems
aiming
to
identify
or
infer
emotions
,
particularly
as
expression
of
emotions
vary
considerably
across
cultures
and
situations
,
and
even
within
a
sing
le
individual
.
Among
the
key
shor
tcomings
of
such
systems
are
the
limited
reliability
,
the
lack
of
specificity
and
the
limit
ed
generalisability
.
Theref
ore
,
AI
systems
identifying
or
inferr
ing
emotions
or
intentions
of
natural
persons
on
the
basis
of
their
biometric
data
may
lead
to
discr
iminat
ory
outcomes
and
can
be
intrusive
to
the
rights
and
freedoms
of
the
concer
ned
persons
.
Consider
ing
the
imbalance
of
power
in
the
cont
ext
of
work
or
education
,
combined
with
the
intrusive
nature
of
these
systems
,
such
systems
could
lead
to
detr
imental
or
unfa
vourable
treatment
of
certain
natural
persons
or
whole
groups
thereof
.
Theref
ore
,
the
placing
on
the
market
,
the
putting
into
service
,
or
the
use
of
AI
systems
intended
to
be
used
to
detect
the
emotional
state
of
individuals
in
situations
related
to
the
workplace
and
education
should
be
prohibite
d.
That
prohibition
should
not
cover
AI
systems
placed
on
the
mark
et
strictly
for
medical
or
safety
reasons
,
such
as
systems
intended
for
therapeutical
use
.
(
45
)
Practices
that
are
prohibited
by
Union
law
,
including
data
protection
law
,
non
-
discr
imination
law
,
consumer
protection
law
,
and
competition
law
,
should
not
be
affected
by
this
Regulation
.
higher
isk
AI
systems
should
only
be
placed
on
the
Union
market
,
put
into
service
or
used
if
they
comply
with
certain
mandato
ry
requirements
.
Those
requirements
should
ensure
that
higher
isk
AI
systems
available
in
the
Union
or
whose
output
is
other
wise
used
in
the
Union
do
not
pose
unaccepta
ble
risks
to
important
Union
public
interests
as
recognised
and
protected
by
Union
law
.
On
the
basis
of
the
New
Legislative
Framework
,
as
clarified
in
the
Commission
notice
‘
The
“
Blue
Guide
”
on
the
imp
lementation
of
EU
product
rules
2022
’
,
the
general
rule
is
that
more
than
one
lega
l
act
of
Union
harmonisation
legislation
,
such
as
Regulations
(
EU
)
and
(
EU
)
of
the
European
Parliament
and
of
the
Council
or
Directive
/EC
of
the
European
Parliament
and
of
the
Council
,
may
be
applicable
to
one
product
,
since
the
making
availa
ble
or
putting
into
service
can
take
/EC
,
Regulation
(
EC
)
No
and
Regulation
(
EC
)
No
and
repealing
Council
Directives
/EEC
and
/EEC
(
5.2017
,
p.
1
)
.
(
22
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
5
Apr
il
2017
on
in
vitro
diagnostic
medical
devices
and
repealing
Directive
/EC
and
Commission
Decision
/EU
(
5.2017
,
p.
176
)
.
(
23
)
Directive
/EC
of
the
European
Parliament
and
of
the
Council
of
17
May
2006
on
machiner
y
,
and
amending
Directive
/EC
(
6.2006
,
p.
24).place
only
when
the
product
comp
lies
with
all
applicable
Union
harmonisation
legislation
.
To
ensure
consiste
ncy
and
avoid
unnecessary
administrative
burdens
or
costs
,
provid
ers
of
a
product
that
contains
one
or
more
higher
isk
AI
systems
,
to
which
the
requirements
of
this
Regulation
and
of
the
Union
harmonisation
legislation
listed
in
an
annex
to
this
Regulation
apply
,
should
have
flexibility
with
regard
to
operational
decisions
on
how
to
ensure
compliance
of
a
product
that
contains
one
or
more
AI
systems
with
all
applicable
requirements
of
the
Union
harmonisation
legislation
in
an
optima
l
manner
.
AI
systems
identified
as
higher
isk
should
be
limit
ed
to
those
that
have
a
significant
harmful
impact
on
the
health
,
safety
and
fundamental
rights
of
persons
in
the
Union
and
such
limitation
should
minimise
any
pote
ntial
restr
iction
to
international
trade
.
(
47
)
AI
systems
could
have
an
adverse
impact
on
the
health
and
safety
of
persons
,
in
particular
when
such
systems
operat
e
as
safety
comp
onents
of
products
.
Consiste
nt
with
the
objectives
of
Union
harmonisation
legislation
to
facilitate
the
free
movement
of
products
in
the
inter
nal
market
and
to
ensure
that
only
safe
and
other
wise
comp
liant
products
find
their
way
into
the
market
,
it
is
imp
ortant
that
the
safety
risks
that
may
be
generated
by
a
product
as
a
whole
due
to
its
digital
com
ponents
,
including
AI
systems
,
are
duly
prevented
and
mitigat
ed
.
For
instance
,
increasing
ly
autono
mous
robots
,
whether
in
the
cont
ext
of
manuf
actur
ing
or
personal
assistance
and
care
should
be
able
to
safely
operate
and
perfo
rms
their
functions
in
complex
environments
.
Similarly
,
in
the
health
secto
r
where
the
stakes
for
life
and
health
are
particularly
high
,
increasing
ly
sophisticat
ed
diagnostics
systems
and
systems
supporting
human
decisions
should
be
reliable
and
accurate
.
(
48
)
The
extent
of
the
adverse
imp
act
caused
by
the
AI
syste
m
on
the
fundamental
rights
protected
by
the
Charter
is
of
particular
relevance
when
classifying
an
AI
syste
m
as
high
risk
.
Those
rights
include
the
right
to
human
dignity
,
respect
for
private
and
family
life
,
protection
of
personal
data
,
freedom
of
expression
and
information
,
freedom
of
assembly
and
of
association
,
the
right
to
non
-
discr
imination
,
the
right
to
education
,
consumer
protection
,
work
ers
’
rights
,
the
rights
of
persons
with
disabilities
,
gender
equality
,
intellectual
proper
ty
rights
,
the
right
to
an
effective
remedy
and
to
a
fair
trial
,
the
right
of
defence
and
the
presump
tion
of
innocence
,
and
the
right
to
good
administration
.
In
addition
to
those
rights
,
it
is
impor
tant
to
highlight
the
fact
that
children
have
specific
rights
as
enshrined
in
Article
24
of
the
Charter
and
in
the
Unit
ed
Nations
Convention
on
the
Rights
of
the
Child
,
further
developed
in
the
UNCR
C
General
Comment
No
25
as
regard
s
the
digital
environment
,
both
of
which
require
consideration
of
the
children
’s
vulnerabilities
and
provision
of
such
protection
and
care
as
necessary
for
their
well
-
being
.
The
fundamental
right
to
a
high
level
of
environmental
protection
enshrined
in
the
Charter
and
implemented
in
Union
policies
should
also
be
considered
when
assessing
the
sever
ity
of
the
harm
that
an
AI
system
can
cause
,
including
in
relation
to
the
health
and
safety
of
persons
.
(
49
)
As
regard
s
higher
isk
AI
systems
that
are
safety
com
ponents
of
products
or
systems
,
or
which
are
themselves
products
or
systems
falling
within
the
scope
of
Regulation
(
EC
)
No
of
the
European
Parliame
nt
and
of
the
Council
,
Regulation
(
EU
)
No
of
the
European
Parliame
nt
and
of
the
Council
,
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
,
Directive
/EU
of
the
European
Parliament
and
of
the
Council
,
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
Regulation
(
EU
)
of
the
(
24
)
Regulation
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
of
11
March
2008
on
common
rules
in
the
field
of
civil
aviation
security
and
repealing
Regulation
(
EC
)
No
(
4.2008
,
p.
72
)
.
(
25
)
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
of
5
Febr
uary
2013
on
the
approv
al
and
market
surveillance
of
agricultural
and
forestr
y
vehicles
(
3.2013
,
p.
1
)
.
(
26
)
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
of
15
Januar
y
2013
on
the
approval
and
market
surveillance
of
two-
or
three
-
wheel
vehicles
and
quadr
icycles
(
3.2013
,
p.
52
)
.
(
27
)
Directive
/EU
of
the
European
Parliament
and
of
the
Council
of
23
July
2014
on
mar
ine
equipment
and
repealing
Council
Directive
/EC
(
8.2014
,
p.
146
)
.
(
28
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
11
May
2016
on
the
interoperability
of
the
rail
system
within
the
European
Union
(
5.2016
,
p.
44
)
.
(
29
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
30
May
2018
on
the
approval
and
market
surveillance
of
motor
vehicles
and
their
trailers
,
and
of
systems
,
componen
ts
and
separate
technical
units
intended
for
such
vehicles
,
amending
Regulations
(
EC
)
No
and
(
EC
)
No
and
repealing
Directive
/EC
(
6.2018
,
p.
1).European
Parliament
and
of
the
Council
,
and
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
it
is
appropriate
to
amend
those
acts
to
ensure
that
the
Commission
takes
into
account
,
on
the
basis
of
the
technical
and
regulato
ry
specificities
of
each
secto
r
,
and
without
interfer
ing
with
existing
gover
nance
,
conf
ormity
assessment
and
enforcement
mechanisms
and
authorities
established
therein
,
the
mandato
ry
requirements
for
higher
isk
AI
systems
laid
down
in
this
Regulation
when
adopti
ng
any
relevant
deleg
ated
or
imp
lementing
acts
on
the
basis
of
those
acts
.
(
50
)
As
regards
AI
systems
that
are
safety
comp
onents
of
products
,
or
which
are
themselves
products
,
falling
within
the
scope
of
certain
Union
harmonisation
legislation
listed
in
an
annex
to
this
Regulation
,
it
is
appropriate
to
classify
them
as
higher
isk
under
this
Regulation
if
the
product
concer
ned
undergoes
the
conf
ormity
assessment
procedure
with
a
third
-
par
ty
conf
ormity
assessment
body
pursuant
to
that
relevant
Union
harmonisation
legislation
.
In
particular
,
such
products
are
machi
nery
,
toys
,
lifts
,
equipment
and
prot
ective
systems
intended
for
use
in
pote
ntially
explosive
atmospheres
,
radio
equipment
,
pressure
equipment
,
recreational
craft
equipment
,
cablewa
y
installations
,
appliances
burning
gaseous
fuels
,
medical
devices
,
in
vitro
diagnostic
medical
devices
,
automot
i
ve
and
aviation
.
(
51
)
The
classif
ication
of
an
AI
syste
m
as
higher
isk
pursuant
to
this
Regulation
should
not
necessar
ily
mean
that
the
product
whose
safety
component
is
the
AI
system
,
or
the
AI
system
itself
as
a
product
,
is
considered
to
be
higher
isk
under
the
criteria
established
in
the
relevant
Union
harmonisation
legislation
that
applies
to
the
product
.
This
is
,
in
particular
,
the
case
for
Regulations
(
EU
)
and
(
EU
)
,
where
a
third
-
par
ty
conf
ormity
assessment
is
provided
for
medium
-
r
isk
and
higher
isk
products
.
(
52
)
As
rega
rds
stand
-
alone
AI
systems
,
namely
higher
isk
AI
systems
other
than
those
that
are
safety
com
ponents
of
products
,
or
that
are
themselves
products
,
it
is
appropriate
to
classify
them
as
higher
isk
if
,
in
light
of
their
intended
purpose
,
they
pose
a
high
risk
of
harm
to
the
health
and
safety
or
the
fundamental
rights
of
persons
,
taking
into
account
both
the
sever
ity
of
the
possible
harm
and
its
probability
of
occur
rence
and
they
are
used
in
a
number
of
specifically
pre
-
defi
ned
areas
specified
in
this
Regulation
.
The
identification
of
those
systems
is
based
on
the
same
methodology
and
criteria
envisag
ed
also
for
any
future
amendments
of
the
list
of
higher
isk
AI
systems
that
the
Commission
should
be
empo
wered
to
adop
t
,
via
delegat
ed
acts
,
to
take
into
account
the
rapid
pace
of
technological
development
,
as
well
as
the
potential
change
s
in
the
use
of
AI
systems
.
(
53
)
It
is
also
impor
tant
to
clarify
that
there
may
be
specific
cases
in
which
AI
systems
refer
red
to
in
pre
-
define
d
areas
specified
in
this
Regulation
do
not
lead
to
a
signifi
ca
nt
risk
of
harm
to
the
lega
l
intere
sts
protected
under
those
areas
because
they
do
not
materi
ally
influence
the
decision
-
making
or
do
not
harm
those
interests
substantially
.
For
the
purposes
of
this
Regulation
,
an
AI
system
that
does
not
mat
erially
influence
the
outcome
of
decision
-
making
should
be
understood
to
be
an
AI
syste
m
that
does
not
have
an
impact
on
the
substance
,
and
thereby
the
outcome
,
of
decision
-
making
,
whether
human
or
automa
ted
.
An
AI
system
that
does
not
materially
influence
the
outcome
of
decision
-
making
could
include
situations
in
which
one
or
more
of
the
follo
wing
conditions
are
fulfilled
.
The
first
such
condition
should
be
that
the
AI
syste
m
is
intende
d
to
perfo
rm
a
narrow
procedural
task
,
such
as
an
AI
system
that
transf
orms
unstr
uctured
data
into
structured
data
,
an
AI
system
that
classif
ies
incoming
documents
into
cate
gories
or
an
AI
syste
m
that
is
used
to
detect
duplicate
s
among
a
large
number
of
applications
.
Those
tasks
are
of
such
narrow
and
limited
nature
that
they
pose
only
limited
risks
which
are
not
increased
through
the
use
of
an
AI
syste
m
in
a
cont
ext
that
is
listed
as
a
higher
isk
use
in
an
annex
to
this
Regulation
.
The
second
condition
should
be
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
4
July
2018
on
common
rules
in
the
field
of
civil
aviation
and
establishing
a
European
Union
Aviation
Safety
Agency
,
and
amending
Regulations
(
EC
)
No
,
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
and
Directives
/EU
and
/EU
of
the
European
Parliament
and
of
the
Council
,
and
repealing
Regulations
(
EC
)
No
and
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
and
Council
Regulation
(
EEC
)
No
(
8.2018
,
p.
1
)
.
(
31
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
27
November
2019
on
type
-
approval
requirements
for
motor
vehicles
and
their
trailers
,
and
systems
,
components
and
separate
technical
units
intended
for
such
vehicles
,
as
regards
their
general
safety
and
the
protection
of
vehicle
occupants
and
vulnerable
road
users
,
amending
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
repealing
Regulations
(
EC
)
No
,
(
EC
)
No
and
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
and
Commission
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
and
(
EU
)
(
12.2019
,
p.
1).that
the
task
perfo
rmed
by
the
AI
system
is
intended
to
imp
rove
the
result
of
a
previously
comp
leted
human
activity
that
may
be
relevant
for
the
purposes
of
the
higher
isk
uses
listed
in
an
annex
to
this
Regulation
.
Consider
ing
those
characteristics
,
the
AI
system
provid
es
only
an
additional
layer
to
a
human
activity
with
consequently
lower
ed
risk
.
That
condition
would
,
for
example
,
apply
to
AI
systems
that
are
intended
to
impro
ve
the
language
used
in
previously
drafted
documents
,
for
example
in
relation
to
profe
ssional
tone
,
academic
style
of
languag
e
or
by
aligning
text
to
a
certain
brand
messaging
.
The
third
condition
should
be
that
the
AI
syste
m
is
intended
to
detect
decision
-
making
patt
erns
or
deviations
from
prior
decision
-
making
patterns
.
The
risk
would
be
lower
ed
because
the
use
of
the
AI
syste
m
follo
ws
a
previously
complet
ed
human
assessment
which
it
is
not
meant
to
replace
or
influence
,
without
proper
human
review
.
Such
AI
systems
include
for
instance
those
that
,
given
a
certain
grading
patt
ern
of
a
teac
her
,
can
be
used
to
check
ex
post
whether
the
teacher
may
have
deviated
from
the
grading
pattern
so
as
to
flag
potent
ial
inconsiste
ncies
or
anomalies
.
The
fourth
condition
should
be
that
the
AI
syste
m
is
intended
to
perf
orm
a
task
that
is
only
preparat
ory
to
an
assessment
relevant
for
the
purposes
of
the
AI
systems
listed
in
an
annex
to
this
Regulation
,
thus
making
the
possible
imp
act
of
the
output
of
the
system
very
low
in
terms
of
representing
a
risk
for
the
assessment
to
follow
.
That
condition
covers
,
inter
alia
,
smar
t
solutions
for
file
handling
,
which
include
various
functions
from
indexing
,
search
ing
,
text
and
speech
processing
or
linking
data
to
other
data
sources
,
or
AI
systems
used
for
translation
of
initial
documents
.
In
any
case
,
AI
systems
used
in
higher
isk
use
-
cases
listed
in
an
annex
to
this
Regulation
should
be
considered
to
pose
signifi
ca
nt
risks
of
harm
to
the
health
,
safety
or
fundamental
rights
if
the
AI
syste
m
imp
lies
profiling
within
the
meaning
of
Article
4
,
point
of
Regulation
(
EU
)
or
Article
3
,
point
of
Directive
(
EU
)
or
Article
3
,
point
of
Regulation
(
EU
)
.
To
ensure
traceability
and
transparency
,
a
provid
er
who
considers
that
an
AI
syste
m
is
not
higher
isk
on
the
basis
of
the
conditions
referred
to
above
should
draw
up
documentation
of
the
assessment
before
that
syste
m
is
placed
on
the
market
or
put
into
service
and
should
provide
that
documentation
to
national
competent
authorities
upon
request
.
Such
a
provid
er
should
be
obliged
to
register
the
AI
syste
m
in
the
EU
database
established
under
this
Regulation
.
With
a
view
to
providing
further
guidance
for
the
practical
imp
lementation
of
the
conditions
under
which
the
AI
systems
listed
in
an
annex
to
this
Regulation
are
,
on
an
excep
tional
basis
,
non
-
higher
isk
,
the
Commission
should
,
after
consulting
the
Board
,
provid
e
guidelines
specifying
that
practical
implementation
,
comp
leted
by
a
compre
hensive
list
of
practical
examples
of
use
cases
of
AI
systems
that
are
higher
isk
and
use
cases
that
are
not
.
(
54
)
As
biometric
data
constitute
s
a
special
categor
y
of
personal
data
,
it
is
appropriate
to
classify
as
higher
isk
several
critical
-
use
cases
of
biometric
systems
,
insofar
as
their
use
is
permitte
d
under
relevant
Union
and
national
law
.
Technical
inaccuracies
of
AI
systems
intende
d
for
the
remote
biometric
identifica
tion
of
natural
persons
can
lead
to
biased
results
and
entail
discr
iminat
ory
effects
.
The
risk
of
such
biased
results
and
discr
iminat
ory
effects
is
particularly
relevant
with
regard
to
age
,
ethnicity
,
race
,
sex
or
disabilities
.
Remote
biometric
identifica
tion
systems
should
theref
ore
be
classif
ied
as
higher
isk
in
view
of
the
risks
that
they
pose
.
Such
a
classif
ication
excludes
AI
systems
intended
to
be
used
for
biometric
verification
,
including
authentication
,
the
sole
purpose
of
which
is
to
confir
m
that
a
specific
natural
person
is
who
that
person
claims
to
be
and
to
confir
m
the
identity
of
a
natural
person
for
the
sole
purpose
of
having
access
to
a
service
,
unlocking
a
device
or
having
secure
access
to
premises
.
In
addition
,
AI
systems
intended
to
be
used
for
biometric
categor
isation
according
to
sensitive
attribut
es
or
character
istics
prot
ected
under
Article
9(1
)
of
Regulation
(
EU
)
on
the
basis
of
biometric
data
,
in
so
far
as
these
are
not
prohibite
d
under
this
Regulation
,
and
emotion
recognition
systems
that
are
not
prohibite
d
under
this
Regulation
,
should
be
classif
ied
as
higher
isk
.
biometric
systems
which
are
intende
d
to
be
used
solely
for
the
purpose
of
enabling
cybersecurity
and
personal
data
protection
measures
should
not
be
considered
to
be
higher
isk
AI
systems
.
(
55
)
As
regards
the
management
and
operation
of
critical
infrastructure
,
it
is
appropriate
to
classify
as
higher
isk
the
AI
systems
intended
to
be
used
as
safety
components
in
the
managem
ent
and
operation
of
critical
digital
infrastructure
as
listed
in
point
of
the
Annex
to
Directive
(
EU
)
,
road
traffic
and
the
supply
of
wate
r
,
gas
,
heating
and
electr
icity
,
since
their
failure
or
malfunctioning
may
put
at
risk
the
life
and
health
of
persons
at
large
scale
and
lead
to
appreciable
disruptions
in
the
ordinary
conduct
of
social
and
economic
activities
.
Safety
compo
nents
of
critical
infrastructure
,
including
critical
digital
infrastructure
,
are
systems
used
to
directly
protect
the
physica
l
integrity
of
critical
infrastructure
or
the
health
and
safety
of
persons
and
proper
ty
but
which
are
not
necessary
in
order
for
the
integrity
of
critical
infrastructure
and
thus
to
risks
to
health
and
safety
of
persons
and
proper
ty
.
Compo
nents
intended
to
be
used
solely
for
cybersecurity
purposes
should
not
qualify
as
safety
compo
nents
.
Examples
of
safety
com
ponents
of
such
critical
infrastructure
may
include
systems
for
monitoring
wate
r
pressure
or
fire
alarm
controlling
systems
in
cloud
comp
uting
centres
.
(
56
)
The
deplo
yment
of
AI
systems
in
education
is
impor
tant
to
promot
e
high
-
quality
digital
education
and
training
and
to
allow
all
learners
and
teache
rs
to
acquire
and
share
the
necessary
digital
skills
and
comp
etences
,
including
media
literacy
,
and
critical
thinking
,
to
take
an
active
part
in
the
economy
,
society
,
and
in
democratic
processes
.
How
ever
,
AI
systems
used
in
education
or
vocational
training
,
in
particular
for
determining
access
or
admission
,
for
assigning
persons
to
educational
and
vocational
training
institutions
or
programmes
at
all
levels
,
for
evaluating
learning
outcomes
of
persons
,
for
assessing
the
appropriate
level
of
education
for
an
individual
and
mat
erially
influencing
the
level
of
education
and
training
that
individuals
will
receive
or
will
be
able
to
access
or
for
monitoring
and
detecting
prohibite
d
behaviour
of
students
during
tests
should
be
classif
ied
as
higher
isk
AI
systems
,
since
they
may
determi
ne
the
educational
and
profe
ssional
course
of
a
person
’s
life
and
theref
ore
may
affect
that
person
’s
ability
to
secure
a
livelihood
.
When
improperly
designed
and
used
,
such
systems
may
be
particularly
intrusive
and
may
violate
the
right
to
education
and
training
as
well
as
the
right
not
to
be
discr
iminated
against
and
perpetuate
histor
ical
patt
erns
of
discr
imination
,
for
example
against
women
,
certain
age
groups
,
persons
with
disabilities
,
or
persons
of
certain
racial
or
ethnic
origins
or
sexual
orientation
.
(
57
)
AI
systems
used
in
employment
,
workers
management
and
access
to
self
-
employment
,
in
particular
for
the
recr
uitment
and
selection
of
persons
,
for
making
decisions
affecting
terms
of
the
work
-related
relationship
,
promotion
and
term
ination
of
work
-
relate
d
contractual
relationships
,
for
allocating
tasks
on
the
basis
of
individual
behavi
our
,
personal
traits
or
character
istics
and
for
monitoring
or
evaluation
of
persons
in
work
-relate
d
contractual
relationships
,
should
also
be
classif
ied
as
higher
isk
,
since
those
systems
may
have
an
appreciable
impact
on
future
career
prospects
,
livelihoods
of
those
persons
and
work
ers
’
rights
.
Relevant
work
-
related
contractual
relationships
should
,
in
a
meaningful
manner
,
involve
emp
loyees
and
persons
provid
ing
services
through
platforms
as
referred
to
in
the
Commission
Work
Programme
2021
.
Throughout
the
recr
uitment
process
and
in
the
evaluation
,
promotion
,
or
retention
of
persons
in
work
-
related
contractual
relationships
,
such
systems
may
perpetuat
e
historical
patterns
of
discr
imination
,
for
example
against
women
,
certain
age
groups
,
persons
with
disabilities
,
or
persons
of
certain
racial
or
ethnic
origins
or
sexual
orientation
.
AI
systems
used
to
monitor
the
perf
ormance
and
behaviour
of
such
persons
may
also
under
mine
their
fundamental
rights
to
data
protection
and
privacy
.
(
58
)
Another
area
in
which
the
use
of
AI
systems
deser
ves
special
consideration
is
the
access
to
and
enjo
yment
of
certain
essential
private
and
public
services
and
benef
its
necessary
for
people
to
fully
participate
in
society
or
to
imp
rove
one
’s
standard
of
living
.
In
particular
,
natural
persons
applying
for
or
receiving
essential
public
assistance
benef
its
and
services
from
public
authorities
namely
healthcare
services
,
social
security
benef
its
,
social
services
providing
protection
in
cases
such
as
maternity
,
illness
,
industr
ial
accidents
,
dependency
or
old
age
and
loss
of
emp
loyment
and
social
and
housing
assistance
,
are
typically
dependent
on
those
benef
its
and
services
and
in
a
vulnerable
position
in
relation
to
the
responsible
authorities
.
If
AI
systems
are
used
for
determining
whether
such
benefits
and
services
should
be
granted
,
denied
,
reduced
,
revok
ed
or
reclaimed
by
authorities
,
including
whether
beneficiar
ies
are
legitimat
ely
entitled
to
such
benefi
ts
or
services
,
those
systems
may
have
a
significant
impact
on
persons
’
livelihood
and
may
infringe
their
fundamental
rights
,
such
as
the
right
to
social
protection
,
non
-
discr
imination
,
human
dignity
or
an
effective
remedy
and
should
theref
ore
be
classif
ied
as
higher
isk
.
Nonetheless
,
this
Regulation
should
not
ham
per
the
development
and
use
of
innovative
approac
he
s
in
the
public
administration
,
which
would
stand
to
benefit
from
a
wider
use
of
compliant
and
safe
AI
systems
,
provid
ed
that
those
systems
do
not
entail
a
high
risk
to
legal
and
natural
persons
.
In
addition
,
AI
systems
used
to
evaluate
the
credit
score
or
creditwor
thiness
of
natural
persons
should
be
classif
ied
as
higher
isk
AI
systems
,
since
they
determi
ne
those
persons
’
access
to
financ
ial
resources
or
essential
services
such
as
housing
,
electr
icity
,
and
telecommunication
services
.
AI
systems
used
for
those
purposes
may
lead
to
discr
imination
between
persons
or
groups
and
may
perpetuat
e
histor
ical
patt
erns
of
discr
imination
,
such
as
that
based
on
racial
or
ethnic
origins
,
gender
,
disabilities
,
age
or
sexual
orientation
,
or
may
create
new
forms
of
discr
iminatory
impacts
.
However
,
AI
systems
provid
ed
for
by
Union
law
for
the
purpose
of
detecting
fraud
in
the
offering
of
financ
ial
services
and
for
prudential
purposes
to
calculate
credit
institutions
’
and
insurance
undertakings
’
capital
requirements
should
not
be
considered
to
be
higher
isk
under
this
Regulation
.
Moreove
r
,
AI
systems
intended
be
used
for
risk
assessment
and
pricing
in
relation
to
natural
persons
for
health
and
life
insurance
can
also
have
a
significant
impact
on
persons
’
livelihood
and
if
not
duly
designed
,
developed
and
used
,
can
infringe
their
fundamental
rights
and
can
lead
to
serious
consequences
for
people
’s
life
and
health
,
including
financ
ial
exclusion
and
discr
imination
.
Finally
,
AI
systems
used
to
evaluate
and
classify
emerge
ncy
calls
by
natural
persons
or
to
dispatc
h
or
establish
priority
in
the
dispatch
ing
of
emerg
ency
first
response
services
,
including
by
police
,
firefighters
and
medical
aid
,
as
well
as
of
emerge
ncy
healthcare
patient
triage
systems
,
should
also
be
classif
ied
as
higher
isk
since
they
make
decisions
in
very
critical
situations
for
the
life
and
health
of
persons
and
their
proper
ty
.
(
59
)
Given
their
role
and
responsibility
,
actions
by
law
enforcement
authorities
involving
certain
uses
of
AI
systems
are
characterised
by
a
signifi
ca
nt
degree
of
power
imbalance
and
may
lead
to
surveillance
,
arrest
or
depr
ivation
of
a
natural
person
’s
liber
ty
as
well
as
other
adverse
impacts
on
fundamental
rights
guaranteed
in
the
Charter
.
In
particular
,
if
the
AI
syste
m
is
not
trained
with
high
-
quality
data
,
does
not
meet
adequate
requirements
in
terms
of
its
perf
ormance
,
its
accuracy
or
robustness
,
or
is
not
properly
designed
and
tested
before
being
put
on
the
market
or
other
wise
put
into
service
,
it
may
sing
le
out
people
in
a
discr
iminat
ory
or
other
wise
incor
rect
or
unjust
manner
.
Further
more
,
the
exercise
of
imp
ortant
procedural
fundamental
rights
,
such
as
the
right
to
an
effective
remedy
and
to
a
fair
trial
as
well
as
the
right
of
defence
and
the
presump
tion
of
innocence
,
could
be
hampered
,
in
particular
,
where
such
AI
systems
are
not
suffi
ciently
transparent
,
explainable
and
documente
d.
It
is
theref
ore
appropriate
to
classify
as
higher
isk
,
insofa
r
as
their
use
is
permitte
d
under
relevant
Union
and
national
law
,
a
number
of
AI
systems
intended
to
be
used
in
the
law
enforcement
cont
ext
where
accuracy
,
reliability
and
transparency
is
particularly
important
to
avoid
adverse
impacts
,
retain
public
trust
and
ensure
accountability
and
effective
redress
.
In
view
of
the
nature
of
the
activities
and
the
risks
relating
thereto
,
those
higher
isk
AI
systems
should
include
in
particular
AI
systems
intended
to
be
used
by
or
on
behalf
of
law
enforcement
authorities
or
by
Union
institutions
,
bodies
,
offices
,
or
agencies
in
support
of
law
enforcement
authorities
for
assessing
the
risk
of
a
natural
person
to
become
a
victim
of
criminal
offenc
es
,
as
polygraphs
and
similar
tools
,
for
the
evaluation
of
the
reliability
of
evidence
in
in
the
course
of
investig
ation
or
prosecution
of
criminal
offences
,
and
,
insofar
as
not
prohibited
under
this
Regulation
,
for
assessing
the
risk
of
a
natural
person
offending
or
reoffe
nding
not
solely
on
the
basis
of
the
prof
iling
of
natural
persons
or
the
assessment
of
personality
traits
and
character
istics
or
the
past
criminal
behavio
ur
of
natural
persons
or
groups
,
for
profiling
in
the
course
of
detection
,
investig
ation
or
prosecution
of
criminal
offences
.
AI
systems
specifically
intended
to
be
used
for
administrative
proceedings
by
tax
and
custo
ms
authorities
as
well
as
by
financial
intellig
ence
units
carrying
out
administrative
tasks
analysing
information
pursuant
to
Union
anti
-
money
launder
ing
law
should
not
be
classif
ied
as
higher
isk
AI
systems
used
by
law
enforcement
authorities
for
the
purpose
of
prevention
,
detection
,
investig
ation
and
prosecution
of
criminal
offenc
es
.
The
use
of
AI
tools
by
law
enforcement
and
other
relevant
authorities
should
not
become
a
factor
of
inequality
,
or
exclus
ion
.
The
imp
act
of
the
use
of
AI
tools
on
the
defe
nce
rights
of
suspects
should
not
be
ignored
,
in
particular
the
difficulty
in
obtaining
meaningful
information
on
the
functioning
of
those
systems
and
the
resulting
difficulty
in
challenging
their
results
in
cour
t
,
in
particular
by
natural
persons
under
investig
ation
.
(
60
)
AI
systems
used
in
migration
,
asylum
and
border
control
managem
ent
affect
persons
who
are
often
in
particularly
vulnerable
position
and
who
are
dependent
on
the
outcome
of
the
actions
of
the
comp
etent
public
authorities
.
The
accuracy
,
non
-
discr
iminat
ory
nature
and
transparency
of
the
AI
systems
used
in
those
contexts
are
theref
ore
particularly
important
to
guarante
e
respect
for
the
fundamental
rights
of
the
affected
persons
,
in
particular
their
rights
to
free
movement
,
non
-
discr
imination
,
protection
of
private
life
and
personal
data
,
international
protection
and
good
administration
.
It
is
theref
ore
appropriate
to
classify
as
higher
isk
,
insofa
r
as
their
use
is
permitted
under
relevant
Union
and
national
law
,
AI
systems
intended
to
be
used
by
or
on
behalf
of
competent
public
authorities
or
by
Union
institutions
,
bodies
,
offices
or
agencies
charged
with
tasks
in
the
fields
of
migration
,
asylum
and
border
control
management
as
polygraphs
and
similar
tools
,
for
assessing
certain
risks
posed
by
natural
persons
entering
the
territory
of
a
Member
State
or
applying
for
visa
or
asylum
,
for
assisting
competent
public
authorities
for
the
examination
,
including
related
assessment
of
the
reliability
of
evidence
,
of
applications
for
asylum
,
visa
and
residence
permits
and
associat
ed
comp
laints
with
regard
to
the
objective
to
establish
the
eligibility
of
the
natural
persons
applying
for
a
status
,
for
the
purpose
of
detecting
,
recognising
or
identifying
natural
persons
in
the
context
of
migration
,
asylum
and
border
control
management
,
with
the
excep
tion
of
verification
of
travel
documents
.
AI
systems
in
the
area
of
migration
,
asylum
and
border
control
management
covered
by
this
Regulation
should
comply
with
the
relevant
procedural
requirements
set
by
the
Regulation
(
EC
)
No
of
the
European
Parliament
and
Union
law
.
The
use
of
AI
systems
in
migration
,
asylum
and
border
control
management
should
,
in
no
circumstances
,
be
used
by
Member
States
or
Union
institutions
,
bodies
,
offices
or
agencies
as
a
means
to
circum
vent
their
international
obligations
under
the
UN
Conve
ntion
relating
to
the
Status
of
Refuge
es
done
at
Geneva
on
28
July
principle
of
non
-
ref
oulement
,
or
to
deny
safe
and
effective
legal
avenues
into
the
territory
of
the
Union
,
including
the
right
to
inter
national
protection
.
(
61
)
Certain
AI
systems
intended
for
the
administration
of
justice
and
democratic
processes
should
be
classified
as
higher
isk
,
consider
ing
their
potent
ially
signifi
ca
nt
impact
on
democracy
,
the
rule
of
law
,
individual
freedoms
as
well
as
the
right
to
an
effective
remedy
and
to
a
fair
trial
.
In
particular
,
to
address
the
risks
of
potential
biases
,
errors
and
opacity
,
it
is
appropriate
to
qualify
as
higher
isk
AI
systems
intended
to
be
used
by
a
judicial
author
ity
or
on
its
behalf
to
assist
judicial
authorities
in
researching
and
interpreting
facts
and
the
law
and
in
applying
the
law
to
a
concrete
set
of
facts
.
AI
systems
intended
to
be
used
by
alternative
dispute
resolution
bodies
for
those
purposes
should
also
be
considered
to
be
higher
isk
when
the
outcomes
of
the
alter
native
dispute
resolution
proceedings
produce
legal
effects
for
the
parties
.
The
use
of
AI
tools
can
support
the
decision
-
making
power
of
judges
or
judicial
independence
,
but
should
not
replace
it
:
the
final
decision
-
making
must
remain
a
human
-
dr
iven
activity
.
The
classification
of
AI
systems
as
higher
isk
should
not
,
however
,
extend
to
AI
systems
intended
for
purely
ancillar
y
administrative
activities
that
do
not
affect
the
actual
administration
of
justice
in
individual
cases
,
such
as
anonymisa
tion
or
pseudon
ymisation
of
judicial
decisions
,
documents
or
data
,
communication
between
personnel
,
administrative
tasks
.
(
62
)
Without
prejudice
to
the
rules
provided
for
in
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
and
in
order
to
address
the
risks
of
undue
exte
rnal
interfere
nce
with
the
right
to
vote
enshrined
in
Article
39
of
the
Charter
,
and
of
adverse
effects
on
democracy
and
the
rule
of
law
,
AI
systems
intended
to
be
used
to
influence
the
outcome
of
an
election
or
referendum
or
the
voting
behaviour
of
natural
persons
in
the
exercise
of
their
vote
in
elections
or
referenda
should
be
classif
ied
as
higher
isk
AI
systems
with
the
exception
of
AI
systems
whose
output
natural
persons
are
not
directly
exposed
to
,
such
as
tools
used
to
organise
,
optimise
and
structure
political
cam
paigns
from
an
administrative
and
logistical
point
of
view
.
(
63
)
The
fact
that
an
AI
syste
m
is
classif
ied
as
a
higher
isk
AI
system
under
this
Regulation
should
not
be
inter
preted
as
indicating
that
the
use
of
the
system
is
lawful
under
other
acts
of
Union
law
or
under
national
law
compatible
with
Union
law
,
such
as
on
the
protection
of
personal
data
,
on
the
use
of
polygraphs
and
similar
tools
or
other
systems
to
detect
the
emotional
state
of
natural
persons
.
Any
such
use
should
continue
to
occur
solely
in
accordance
with
the
applicable
requirements
resulting
from
the
Charter
and
from
the
applicable
acts
of
secondar
y
Union
law
and
national
law
.
This
Regulation
should
not
be
understood
as
provid
ing
for
the
lega
l
ground
for
processing
of
personal
data
,
including
special
categor
ies
of
personal
data
,
where
relevant
,
unless
it
is
specifically
other
wise
provid
ed
for
in
this
Regulation
.
(
64
)
To
mitigat
e
the
risks
from
higher
isk
AI
systems
placed
on
the
market
or
put
into
service
and
to
ensure
a
high
level
of
trustwo
rthiness
,
certain
mandat
ory
requirements
should
apply
to
higher
isk
AI
systems
,
taking
into
account
the
intended
purpose
and
the
cont
ext
of
use
of
the
AI
syste
m
and
according
to
the
risk
-
management
system
to
be
established
by
the
provider
.
The
measures
adop
ted
by
the
providers
to
comply
with
the
mandato
ry
requirements
of
this
Regulation
should
take
into
account
the
generally
ackno
wledged
state
of
the
art
on
AI
,
be
propor
tionate
and
effective
to
meet
the
objectives
of
this
Regulation
.
Based
on
the
New
Legislative
Framework
,
as
clarified
in
Commission
notice
‘
The
“
Blue
Guide
”
on
the
implementation
of
EU
product
rules
2022
’
,
the
general
rule
is
that
more
than
one
legal
act
of
Union
harmonisation
legislation
may
be
applicable
to
one
product
,
since
the
making
available
or
putting
into
service
can
take
place
only
when
the
product
complie
s
with
all
applicable
Union
harmonisation
legislation
.
The
hazards
of
AI
systems
covered
by
the
requirements
of
this
Regulation
concer
n
different
aspects
than
the
existing
Union
harmonisation
legislation
and
theref
ore
the
requirements
of
this
Regulation
would
complement
the
existing
body
of
the
Union
harmonisation
legislation
.
For
example
,
machi
nery
or
medical
devices
products
incor
porating
an
AI
system
might
present
risks
not
addressed
by
the
essential
health
and
safety
Regulation
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
of
13
July
2009
establishing
a
Community
Code
on
Visas
(
Visa
Code
)
(
9.2009
,
p.
1
)
.
(
33
)
Directive
/EU
of
the
European
Parliament
and
of
the
Council
of
26
June
2013
on
common
procedures
for
granting
and
withdraw
ing
international
protection
(
6.2013
,
p.
60
)
.
(
34
)
Regulation
(
EU
)
of
the
European
parliament
and
of
the
Council
of
13
March
2024
on
the
transparency
and
targe
ting
of
political
adver
tising.requirements
set
out
in
the
relevant
Union
harmonised
legislation
,
as
that
sectoral
law
does
not
deal
with
risks
specific
to
AI
systems
.
This
calls
for
a
simultaneous
and
comp
lementar
y
application
of
the
various
legislative
acts
.
To
ensure
consist
ency
and
to
avoid
an
unnecessary
administrative
burden
and
unnecessary
costs
,
provid
ers
of
a
product
that
contains
one
or
more
higher
isk
AI
system
,
to
which
the
requirements
of
this
Regulation
and
of
the
Union
harmonisation
legislation
based
on
the
New
Legislative
Framewo
rk
and
listed
in
an
annex
to
this
Regulation
apply
,
should
have
flexibility
with
regard
to
operational
decisions
on
how
to
ensure
compliance
of
a
product
that
contains
one
or
more
AI
systems
with
all
the
applicable
requirements
of
that
Union
harmonised
legislation
in
an
optimal
manner
.
That
flexibility
could
mean
,
for
example
a
decision
by
the
provider
to
integrate
a
part
of
the
necessary
testing
and
repor
ting
processes
,
information
and
documentation
required
under
this
Regulation
into
already
existing
documentation
and
procedures
required
under
existing
Union
harmonisation
legislation
based
on
the
New
Legislative
Framework
and
listed
in
an
annex
to
this
Regulation
.
This
should
not
,
in
any
way
,
under
mine
the
oblig
ation
of
the
provid
er
to
comply
with
all
the
applicable
requirements
.
(
65
)
The
risk
-
managem
ent
syste
m
should
consist
of
a
continuous
,
iterative
process
that
is
planned
and
run
throughout
the
entire
lifecycle
of
a
higher
isk
AI
syste
m.
That
process
should
be
aimed
at
identifying
and
mitigating
the
relevant
risks
of
AI
systems
on
health
,
safety
and
fundamental
rights
.
The
risk
-
management
system
should
be
regularly
reviewed
and
update
d
to
ensure
its
continuing
effectiveness
,
as
well
as
justifi
cation
and
documentation
of
any
significant
decisions
and
actions
take
n
subject
to
this
Regulation
.
This
process
should
ensure
that
the
provid
er
identifies
risks
or
adverse
impacts
and
imp
lements
mitigation
measures
for
the
kno
wn
and
reasonably
foreseeable
risks
of
AI
systems
to
the
health
,
safety
and
fundamental
rights
in
light
of
their
intended
purpose
and
reasonably
foreseeable
misuse
,
including
the
possible
risks
arising
from
the
interaction
between
the
AI
system
and
the
environment
within
which
it
operates
.
The
risk
-
management
syste
m
should
adop
t
the
most
appropriate
risk
-
management
measures
in
light
of
the
state
of
the
art
in
AI
.
When
identifying
the
most
appropriate
risk
-
management
measures
,
the
provid
er
should
document
and
explain
the
choices
made
and
,
when
relevant
,
involve
exper
ts
and
exter
nal
stak
eholders
.
In
identifying
the
reasonably
foreseeable
misuse
of
higher
isk
AI
systems
,
the
provid
er
should
cover
uses
of
AI
systems
which
,
while
not
directly
covered
by
the
intende
d
purpose
and
provided
for
in
the
instr
uction
for
use
may
never
theless
be
reasonably
expected
to
result
from
readily
predictable
human
behavi
our
in
the
cont
ext
of
the
specific
charact
eristics
and
use
of
a
particular
AI
syste
m.
Any
kno
wn
or
foreseeable
circumstances
related
to
the
use
of
the
higher
isk
AI
syste
m
in
accordance
with
its
intended
purpose
or
under
conditions
of
reasonably
foreseeable
misuse
,
which
may
lead
to
risks
to
the
health
and
safety
or
fundamental
rights
should
be
included
in
the
instr
uctions
for
use
that
are
provid
ed
by
the
provider
.
This
is
to
ensure
that
the
deplo
yer
is
aware
and
takes
them
into
account
when
using
the
higher
isk
AI
syste
m.
Identifying
and
implementing
risk
mitigation
measures
for
foreseeable
misuse
under
this
Regulation
should
not
require
specific
additional
training
for
the
higher
isk
AI
syste
m
by
the
provid
er
to
address
foreseeable
misuse
.
The
provider
s
however
are
encourage
d
to
consider
such
additional
training
measures
to
mitiga
te
reasonable
foreseeable
misuses
as
necessary
and
appropriate
.
(
66
)
Requirements
should
apply
to
higher
isk
AI
systems
as
regard
s
risk
management
,
the
quality
and
relevance
of
data
sets
used
,
technical
documentation
and
record
-
k
eeping
,
transparency
and
the
provision
of
information
to
deplo
yers
,
human
oversight
,
and
robustness
,
accuracy
and
cybersecurity
.
Those
requirements
are
necessary
to
effectively
mitig
ate
the
risks
for
health
,
safety
and
fundamental
rights
.
As
no
other
less
trade
restr
ictive
measures
are
reasonably
available
those
requirements
are
not
unjustified
restrictions
to
trade
.
(
67
)
High
-
quality
data
and
access
to
high
-
quality
data
plays
a
vital
role
in
providing
structure
and
in
ensur
ing
the
perf
ormance
of
many
AI
systems
,
especially
when
techniques
involving
the
training
of
models
are
used
,
with
a
view
to
ensure
that
the
higher
isk
AI
syste
m
performs
as
intended
and
safely
and
it
does
not
become
a
source
of
discr
imination
prohibite
d
by
Union
law
.
High
-
quality
data
sets
for
training
,
validation
and
testing
require
the
implementation
of
appropriate
data
gover
nance
and
managem
ent
practices
.
Data
sets
for
training
,
validation
and
testing
,
including
the
labels
,
should
be
relevant
,
sufficiently
representative
,
and
to
the
best
extent
possible
free
of
errors
and
complet
e
in
view
of
the
intended
purpose
of
the
system
.
In
order
to
facilitate
compliance
with
Union
data
protection
law
,
such
as
Regulation
(
EU
)
,
data
gove
rnance
and
management
practices
should
include
,
in
the
case
of
personal
data
,
transparency
about
the
original
purpose
of
the
data
collection
.
The
data
sets
should
also
have
the
appropriate
statistical
proper
ties
,
including
as
regard
s
the
persons
or
groups
of
persons
in
relation
to
whom
the
higher
isk
AI
syste
m
is
intended
to
be
used
,
with
specific
attention
to
the
mitigation
of
possible
biases
in
the
data
sets
,
that
are
likely
to
affect
the
health
and
safety
of
persons
,
have
a
negative
imp
act
on
fundamental
rights
or
lead
to
discr
imination
prohibite
d
under
Union
law
,
especially
where
data
outputs
influence
inputs
for
future
operations
(
feedbac
k
loops
)
.
Biases
can
for
example
be
inherent
in
underlying
data
sets
,
especially
when
histor
ical
data
is
being
used
,
or
generated
when
the
systems
are
implement
ed
in
real
world
settings
.
Results
provided
by
AI
systems
could
be
influenced
by
such
inherent
biases
that
are
inclined
to
gradually
increase
and
thereby
perpetuate
and
amplify
existing
discr
imination
,
in
particular
for
persons
belonging
to
certain
vulnerable
groups
,
including
racial
or
ethnic
groups
.
The
requirement
for
the
data
sets
to
be
to
the
best
extent
possible
complet
e
and
free
of
errors
should
not
affect
the
use
of
privacy
-
preser
ving
techniques
in
the
cont
ext
of
the
development
and
testing
of
AI
systems
.
In
particular
,
data
sets
should
take
into
account
,
to
the
extent
required
by
their
intended
purpose
,
the
features
,
character
istics
or
elements
that
are
particular
to
the
specific
geographical
,
contextual
,
behavi
oural
or
functional
setting
which
the
AI
syste
m
is
intended
to
be
used
.
The
requirements
related
to
data
gove
rnance
can
be
complied
with
by
having
recourse
to
third
parties
that
offer
certified
compliance
services
including
verification
of
data
gover
nance
,
data
set
integrity
,
and
data
training
,
validation
and
testing
practices
,
as
far
as
compliance
with
the
data
requirements
of
this
Regulation
are
ensured
.
(
68
)
For
the
development
and
assessment
of
higher
isk
AI
systems
,
certain
actors
,
such
as
provid
ers
,
notif
ied
bodies
and
other
relevant
entities
,
such
as
European
Digital
Innovation
Hubs
,
testing
exper
imentation
facilities
and
researchers
,
should
be
able
to
access
and
use
high
-
quality
data
sets
within
the
fields
of
activities
of
those
actor
s
which
are
related
to
this
Regulation
.
European
common
data
spaces
established
by
the
Commission
and
the
facilitation
of
data
shar
ing
between
businesses
and
with
gover
nment
in
the
public
intere
st
will
be
instr
umental
to
provide
trustful
,
accountable
and
non
-
discr
iminat
ory
access
to
high
-
quality
data
for
the
training
,
validation
and
testing
of
AI
systems
.
For
example
,
in
health
,
the
European
health
data
space
will
facilitat
e
non
-
discr
iminatory
access
to
health
data
and
the
training
of
AI
algor
ithms
on
those
data
sets
,
in
a
privacy
-
preser
ving
,
secure
,
timely
,
transparent
and
trustw
orthy
manner
,
and
with
an
appropriate
institutional
gove
rnance
.
Relevant
comp
etent
authorities
,
including
sectoral
ones
,
providing
or
supporting
the
access
to
data
may
also
support
the
provision
of
high
-
quality
data
for
the
training
,
validation
and
testing
of
AI
systems
.
(
69
)
The
right
to
privacy
and
to
protection
of
personal
data
must
be
guarante
ed
throughout
the
entire
lifecycle
of
the
AI
syste
m.
In
this
regard
,
the
principles
of
data
minimisation
and
data
protection
by
design
and
by
default
,
as
set
out
in
Union
data
protect
ion
law
,
are
applicable
when
personal
data
are
processed
.
Measures
take
n
by
provid
ers
to
ensure
compliance
with
those
principles
may
include
not
only
anonymisati
on
and
encr
yption
,
but
also
the
use
of
technology
that
permits
algor
ithms
to
be
brought
to
the
data
and
allows
training
of
AI
systems
without
the
transmission
between
parties
or
copying
of
the
raw
or
structured
data
themselves
,
without
prejudice
to
the
requirements
on
data
gover
nance
provid
ed
for
in
this
Regulation
.
(
70
)
In
order
to
protect
the
right
of
others
from
the
discr
imination
that
might
result
from
the
bias
in
AI
systems
,
the
providers
should
,
exceptiona
lly
,
to
the
extent
that
it
is
strictly
necessary
for
the
purpose
of
ensur
ing
bias
detection
and
correction
in
relation
to
the
higher
isk
AI
systems
,
subject
to
appropriate
safeguards
for
the
fundamental
rights
and
freedoms
of
natural
persons
and
followi
ng
the
application
of
all
applicable
conditions
laid
down
under
this
Regulation
in
addition
to
the
conditions
laid
down
in
Regulations
(
EU
)
and
(
EU
)
and
Directive
(
EU
)
,
be
able
to
process
also
special
cate
gories
of
personal
data
,
as
a
matt
er
of
substantial
public
interest
within
the
meaning
of
Article
9(2
)
,
point
(
g
)
of
Regulation
(
EU
)
and
Article
10(2
)
,
point
(
g
)
of
Regulation
(
EU
)
.
(
71
)
Having
comp
rehensible
information
on
how
higher
isk
AI
systems
have
been
developed
and
how
they
perf
orm
throughout
their
lifetime
is
essential
to
enable
traceability
of
those
systems
,
verify
compliance
with
the
requirements
under
this
Regulation
,
as
well
as
monitoring
of
their
operations
and
post
mark
et
monitoring
.
This
requires
keeping
records
and
the
availability
of
technical
documentation
,
containing
information
which
is
necessary
to
assess
the
compliance
of
the
AI
syste
m
with
the
relevant
requirements
and
facilitate
post
market
monitorin
g.
Such
information
should
include
the
general
charact
eristics
,
capabilities
and
limitations
of
the
system
,
algor
ithms
,
data
,
training
,
testing
and
validation
processes
used
as
well
as
documentation
on
the
relevant
risk
-
management
system
and
drawn
in
a
clear
and
comp
rehensive
form
.
The
technical
documentation
should
be
kept
up
to
date
,
appropriately
throughout
the
lifetime
of
the
AI
syste
m.
Further
more
,
higher
isk
AI
systems
should
technically
allow
for
the
auto
matic
recording
of
events
,
by
means
of
logs
,
over
the
duration
of
the
lifetime
of
the
syste
m.
To
address
concer
ns
relate
d
to
opacity
and
complexity
of
certain
AI
systems
and
help
deplo
yers
to
fulfil
their
oblig
ations
under
this
Regulation
,
transparency
should
be
required
for
higher
isk
AI
systems
before
they
are
placed
on
the
market
or
put
it
into
service
.
higher
isk
AI
systems
should
be
designed
in
a
manner
to
enable
deplo
yers
to
understand
how
the
AI
syste
m
works
,
evaluate
its
functionality
,
and
comp
rehend
its
strengths
and
limitations
.
higher
isk
AI
systems
should
be
accom
panied
by
appropriate
information
in
the
form
of
instr
uctions
of
use
.
Such
information
should
include
the
charact
eristics
,
capabilities
and
limitations
of
perfo
rmance
of
the
AI
system
.
Those
would
cover
information
on
possible
kno
wn
and
foreseeable
circumstances
related
to
the
use
of
the
higher
isk
AI
syste
m
,
including
deplo
yer
action
that
may
influence
syste
m
behavio
ur
and
perfor
mance
,
under
which
the
AI
system
can
lead
to
risks
to
health
,
safety
,
and
fundamental
rights
,
on
the
changes
that
have
been
pre
-
deter
mined
and
assessed
for
conf
ormity
by
the
provider
and
on
the
relevant
human
oversight
measures
,
including
the
measures
to
facilitate
the
inter
pretation
of
the
outputs
of
the
AI
syste
m
by
the
deplo
yers
.
Transparency
,
including
the
accom
panying
instr
uctions
for
use
,
should
assist
deplo
yers
in
the
use
of
the
syste
m
and
support
informed
decision
making
by
them
.
Deplo
yers
should
,
inter
alia
,
be
in
a
better
position
to
mak
e
the
correct
choice
of
the
syste
m
that
they
intend
to
use
in
light
of
the
obligations
applicable
to
them
,
be
educat
ed
about
the
intended
and
precluded
uses
,
and
use
the
AI
system
correctly
and
as
appropriate
.
In
order
to
enhance
legibility
and
accessibility
of
the
information
included
in
the
instr
uctions
of
use
,
where
appropriate
,
illustrative
examples
,
for
instance
on
the
limitations
and
on
the
intende
d
and
precluded
uses
of
the
AI
syste
m
,
should
be
included
.
Provi
ders
should
ensure
that
all
documentation
,
including
the
instr
uctions
for
use
,
contains
meaningful
,
compre
hensive
,
accessible
and
understandable
information
,
taking
into
account
the
needs
and
foreseeable
kno
wledge
of
the
targe
t
deplo
yers
.
Instr
uctions
for
use
should
be
made
available
in
a
language
which
can
be
easily
understood
by
targ
et
deplo
yers
,
as
determined
by
the
Member
State
concer
ned
.
higher
isk
AI
systems
should
be
designed
and
developed
in
such
a
way
that
natural
persons
can
oversee
their
functioning
,
ensure
that
they
are
used
as
intended
and
that
their
imp
acts
are
addressed
over
the
system
’s
lifecy
cle
.
To
that
end
,
appropriate
human
oversight
measures
should
be
identifie
d
by
the
provider
of
the
syste
m
before
its
placing
on
the
market
or
putting
into
service
.
In
particular
,
where
appropriate
,
such
measures
should
guarantee
that
the
syste
m
is
subject
to
in
-
built
operational
constraints
that
can
not
be
overr
idden
by
the
system
itself
and
is
responsive
to
the
human
operat
or
,
and
that
the
natural
persons
to
whom
human
oversight
has
been
assigned
have
the
necessary
com
petence
,
training
and
author
ity
to
carry
out
that
role
.
It
is
also
essential
,
as
appropriate
,
to
ensure
that
higher
isk
AI
systems
include
mechanisms
to
guide
and
inform
a
natural
person
to
whom
human
oversight
has
been
assigned
to
mak
e
informed
decisions
if
,
when
and
how
to
intervene
in
order
to
avoid
nega
tive
consequences
or
risks
,
or
stop
the
syste
m
if
it
does
not
perform
as
intended
.
Consider
ing
the
significant
consequences
for
persons
in
the
case
of
an
incor
rect
matc
h
by
certain
biometric
identification
systems
,
it
is
appropriate
to
provide
for
an
enhanced
human
oversight
requirement
for
those
systems
so
that
no
action
or
decision
may
be
take
n
by
the
deplo
yer
on
the
basis
of
the
identifica
tion
resulting
from
the
system
unless
this
has
been
separate
ly
verified
and
conf
irmed
by
at
least
two
natural
persons
.
Those
persons
could
be
from
one
or
more
entities
and
include
the
person
operating
or
using
the
syste
m.
This
requirement
should
not
pose
unnecessary
burden
or
dela
ys
and
it
could
be
suffi
cient
that
the
separate
verificati
ons
by
the
diffe
rent
persons
are
auto
matically
recorded
in
the
logs
generated
by
the
syste
m.
Given
the
specificities
of
the
areas
of
law
enforcement
,
migration
,
border
control
and
asylum
,
this
requirement
should
not
apply
where
Union
or
national
law
considers
the
application
of
that
requirement
to
be
dispropor
tionate
.
higher
isk
AI
systems
should
perf
orm
consiste
ntly
throughout
their
lifecycle
and
meet
an
appropriate
level
of
accuracy
,
robustness
and
cybersecurity
,
in
light
of
their
intended
purpose
and
in
accordance
with
the
generally
acknowledg
ed
state
of
the
art
.
The
Commission
and
relevant
organisations
and
stak
eholders
are
encourage
d
to
take
due
consideration
of
the
mitiga
tion
of
risks
and
the
nega
tive
impacts
of
the
AI
system
.
The
expecte
d
level
of
perf
ormance
metr
ics
should
be
declared
in
the
accom
panying
instr
uctions
of
use
.
Providers
are
urged
to
communicate
that
information
to
deplo
yers
in
a
clear
and
easily
understandable
way
,
free
of
misunderstandings
or
misleading
statements
.
Union
law
on
lega
l
metrology
,
including
Directives
/EU
and
/EU
of
the
European
Parliament
and
of
the
Council
,
aims
to
ensure
the
accuracy
of
measurements
and
to
help
the
transparency
and
fairne
ss
of
commercial
transactions
.
In
that
context
,
in
cooperation
with
relevant
stakeh
olders
and
organisation
,
such
as
metrology
and
bench
marking
authorities
,
the
Commission
should
encourage
,
as
appropriate
,
the
development
of
bench
mark
s
and
measurement
methodologies
for
AI
systems
.
In
doing
so
,
the
Commission
should
take
note
and
collaborate
with
inter
national
partners
working
on
metrology
and
relevant
measurement
indicato
rs
relating
to
AI
.
(
35
)
Directive
/EU
of
the
European
Parliament
and
of
the
Council
of
26
Febr
uary
2014
on
the
harmonisation
of
the
laws
of
the
Member
State
s
relating
to
the
making
available
on
the
market
of
non
-
aut
omatic
weighing
instr
uments
(
3.2014
,
p.
107
)
.
(
36
)
Directive
/EU
of
the
European
Parliament
and
of
the
Council
of
26
Febr
uary
2014
on
the
harmonisation
of
the
laws
of
the
Member
States
relating
to
the
making
available
on
the
market
of
measur
ing
instr
uments
(
3.2014
,
p.
149).(75
)
Technical
robustness
is
a
key
requirement
for
higher
isk
AI
systems
.
They
should
be
resilient
in
relation
to
harmful
or
other
wise
undesirable
behavi
our
that
may
result
from
limitations
within
the
systems
or
the
environment
in
which
the
systems
operate
(
e.g.
errors
,
faults
,
inconsiste
ncies
,
unexpected
situations
)
.
Theref
ore
,
technical
and
organisational
measures
should
be
take
n
to
ensure
robustness
of
higher
isk
AI
systems
,
for
example
by
designing
and
developing
appropriate
technical
solutions
to
prevent
or
minimise
harmful
or
other
wise
undesirable
behavi
our
.
Those
technical
solution
may
include
for
instance
mec
hanisms
enabling
the
system
to
safely
inter
rupt
its
operation
(
fail
-
saf
e
plans
)
in
the
presence
of
certain
anomalies
or
when
operation
take
s
place
outside
certain
predeter
mined
boundar
ies
.
Failure
to
protect
against
these
risks
could
lead
to
safety
impacts
or
nega
tively
affect
the
fundamental
rights
,
for
example
due
to
erroneous
decisions
or
wrong
or
biased
outputs
generated
by
the
AI
syste
m.
(
76
)
Cybersecurity
plays
a
crucial
role
in
ensur
ing
that
AI
systems
are
resilient
against
attem
pts
to
alter
their
use
,
behavi
our
,
perf
ormance
or
comp
romise
their
security
proper
ties
by
malicious
third
parties
exploiting
the
syste
m
’s
vulnerabilities
.
Cyberattacks
against
AI
systems
can
leverage
AI
specific
assets
,
such
as
training
data
sets
(
e.g.
data
poisoning
)
or
trained
models
(
e.g.
adversar
ial
attacks
or
membership
inference
)
,
or
exploit
vulnerabilities
in
the
AI
syste
m
’s
digital
assets
or
the
underlying
ICT
infrastructure
.
To
ensure
a
level
of
cybersecurity
appropriate
to
the
risks
,
suitable
measures
,
such
as
security
controls
,
should
theref
ore
be
take
n
by
the
provid
ers
of
higher
isk
AI
systems
,
also
taking
into
account
as
appropriate
the
underlying
ICT
infrastructure
.
(
77
)
Without
prejudice
to
the
requirements
related
to
robustness
and
accuracy
set
out
in
this
Regulation
,
higher
isk
AI
systems
which
fall
within
the
scope
of
a
regulation
of
the
European
Parliament
and
of
the
Council
on
horizontal
cybersecurity
requirements
for
products
with
digital
elements
,
in
accordance
with
that
regulation
may
demonstrate
compliance
with
the
cybersecurity
requirements
of
this
Regulation
by
fulfilling
the
essential
cybersecurity
requirements
set
out
in
that
regulation
.
When
higher
isk
AI
systems
fulfil
the
essential
requirements
of
a
regulation
of
the
European
Parliament
and
of
the
Council
on
horizontal
cybersecurity
requirements
for
products
with
digital
elements
,
they
should
be
deemed
compliant
with
the
cybersecurity
requirements
set
out
in
this
Regulation
in
so
far
as
the
achievement
of
those
requirements
is
demonstrated
in
the
EU
declaration
of
conf
ormity
or
parts
thereof
issued
under
that
regulation
.
To
that
end
,
the
assessment
of
the
cybersecurity
risks
,
associated
to
a
product
with
digital
elements
classif
ied
as
higher
isk
AI
syste
m
according
to
this
Regulation
,
carried
out
under
a
regulation
of
the
European
Parliament
and
of
the
Council
on
horizontal
cybersecurity
requirements
for
products
with
digital
elements
,
should
consider
risks
to
the
cyber
resilience
of
an
AI
system
as
regard
s
attem
pts
by
unauthorised
third
parties
to
alter
its
use
,
behavio
ur
or
perf
ormance
,
including
AI
specific
vulnerabilities
such
as
data
poisoning
or
adversar
ial
attacks
,
as
well
as
,
as
relevant
,
risks
to
fundamental
rights
as
required
by
this
Regulation
.
(
78
)
The
conf
ormity
assessment
procedure
provid
ed
by
this
Regulation
should
apply
in
relation
to
the
essential
cybersecurity
requirements
of
a
product
with
digital
elements
covered
by
a
regulation
of
the
European
Parliament
and
of
the
Council
on
horizontal
cybersecurity
requirements
for
products
with
digital
elements
and
classified
as
a
higher
isk
AI
system
under
this
Regulation
.
However
,
this
rule
should
not
result
in
reducing
the
necessary
level
of
assurance
for
critical
products
with
digital
elements
covered
by
a
regulation
of
the
European
Parliament
and
of
the
Council
on
horizontal
cybersecurity
requirements
for
products
with
digital
elements
.
Theref
ore
,
by
way
of
deroga
tion
from
this
rule
,
higher
isk
AI
systems
that
fall
within
the
scope
of
this
Regulation
and
are
also
qualified
as
important
and
critical
products
with
digital
elements
pursuant
to
a
regulation
of
the
European
Parliament
and
of
the
Council
on
horizontal
cybersecurity
requirements
for
products
with
digital
elements
and
to
which
the
conf
ormity
assessment
procedure
based
on
inter
nal
control
set
out
in
an
annex
to
this
Regulation
applies
,
are
subject
to
the
conf
ormity
assessment
provisions
of
a
regulation
of
the
European
Parliament
and
of
the
Council
on
horizontal
cybersecurity
requirements
for
products
with
digital
elements
insofar
as
the
essential
cybersecurity
requirements
of
that
regulation
are
concer
ned
.
In
this
case
,
for
all
the
other
aspects
covered
by
this
Regulation
the
respective
provisions
on
conf
ormity
assessment
based
on
inter
nal
control
set
out
in
an
annex
to
this
Regulation
should
apply
.
Building
on
the
kno
wledge
and
exper
tise
of
ENISA
on
the
cybersecurity
policy
and
tasks
assigned
to
ENISA
under
the
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
the
Commission
should
cooperate
with
ENISA
on
issues
related
to
cybersecurity
of
AI
systems
.
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
17
Apr
il
2019
on
ENISA
(
the
European
Union
Agency
for
Cybersecurity
)
and
on
information
and
communications
technology
cybersecurity
certification
and
repealing
Regulation
(
EU
)
No
(
Cybersecurity
Act
)
(
6.2019
,
p.
15).(79
)
It
is
appropriate
that
a
specific
natural
or
lega
l
person
,
defined
as
the
provider
,
takes
responsibility
for
the
placing
on
the
market
or
the
putting
into
service
of
a
higher
isk
AI
syste
m
,
regardless
of
whether
that
natural
or
lega
l
person
is
the
person
who
designed
or
developed
the
syste
m.
(
80
)
As
signat
ories
to
the
United
Nations
Convention
on
the
Rights
of
Persons
with
Disabilities
,
the
Union
and
the
Member
States
are
legally
oblige
d
to
prot
ect
persons
with
disabilities
from
discr
imination
and
promot
e
their
equality
,
to
ensure
that
persons
with
disabilities
have
access
,
on
an
equal
basis
with
others
,
to
information
and
communications
technologies
and
systems
,
and
to
ensure
respect
for
privacy
for
persons
with
disabilities
.
Given
the
growing
impor
tance
and
use
of
AI
systems
,
the
application
of
universal
design
principles
to
all
new
technologi
es
and
services
should
ensure
full
and
equal
access
for
ever
yone
potentially
affect
ed
by
or
using
AI
technologies
,
including
persons
with
disabilities
,
in
a
way
that
take
s
full
account
of
their
inherent
dignity
and
diversity
.
It
is
theref
ore
essential
that
provid
ers
ensure
full
compliance
with
accessibility
requirements
,
including
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
Directive
(
EU
)
.
Providers
should
ensure
compliance
with
these
requirements
by
design
.
Theref
ore
,
the
necessary
measures
should
be
integrat
ed
as
much
as
possible
into
the
design
of
the
higher
isk
AI
syste
m.
(
81
)
The
provider
should
establish
a
sound
quality
management
syste
m
,
ensure
the
accom
plishment
of
the
required
conf
ormity
assessment
procedure
,
draw
up
the
relevant
documentation
and
establish
a
robust
post
-
mark
et
monitoring
syste
m.
Provi
ders
of
higher
isk
AI
systems
that
are
subject
to
oblig
ations
regarding
quality
management
systems
under
relevant
secto
ral
Union
law
should
have
the
possibility
to
include
the
elements
of
the
quality
management
syste
m
provid
ed
for
in
this
Regulation
as
part
of
the
existing
quality
management
system
provid
ed
for
in
that
other
sectoral
Union
law
.
The
complement
arity
between
this
Regulation
and
existing
secto
ral
Union
law
should
also
be
take
n
into
account
in
future
standardisation
activities
or
guidance
adop
ted
by
the
Commission
.
Public
authorities
which
put
into
service
higher
isk
AI
systems
for
their
own
use
may
adop
t
and
implement
the
rules
for
the
quality
managem
ent
syste
m
as
part
of
the
quality
managem
ent
syste
m
adop
ted
at
a
national
or
regional
level
,
as
appropriate
,
taking
into
account
the
specificities
of
the
secto
r
and
the
comp
etences
and
organisation
of
the
public
author
ity
concer
ned
.
(
82
)
To
enable
enforcement
of
this
Regulation
and
create
a
level
playing
field
for
operators
,
and
,
taking
into
account
the
different
forms
of
making
available
of
digital
products
,
it
is
impor
tant
to
ensure
that
,
under
all
circumstances
,
a
person
established
in
the
Union
can
provide
authorities
with
all
the
necessary
information
on
the
compliance
of
an
AI
syste
m.
Theref
ore
,
prior
to
making
their
AI
systems
available
in
the
Union
,
provid
ers
established
in
third
countr
ies
should
,
by
written
mandat
e
,
appoint
an
authorised
representative
established
in
the
Union
.
This
authorised
representative
plays
a
pivot
al
role
in
ensur
ing
the
compliance
of
the
higher
isk
AI
systems
placed
on
the
market
or
put
into
service
in
the
Union
by
those
providers
who
are
not
established
in
the
Union
and
in
serving
as
their
contact
person
established
in
the
Union
.
(
83
)
In
light
of
the
nature
and
comp
lexity
of
the
value
chain
for
AI
systems
and
in
line
with
the
New
Legislative
Framewo
rk
,
it
is
essential
to
ensure
lega
l
certainty
and
facilitate
the
compliance
with
this
Regulation
.
Theref
ore
,
it
is
necessary
to
clarify
the
role
and
the
specific
obligations
of
relevant
operators
along
that
value
chain
,
such
as
importers
and
distr
ibutors
who
may
contribut
e
to
the
development
of
AI
systems
.
In
certain
situations
those
operators
could
act
in
more
than
one
role
at
the
same
time
and
should
theref
ore
fulfil
cumulatively
all
relevant
oblig
ations
associate
d
with
those
roles
.
For
example
,
an
operator
could
act
as
a
distr
ibutor
and
an
imp
orter
at
the
same
time
.
(
84
)
To
ensure
legal
certainty
,
it
is
necessary
to
clarify
that
,
under
certain
specific
conditions
,
any
distr
ibutor
,
imp
orter
,
deplo
yer
or
other
third
-
par
ty
should
be
considered
to
be
a
provider
of
a
higher
isk
AI
syste
m
and
theref
ore
assume
all
the
relevant
obligations
.
This
would
be
the
case
if
that
party
puts
its
name
or
trademark
on
a
higher
isk
AI
system
already
placed
on
the
market
or
put
into
service
,
without
prejudice
to
contractual
arrang
ements
stipulating
that
the
oblig
ations
are
allocated
other
wise
.
This
would
also
be
the
case
if
that
party
makes
a
substantial
modifi
cation
to
a
higher
isk
AI
syste
m
that
has
already
been
placed
on
the
market
or
has
already
been
put
into
service
in
a
way
that
it
remains
a
higher
isk
AI
syste
m
in
accordance
with
this
Regulation
,
or
if
it
modif
ies
the
intended
purpose
of
an
AI
syste
m
,
including
a
general
-
purpo
se
AI
system
,
which
has
not
been
classif
ied
as
higher
isk
and
has
already
been
placed
on
the
mark
et
or
put
into
service
,
in
a
way
that
the
AI
syste
m
becomes
a
higher
isk
AI
syste
m
in
accordance
with
this
Regulation
.
Those
provis
ions
should
apply
without
prejudice
to
more
specific
provisions
established
in
certain
Union
harmonisation
legislation
based
on
the
New
Legislative
Framewo
rk
,
together
with
which
this
(
38
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
26
October
2016
on
the
accessibility
of
the
websit
es
and
mobile
applications
of
public
sector
bodies
(
12.2016
,
p.
1).Regulation
should
apply
.
For
example
,
Article
16(2
)
of
Regulation
(
EU
)
,
establishing
that
certain
change
s
should
not
be
considered
to
be
modifi
cations
of
a
device
that
could
affect
its
compliance
with
the
applicable
requirements
,
should
continue
to
apply
to
higher
isk
AI
systems
that
are
medical
devices
within
the
meaning
of
that
Regulation
.
(
85
)
General
-
pur
pose
AI
systems
may
be
used
as
higher
isk
AI
systems
by
themselves
or
be
comp
onents
of
other
higher
isk
AI
systems
.
Theref
ore
,
due
to
their
particular
nature
and
in
order
to
ensure
a
fair
shar
ing
of
responsibilities
along
the
AI
value
chain
,
the
provider
s
of
such
systems
should
,
irrespective
of
whether
they
may
be
used
as
higher
isk
AI
systems
as
such
by
other
providers
or
as
compo
nents
of
higher
isk
AI
systems
and
unless
provid
ed
other
wise
under
this
Regulation
,
closely
cooperate
with
the
providers
of
the
relevant
higher
isk
AI
systems
to
enable
their
compliance
with
the
relevant
obligations
under
this
Regulation
and
with
the
competent
authorities
established
under
this
Regulation
.
(
86
)
Where
,
under
the
conditions
laid
down
in
this
Regulation
,
the
provid
er
that
initially
placed
the
AI
syste
m
on
the
mark
et
or
put
it
into
service
should
no
longe
r
be
considered
to
be
the
provid
er
for
the
purposes
of
this
Regulation
,
and
when
that
provid
er
has
not
expressly
excluded
the
chang
e
of
the
AI
syste
m
into
a
higher
isk
AI
syste
m
,
the
former
provid
er
should
nonetheless
closely
cooperate
and
make
available
the
necessary
information
and
provid
e
the
reasonably
expecte
d
technical
access
and
other
assistance
that
are
required
for
the
fulfilment
of
the
obligations
set
out
in
this
Regulation
,
in
particular
regarding
the
compliance
with
the
conf
ormity
assessment
of
higher
isk
AI
systems
.
(
87
)
In
addition
,
where
a
higher
isk
AI
syste
m
that
is
a
safety
comp
onent
of
a
product
which
falls
within
the
scope
of
Union
harmonisation
legislation
based
on
the
New
Legislative
Framework
is
not
placed
on
the
market
or
put
into
service
independently
from
the
product
,
the
product
manufa
cturer
defined
in
that
legislation
should
comp
ly
with
the
obligations
of
the
provider
established
in
this
Regulation
and
should
,
in
particular
,
ensure
that
the
AI
system
embedded
in
the
final
product
comp
lies
with
the
requirements
of
this
Regulation
.
(
88
)
Along
the
AI
value
chain
multiple
parties
often
supply
AI
systems
,
tools
and
services
but
also
compo
nents
or
processes
that
are
incor
porate
d
by
the
provid
er
into
the
AI
syste
m
with
various
objectives
,
including
the
model
training
,
model
retraining
,
model
testing
and
evaluation
,
integration
into
software
,
or
other
aspects
of
model
development
.
Those
parties
have
an
imp
ortant
role
to
play
in
the
value
chain
towards
the
provid
er
of
the
higher
isk
AI
syste
m
into
which
their
AI
systems
,
tools
,
services
,
comp
onents
or
processes
are
integrat
ed
,
and
should
provide
by
written
agreement
this
provid
er
with
the
necessary
information
,
capabilities
,
technical
access
and
other
assistance
based
on
the
generally
ackno
wledged
state
of
the
art
,
in
order
to
enable
the
provid
er
to
fully
comply
with
the
oblig
ations
set
out
in
this
Regulation
,
without
com
promising
their
own
intellect
ual
proper
ty
rights
or
trade
secrets
.
(
89
)
Third
parties
making
accessible
to
the
public
tools
,
services
,
processes
,
or
AI
comp
onents
other
than
general
-
pur
pose
AI
models
,
should
not
be
mandat
ed
to
comp
ly
with
requirements
targ
eting
the
responsibilities
along
the
AI
value
chain
,
in
particular
towards
the
provid
er
that
has
used
or
integrat
ed
them
,
when
those
tools
,
services
,
processes
,
or
AI
comp
onents
are
made
accessible
under
a
free
and
open
-
source
licence
.
Developers
of
free
and
open
-
source
tools
,
services
,
processes
,
or
AI
comp
onents
other
than
general
-
pur
pose
AI
models
should
be
encourag
ed
to
implement
widely
adop
ted
documentation
practices
,
such
as
model
cards
and
data
sheets
,
as
a
way
to
accelerat
e
information
shar
ing
along
the
AI
value
chain
,
allowi
ng
the
promotion
of
trustworthy
AI
systems
in
the
Union
.
(
90
)
The
Commission
could
develop
and
recommend
voluntary
model
contractual
term
s
between
providers
of
higher
isk
AI
systems
and
third
parties
that
supply
tools
,
services
,
comp
onents
or
processes
that
are
used
or
integrat
ed
in
higher
isk
AI
systems
,
to
facilitate
the
cooperation
along
the
value
chain
.
When
developing
voluntar
y
model
contractual
terms
,
the
Commission
should
also
take
into
account
possible
contractual
requirements
applicable
in
specific
sectors
or
business
cases
.
(
91
)
Given
the
nature
of
AI
systems
and
the
risks
to
safety
and
fundamental
rights
possibly
associate
d
with
their
use
,
including
as
rega
rds
the
need
to
ensure
proper
monitori
ng
of
the
perfor
mance
of
an
AI
syste
m
in
a
real
-
life
setting
,
it
is
appropriate
to
set
specific
responsibilities
for
deplo
yers
.
Deplo
yers
should
in
particular
take
appropriate
technical
and
organisational
measures
to
ensure
they
use
higher
isk
AI
systems
in
accordance
with
the
instr
uctions
of
use
and
certain
other
obligations
should
be
provided
for
with
regard
to
monitoring
of
the
functioning
of
the
AI
systems
and
with
regard
to
record
-
keeping
,
as
appropriate
.
Further
more
,
deplo
yers
should
ensure
that
the
persons
assigned
to
implement
the
instr
uctions
for
use
and
human
oversight
as
set
out
in
this
Regulation
have
the
necessary
petence
,
in
particular
an
adequate
level
of
AI
literacy
,
training
and
author
ity
to
properly
fulfil
those
tasks
.
Those
oblig
ations
should
be
without
prejudice
to
other
deplo
yer
obligations
in
relation
to
higher
isk
AI
systems
under
Union
or
national
law
.
(
92
)
This
Regulation
is
without
prejudice
to
obligations
for
emplo
yers
to
inform
or
to
inform
and
consult
workers
or
their
representatives
under
Union
or
national
law
and
practice
,
including
Directive
/EC
of
the
European
Parliament
and
of
the
Council
,
on
decisions
to
put
into
service
or
use
AI
systems
.
It
remains
necessary
to
ensure
information
of
workers
and
their
representatives
on
the
planned
deplo
yment
of
higher
isk
AI
systems
at
the
workplace
where
the
conditions
for
those
information
or
information
and
consultation
obligations
in
other
legal
instr
uments
are
not
fulfilled
.
Moreo
ver
,
such
information
right
is
ancillar
y
and
necessary
to
the
objective
of
prot
ecting
fundamental
rights
that
underlies
this
Regulation
.
Theref
ore
,
an
information
requirement
to
that
effect
should
be
laid
down
in
this
Regulation
,
without
affecting
any
existing
rights
of
work
ers
.
(
93
)
Whilst
risks
relate
d
to
AI
systems
can
result
from
the
way
such
systems
are
designed
,
risks
can
as
well
stem
from
how
such
AI
systems
are
used
.
Deplo
yers
of
higher
isk
AI
syste
m
theref
ore
play
a
critical
role
in
ensur
ing
that
fundamental
rights
are
protected
,
compl
ementing
the
obligati
ons
of
the
provider
when
developing
the
AI
syste
m.
Deplo
yers
are
best
placed
to
understand
how
the
higher
isk
AI
syste
m
will
be
used
concretely
and
can
theref
ore
identify
potent
ial
signif
icant
risks
that
were
not
foreseen
in
the
development
phase
,
due
to
a
more
precise
kno
wledge
of
the
context
of
use
,
the
persons
or
groups
of
persons
likely
to
be
affect
ed
,
including
vulnerable
groups
.
Deplo
yers
of
higher
isk
AI
systems
listed
in
an
annex
to
this
Regulation
also
play
a
critical
role
in
informing
natural
persons
and
should
,
when
they
mak
e
decisions
or
assist
in
making
decisions
related
to
natural
persons
,
where
applicable
,
inform
the
natural
persons
that
they
are
subject
to
the
use
of
the
higher
isk
AI
system
.
This
information
should
include
the
intended
purpose
and
the
type
of
decisions
it
makes
.
The
deplo
yer
should
also
inform
the
natural
persons
about
their
right
to
an
explanation
provided
under
this
Regulation
.
With
regard
to
higher
isk
AI
systems
used
for
law
enforcement
purposes
,
that
obligation
should
be
imp
lemented
in
accordance
with
Article
13
of
Directive
(
EU
)
.
(
94
)
Any
processing
of
biometric
data
involved
in
the
use
of
AI
systems
for
biometric
identifica
tion
for
the
purpose
of
law
enforcement
needs
to
comp
ly
with
Article
10
of
Directive
(
EU
)
,
that
allows
such
processing
only
where
strictly
necessary
,
subject
to
appropriate
safeguards
for
the
rights
and
freedoms
of
the
data
subject
,
and
where
authorised
by
Union
or
Member
State
law
.
Such
use
,
when
authorised
,
also
needs
to
respect
the
principles
laid
down
in
Article
4
of
Directive
(
EU
)
including
lawfulness
,
fairness
and
transparency
,
purpose
limitation
,
accuracy
and
storag
e
limitation
.
(
95
)
Without
prejudice
to
applicable
Union
law
,
in
particular
Regulation
(
EU
)
and
Directive
(
EU
)
,
consider
ing
the
intrusive
nature
of
post
-
remote
biometric
identifica
tion
systems
,
the
use
of
post
-
remote
biometric
identification
systems
should
be
subject
to
safeguards
.
Post
-
remot
e
biometric
identification
systems
should
alwa
ys
be
used
in
a
way
that
is
propor
tionate
,
legitimate
and
strictly
necessary
,
and
thus
targe
ted
,
in
terms
of
the
individuals
to
be
identified
,
the
location
,
temporal
scope
and
based
on
a
closed
data
set
of
legally
acquired
video
footage
.
In
any
case
,
post
-
remote
biometric
identifica
tion
systems
should
not
be
used
in
the
framework
of
law
enforcement
to
lead
to
indiscr
iminate
surveillance
.
The
conditions
for
post
-
remote
biometric
identifica
tion
should
in
any
case
not
provide
a
basis
to
circumvent
the
conditions
of
the
prohibition
and
strict
excep
tions
for
real
time
remote
biometric
identification
.
(
96
)
In
order
to
efficiently
ensure
that
fundamental
rights
are
protected
,
deplo
yers
of
higher
isk
AI
systems
that
are
bodies
gove
rned
by
public
law
,
or
private
entities
providing
public
services
and
deplo
yers
of
certain
higher
isk
AI
systems
listed
in
an
annex
to
this
Regulation
,
such
as
banking
or
insurance
entities
,
should
carry
out
a
fundamental
rights
impact
assessment
prior
to
putting
it
into
use
.
Services
imp
ortant
for
individuals
that
are
of
public
nature
may
also
be
provided
by
private
entities
.
Private
entities
providing
such
public
services
are
linked
to
tasks
in
the
public
interest
such
as
in
the
areas
of
education
,
healthcare
,
social
services
,
housing
,
administration
of
justice
.
The
aim
of
the
fundamental
rights
impact
assessment
is
for
the
deplo
yer
to
identify
the
specific
risks
to
the
rights
of
individuals
or
groups
of
individuals
likely
to
be
affected
,
identify
measures
to
be
take
n
in
the
case
of
a
mat
erialisation
of
those
risks
.
The
impact
assessment
should
be
perf
ormed
prior
to
deplo
ying
the
higher
isk
AI
syste
m
,
and
should
be
updated
(
39
)
Directive
/EC
of
the
European
Parliament
and
of
the
Council
of
11
March
2002
establishing
a
general
framework
for
informing
and
consulting
employees
in
the
European
Community
(
3.2002
,
p.
29).when
the
deplo
yer
considers
that
any
of
the
relevant
factors
have
changed
.
The
impact
assessment
should
identify
the
deplo
yer
’s
relevant
processes
in
which
the
higher
isk
AI
system
will
be
used
in
line
with
its
intended
purpose
,
and
should
include
a
descr
iption
of
the
period
of
time
and
frequency
in
which
the
syste
m
is
intended
to
be
used
as
well
as
of
specific
categor
ies
of
natural
persons
and
groups
who
are
likely
to
be
affect
ed
in
the
specific
context
of
use
.
The
assessment
should
also
include
the
identifi
cation
of
specific
risks
of
harm
likel
y
to
have
an
impact
on
the
fundamental
rights
of
those
persons
or
groups
.
While
performing
this
assessment
,
the
deplo
yer
should
take
into
account
information
relevant
to
a
proper
assessment
of
the
imp
act
,
including
but
not
limit
ed
to
the
information
given
by
the
provid
er
of
the
higher
isk
AI
system
in
the
instr
uctions
for
use
.
In
light
of
the
risks
identified
,
deplo
yers
should
determine
measures
to
be
taken
in
the
case
of
a
materialis
ation
of
those
risks
,
including
for
example
gove
rnance
arrangements
in
that
specific
context
of
use
,
such
as
arrang
ements
for
human
oversight
according
to
the
instr
uctions
of
use
or
,
complai
nt
handling
and
redress
procedures
,
as
they
could
be
instr
umental
in
mitigating
risks
to
fundamental
rights
in
concrete
use
-
cases
.
After
perfor
ming
that
impact
assessment
,
the
deplo
yer
should
notify
the
relevant
market
surveillance
author
ity
.
Where
appropriate
,
to
collect
relevant
information
necessary
to
perfo
rm
the
impact
assessment
,
deplo
yers
of
higher
isk
AI
syste
m
,
in
particular
when
AI
systems
are
used
in
the
public
sector
,
could
involve
relevant
stakeh
olders
,
including
the
representatives
of
groups
of
persons
likely
to
be
affect
ed
by
the
AI
syste
m
,
independent
exper
ts
,
and
civil
society
organisations
in
conducting
such
impact
assessments
and
designing
measures
to
be
take
n
in
the
case
of
materi
alisation
of
the
risks
.
The
European
Artificial
Intellig
ence
Office
(
AI
Offi
ce
)
should
develop
a
templat
e
for
a
questionnaire
in
order
to
facilitate
compliance
and
reduce
the
administrative
burden
for
deplo
yers
.
(
97
)
The
notion
of
general
-
pur
pose
AI
models
should
be
clearly
defined
and
set
apar
t
from
the
notion
of
AI
systems
to
enable
legal
certainty
.
The
definition
should
be
based
on
the
key
functional
character
istics
of
a
general
-
pur
pose
AI
model
,
in
particular
the
generality
and
the
capability
to
com
petently
perf
orm
a
wide
rang
e
of
distinct
tasks
.
These
models
are
typically
trained
on
large
amounts
of
data
,
through
various
methods
,
such
as
self
-
super
vised
,
unsuper
vised
or
reinf
orcement
learning
.
General
-
pur
pose
AI
models
may
be
placed
on
the
market
in
various
ways
,
including
through
librar
ies
,
application
programming
interfaces
(
APIs
)
,
as
direct
download
,
or
as
physica
l
copy
.
These
models
may
be
further
modif
ied
or
fine
-
tuned
into
new
models
.
Although
AI
models
are
essential
com
ponents
of
AI
systems
,
they
do
not
constitute
AI
systems
on
their
own
.
AI
models
require
the
addition
of
further
com
ponents
,
such
as
for
example
a
user
interface
,
to
become
AI
systems
.
AI
models
are
typically
integrat
ed
into
and
form
part
of
AI
systems
.
This
Regulation
provid
es
specific
rules
for
general
-
pur
pose
AI
models
and
for
general
-
pur
pose
AI
models
that
pose
syste
mic
risks
,
which
should
apply
also
when
these
models
are
integrat
ed
or
form
part
of
an
AI
syste
m.
It
should
be
understood
that
the
obligations
for
the
providers
of
gene
ral
-
pur
pose
AI
models
should
apply
once
the
general
-
pur
pose
AI
models
are
placed
on
the
market
.
When
the
provid
er
of
a
general
-
pur
pose
AI
model
integrates
an
own
model
into
its
own
AI
syste
m
that
is
made
available
on
the
market
or
put
into
service
,
that
model
should
be
considered
to
be
placed
on
the
market
and
,
theref
ore
,
the
obligations
in
this
Regulation
for
models
should
continue
to
apply
in
addition
to
those
for
AI
systems
.
The
obligations
laid
down
for
models
should
in
any
case
not
apply
when
an
own
model
is
used
for
purely
inter
nal
processes
that
are
not
essential
for
providing
a
product
or
a
service
to
third
parties
and
the
rights
of
natural
persons
are
not
affected
.
Consider
ing
their
potent
ial
signifi
cantly
nega
tive
effects
,
the
general
-
pur
pose
AI
models
with
syste
mic
risk
should
alwa
ys
be
subject
to
the
relevant
obligations
under
this
Regulation
.
The
definition
should
not
cover
AI
models
used
before
their
placing
on
the
mark
et
for
the
sole
purpose
of
researc
h
,
development
and
prot
otyping
activities
.
This
is
without
prejudice
to
the
obligation
to
com
ply
with
this
Regulation
when
,
follo
wing
such
activities
,
a
model
is
placed
on
the
mark
et
.
(
98
)
Whereas
the
generality
of
a
model
could
,
inter
alia
,
also
be
determined
by
a
number
of
paramet
ers
,
models
with
at
least
a
billion
of
paramet
ers
and
trained
with
a
large
amount
of
data
using
self
-
super
vision
at
scale
should
be
considered
to
displa
y
significant
generality
and
to
competently
perf
orm
a
wide
rang
e
of
distinctive
tasks
.
(
99
)
Larg
e
generative
AI
models
are
a
typical
example
for
a
general
-
pur
pose
AI
model
,
given
that
they
allow
for
flexible
generation
of
cont
ent
,
such
as
in
the
form
of
text
,
audio
,
imag
es
or
video
,
that
can
readily
accommodate
a
wide
rang
e
of
distinctive
tasks
.
(
100
)
When
a
general
-
pur
pose
AI
model
is
integrated
into
or
forms
part
of
an
AI
syste
m
,
this
syste
m
should
be
considered
to
be
general
-
pur
pose
AI
syste
m
when
,
due
to
this
integration
,
this
system
has
the
capability
to
serve
a
variety
of
purposes
.
A
general
-
purpo
se
AI
system
can
be
used
directly
,
or
it
may
be
integrat
ed
into
other
AI
systems
.
Provi
ders
of
general
-
purpo
se
AI
models
have
a
particular
role
and
responsibility
along
the
AI
value
chain
,
as
the
models
they
provide
may
form
the
basis
for
a
rang
e
of
downstream
systems
,
often
provided
by
downstream
providers
that
necessitate
a
good
understanding
of
the
models
and
their
capabilities
,
both
to
enable
the
integration
of
such
models
into
their
products
,
and
to
fulfil
their
obligations
under
this
or
other
regulations
.
Theref
ore
,
propor
tionate
transparency
measures
should
be
laid
down
,
including
the
drawi
ng
up
and
keeping
up
to
date
of
documentation
,
and
the
provision
of
information
on
the
general
-
pur
pose
AI
model
for
its
usage
by
the
downstream
providers
.
Technical
documentation
should
be
prepared
and
kept
up
to
date
by
the
general
-
pur
pose
AI
model
provider
for
the
purpose
of
making
it
available
,
upon
request
,
to
the
AI
Offi
ce
and
the
national
competent
authorities
.
The
minimal
set
of
elements
to
be
included
in
such
documentation
should
be
set
out
in
specific
annexe
s
to
this
Regulation
.
The
Commission
should
be
empo
wered
to
amend
those
annexe
s
by
means
of
delegat
ed
acts
in
light
of
evolving
technological
developments
.
(
102
)
Software
and
data
,
including
models
,
released
under
a
free
and
open
-
source
licence
that
allows
them
to
be
openly
shared
and
where
users
can
freely
access
,
use
,
modify
and
redistr
ibute
them
or
modif
ied
versions
thereof
,
can
contribut
e
to
research
and
innovation
in
the
mark
et
and
can
provide
significant
growth
oppor
tunities
for
the
Union
economy
.
General
-
pur
pose
AI
models
released
under
free
and
open
-
source
licences
should
be
considered
to
ensure
high
levels
of
transparency
and
openness
if
their
parameter
s
,
including
the
weights
,
the
information
on
the
model
architecture
,
and
the
information
on
model
usage
are
made
publicly
available
.
The
licence
should
be
considered
to
be
free
and
open
-
source
also
when
it
allows
users
to
run
,
copy
,
distr
ibut
e
,
study
,
change
and
improve
software
and
data
,
including
models
under
the
condition
that
the
original
provider
of
the
model
is
credited
,
the
identical
or
compar
able
terms
of
distr
ibution
are
respected
.
(
103
)
Free
and
open
-
source
AI
comp
onents
covers
the
software
and
data
,
including
models
and
general
-
pur
pose
AI
models
,
tools
,
services
or
processes
of
an
AI
syste
m.
Free
and
open
-
source
AI
components
can
be
provided
through
different
channels
,
including
their
development
on
open
repositor
ies
.
For
the
purposes
of
this
Regulation
,
AI
com
ponents
that
are
provid
ed
against
a
price
or
other
wise
monetised
,
including
through
the
provision
of
technical
support
or
other
services
,
including
through
a
software
platform
,
related
to
the
AI
comp
onent
,
or
the
use
of
personal
data
for
reasons
other
than
exclusively
for
improving
the
security
,
compatibility
or
interoperability
of
the
software
,
with
the
exception
of
transactions
between
microenter
prises
,
should
not
benef
it
from
the
excep
tions
provided
to
free
and
open
-
source
AI
components
.
The
fact
of
making
AI
components
availa
ble
through
open
reposit
ories
should
not
,
in
itself
,
constitute
a
monetisation
.
(
104
)
The
provid
ers
of
general
-
pur
pose
AI
models
that
are
released
under
a
free
and
open
-
source
licence
,
and
whose
paramet
ers
,
including
the
weights
,
the
information
on
the
model
archit
ecture
,
and
the
information
on
model
usage
,
are
made
publicly
available
should
be
subject
to
exceptions
as
rega
rds
the
transparency
-
related
requirements
imposed
on
general
-
pur
pose
AI
models
,
unless
they
can
be
considered
to
present
a
systemic
risk
,
in
which
case
the
circumstance
that
the
model
is
transparent
and
accompanied
by
an
open
-
source
license
should
not
be
considered
to
be
a
suffi
cient
reason
to
exclude
compliance
with
the
obligati
ons
under
this
Regulation
.
In
any
case
,
given
that
the
release
of
general
-
pur
pose
AI
models
under
free
and
open
-
source
licence
does
not
necessar
ily
reveal
substantial
information
on
the
data
set
used
for
the
training
or
fine
-
tuning
of
the
model
and
on
how
compliance
of
copyright
law
was
thereby
ensured
,
the
excep
tion
provid
ed
for
general
-
pur
pose
AI
models
from
compliance
with
the
transparency
-
relate
d
requirements
should
not
concer
n
the
obligati
on
to
produce
a
summar
y
about
the
cont
ent
used
for
model
training
and
the
obligation
to
put
in
place
a
policy
to
comply
with
Union
copyright
law
,
in
particular
to
identify
and
comply
with
the
reser
vation
of
rights
pursuant
to
Article
4(3
)
of
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
.
(
105
)
General
-
pur
pose
AI
models
,
in
particular
large
generative
AI
models
,
capable
of
generating
text
,
imag
es
,
and
other
cont
ent
,
present
unique
innovation
oppor
tunities
but
also
challeng
es
to
artists
,
authors
,
and
other
creators
and
the
way
their
creative
content
is
create
d
,
distr
ibut
ed
,
used
and
consumed
.
The
development
and
training
of
such
models
require
access
to
vast
amounts
of
text
,
images
,
videos
and
other
data
.
Text
and
data
mining
techniques
may
be
used
exte
nsively
in
this
context
for
the
retrieval
and
analysis
of
such
cont
ent
,
which
may
be
protected
by
copyright
and
relate
d
rights
.
Any
use
of
copyright
protected
cont
ent
requires
the
author
isation
of
the
rightsholder
concer
ned
unless
relevant
copyright
excep
tions
and
limitations
apply
.
Directive
(
EU
)
introduced
exceptions
and
limitations
allowing
reproductions
and
extractions
of
works
or
other
subject
matter
,
for
the
purpose
of
text
and
data
(
40
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
17
Apr
il
2019
on
copyright
and
relat
ed
rights
in
the
Digital
Sing
le
Market
and
amending
Directives
/EC
and
/EC
(
5.2019
,
p.
92).mining
,
under
certain
conditions
.
Und
er
these
rules
,
rightsholders
may
choose
to
reser
ve
their
rights
over
their
works
or
other
subject
matter
to
prevent
text
and
data
mining
,
unless
this
is
done
for
the
purposes
of
scientific
researc
h.
Where
the
rights
to
opt
out
has
been
expressly
reser
ved
in
an
appropriate
manner
,
provid
ers
of
general
-
pur
pose
AI
models
need
to
obtain
an
author
isation
from
rightsholders
if
they
want
to
carry
out
text
and
data
mining
over
such
works
.
(
106
)
Provi
ders
that
place
general
-
pur
pose
AI
models
on
the
Union
market
should
ensure
compliance
with
the
relevant
oblig
ations
in
this
Regulation
.
To
that
end
,
providers
of
general
-
pur
pose
AI
models
should
put
in
place
a
policy
to
com
ply
with
Union
law
on
copyright
and
related
rights
,
in
particular
to
identify
and
com
ply
with
the
reser
vation
of
rights
expressed
by
rightsholders
pursuant
to
Article
4(3
)
of
Directive
(
EU
)
.
Any
provider
placing
a
general
-
pur
pose
AI
model
on
the
Union
market
should
comply
with
this
obligation
,
regard
less
of
the
jurisdiction
in
which
the
copyright
-
relevant
acts
under
pinning
the
training
of
those
general
-
pur
pose
AI
models
take
place
.
This
is
necessary
to
ensure
a
level
playing
field
among
provider
s
of
gene
ral
-
pur
pose
AI
models
where
no
provider
should
be
able
to
gain
a
competitive
advantage
in
the
Union
market
by
applying
lower
copyright
standards
than
those
provided
in
the
Union
.
(
107
)
In
order
to
increase
transparency
on
the
data
that
is
used
in
the
pre
-
training
and
training
of
general
-
pur
pose
AI
models
,
including
text
and
data
protected
by
copyright
law
,
it
is
adequate
that
provid
ers
of
such
models
draw
up
and
mak
e
publicly
available
a
sufficiently
detailed
summar
y
of
the
content
used
for
training
the
general
-
pur
pose
AI
model
.
While
taking
into
due
account
the
need
to
protect
trade
secrets
and
conf
idential
business
information
,
this
summar
y
should
be
generally
comp
rehensive
in
its
scope
instead
of
technically
detailed
to
facilitat
e
parties
with
legitimat
e
interests
,
including
copyright
holders
,
to
exercise
and
enforce
their
rights
under
Union
law
,
for
example
by
listing
the
main
data
collections
or
sets
that
went
into
training
the
model
,
such
as
large
private
or
public
databases
or
data
archives
,
and
by
providing
a
narrative
explanation
about
other
data
sources
used
.
It
is
appropriate
for
the
AI
Office
to
provid
e
a
template
for
the
summar
y
,
which
should
be
simple
,
effective
,
and
allow
the
provid
er
to
provide
the
required
summar
y
in
narrative
form
.
(
108
)
With
rega
rd
to
the
obligations
imp
osed
on
providers
of
general
-
pur
pose
AI
models
to
put
in
place
a
policy
to
com
ply
with
Union
copyright
law
and
make
publicly
available
a
summar
y
of
the
cont
ent
used
for
the
training
,
the
AI
Office
should
monitor
whether
the
provider
has
fulfilled
those
obligations
without
verifying
or
proceeding
to
a
work
-
by
-
w
ork
assessment
of
the
training
data
in
terms
of
copyright
compliance
.
This
Regulation
does
not
affect
the
enforcement
of
copyright
rules
as
provided
for
under
Union
law
.
(
109
)
compliance
with
the
obligations
applicable
to
the
providers
of
general
-
pur
pose
AI
models
should
be
commensurate
and
propor
tionate
to
the
type
of
model
provider
,
excluding
the
need
for
compliance
for
persons
who
develop
or
use
models
for
non
-
profess
ional
or
scientific
research
purposes
,
who
should
never
theless
be
encouraged
to
voluntar
ily
com
ply
with
these
requirements
.
Without
prejudice
to
Union
copyright
law
,
compliance
with
those
obligations
should
take
due
account
of
the
size
of
the
provider
and
allow
simplified
ways
of
compliance
for
SMEs
,
including
start
-
ups
,
that
should
not
represent
an
excessive
cost
and
not
discourage
the
use
of
such
models
.
In
the
case
of
a
modifi
cation
or
fine
-
tuning
of
a
model
,
the
obligations
for
provid
ers
of
general
-
pur
pose
AI
models
should
be
limit
ed
to
that
modifi
cation
or
fine
-
tuning
,
for
example
by
complementing
the
already
existing
technical
documentation
with
information
on
the
modifi
cations
,
including
new
training
data
sources
,
as
a
means
to
com
ply
with
the
value
chain
oblig
ations
provided
in
this
Regulation
.
(
110
)
General
-
pur
pose
AI
models
could
pose
systemic
risks
which
include
,
but
are
not
limit
ed
to
,
any
actual
or
reasonably
foreseeable
nega
tive
effects
in
relation
to
major
accidents
,
disruptions
of
critical
sectors
and
serious
consequences
to
public
health
and
safety
;
any
actual
or
reasonably
foreseeable
negative
effects
on
democratic
processes
,
public
and
economic
security
;
the
dissemination
of
illega
l
,
false
,
or
discr
iminat
ory
cont
ent
.
Systemic
risks
should
be
understood
to
increase
with
model
capabilities
and
model
reach
,
can
arise
along
the
entire
lifecy
cle
of
the
model
,
and
are
influenced
by
conditions
of
misuse
,
model
reliability
,
model
fairness
and
model
security
,
the
level
of
autonom
y
of
model
,
its
access
to
tools
,
novel
or
combined
modalities
,
release
and
distr
ibution
strategies
,
the
potential
to
remo
ve
guardrails
and
other
factor
s.
In
particular
,
international
approac
he
s
have
so
far
identifie
d
the
need
to
pay
attention
to
risks
from
pote
ntial
intent
ional
misuse
or
uninte
nded
issues
of
control
relating
to
alignment
with
human
intent
;
chemical
,
biological
,
radiological
,
and
nuclear
risks
,
such
as
the
ways
in
which
barriers
to
entr
y
can
be
lowered
,
including
for
weapons
development
,
design
acquisition
,
or
use
;
offensive
cyber
capabilities
,
such
as
the
ways
in
vulnerability
discove
ry
,
exploitation
,
or
operational
use
can
be
enabled
;
the
effects
of
interac
tion
and
tool
use
,
including
for
example
the
capacity
to
control
physical
systems
and
interfere
with
critical
infrastructure
;
risks
from
models
of
making
copies
of
themselves
or
‘
self
-
replicating
’
or
training
other
models
;
the
ways
in
which
models
can
give
rise
to
harmful
bias
and
discr
imination
with
risks
to
individuals
,
communities
or
societies
;
the
facili
tation
of
disinf
ormation
or
harming
privacy
with
threats
to
democratic
values
and
human
rights
;
risk
that
a
particular
event
could
lead
to
a
chain
reaction
with
considerable
nega
tive
effects
that
could
affect
up
to
an
entire
city
,
an
entire
domain
activity
or
an
entire
community
.
(
111
)
It
is
appropriate
to
establish
a
methodology
for
the
classif
ication
of
general
-
pur
pose
AI
models
as
general
-
pur
pose
AI
model
with
systemic
risks
.
Since
syste
mic
risks
result
from
particularly
high
capabilities
,
a
general
-
pur
pose
AI
model
should
be
considered
to
present
systemic
risks
if
it
has
high
-
impact
capabilities
,
evaluate
d
on
the
basis
of
appropriate
technical
tools
and
methodologies
,
or
significant
impact
on
the
internal
market
due
to
its
reac
h.
High
-
im
pact
capabilities
in
general
-
pur
pose
AI
models
means
capabilities
that
matc
h
or
exceed
the
capabilities
recorded
in
the
most
advanced
general
-
pur
pose
AI
models
.
The
full
range
of
capabilities
in
a
model
could
be
better
understo
od
after
its
placing
on
the
market
or
when
deplo
yers
interact
with
the
model
.
According
to
the
state
of
the
art
at
the
time
of
entr
y
into
force
of
this
Regulation
,
the
cumulative
amount
of
comp
utation
used
for
the
training
of
the
general
-
pur
pose
AI
model
measured
in
floating
point
operations
is
one
of
the
relevant
appro
ximations
for
model
capabilities
.
The
cumulative
amount
of
comp
utation
used
for
training
includes
the
computation
used
across
the
activities
and
methods
that
are
intende
d
to
enhance
the
capabilities
of
the
model
prior
to
deplo
yment
,
such
as
pre
-
training
,
synthetic
data
generation
and
fine
-
tuning
.
Theref
ore
,
an
initial
threshold
of
floating
point
operations
should
be
set
,
which
,
if
met
by
a
general
-
pur
pose
AI
model
,
leads
to
a
presump
tion
that
the
model
is
a
general
-
pur
pose
AI
model
with
systemic
risks
.
This
threshold
should
be
adjuste
d
over
time
to
reflect
technological
and
industr
ial
chang
es
,
such
as
algor
ithmic
improvements
or
increased
hardware
efficiency
,
and
should
be
supplement
ed
with
bench
marks
and
indicator
s
for
model
capability
.
To
inform
this
,
the
AI
Offi
ce
should
enga
ge
with
the
scientific
community
,
industr
y
,
civil
society
and
other
exper
ts
.
Thresholds
,
as
well
as
tools
and
bench
mark
s
for
the
assessment
of
high
-
imp
act
capabilities
,
should
be
strong
predict
ors
of
generality
,
its
capabilities
and
associat
ed
systemic
risk
of
general
-
pur
pose
AI
models
,
and
could
take
into
account
the
way
the
model
will
be
placed
on
the
market
or
the
number
of
users
it
may
affect
.
To
com
plement
this
system
,
there
should
be
a
possibility
for
the
Commission
to
take
individual
decisions
designating
a
general
-
pur
pose
AI
model
as
a
general
-
pur
pose
AI
model
with
systemic
risk
if
it
is
found
that
such
model
has
capabilities
or
an
impact
equivalent
to
those
capture
d
by
the
set
threshold
.
That
decision
should
be
take
n
on
the
basis
of
an
overa
ll
assessment
of
the
criteria
for
the
designation
of
a
general
-
pur
pose
AI
model
with
systemic
risk
set
out
in
an
annex
to
this
Regulation
,
such
as
quality
or
size
of
the
training
data
set
,
number
of
business
and
end
users
,
its
input
and
output
modalities
,
its
level
of
autonom
y
and
scalability
,
or
the
tools
it
has
access
to
.
Upon
a
reasoned
request
of
a
provider
whose
model
has
been
designated
as
a
general
-
pur
pose
AI
model
with
systemic
risk
,
the
Commission
should
take
the
request
into
account
and
may
decide
to
reassess
whether
the
general
-
pur
pose
AI
model
can
still
be
considered
to
present
syste
mic
risks
.
(
112
)
It
is
also
necessary
to
clarify
a
procedure
for
the
classific
ation
of
a
general
-
pur
pose
AI
model
with
systemic
risks
.
A
general
-
pur
pose
AI
model
that
meets
the
applicable
threshold
for
high
-
imp
act
capabilities
should
be
presumed
to
be
a
general
-
pur
pose
AI
models
with
syste
mic
risk
.
The
provid
er
should
notify
the
AI
Offi
ce
at
the
latest
two
weeks
after
the
requirements
are
met
or
it
becomes
kno
wn
that
a
general
-
pur
pose
AI
model
will
meet
the
requirements
that
lead
to
the
presump
tion
.
This
is
especially
relevant
in
relation
to
the
threshold
of
floating
point
operations
because
training
of
gene
ral
-
pur
pose
AI
models
take
s
considerable
planning
which
includes
the
upfront
allocation
of
com
pute
resources
and
,
theref
ore
,
provid
ers
of
general
-
pur
pose
AI
models
are
able
to
kno
w
if
their
model
would
meet
the
threshold
before
the
training
is
complet
ed
.
In
the
cont
ext
of
that
notif
ication
,
the
provid
er
should
be
able
to
demonstrat
e
that
,
because
of
its
specific
character
istics
,
a
general
-
pur
pose
AI
model
exceptionally
does
not
present
syste
mic
risks
,
and
that
it
thus
should
not
be
classified
as
a
general
-
pur
pose
AI
model
with
syste
mic
risks
.
That
information
is
valuable
for
the
AI
Office
to
anticipate
the
placing
on
the
market
of
general
-
pur
pose
AI
models
with
syste
mic
risks
and
the
provider
s
can
start
to
engag
e
with
the
AI
Office
early
on
.
That
information
is
especially
the
open
-
source
model
release
,
necessary
measures
to
ensure
compliance
with
the
obligations
under
this
Regulation
may
be
more
diffic
ult
to
imp
lement
.
(
113
)
If
the
Commission
becomes
awar
e
of
the
fact
that
a
general
-
pur
pose
AI
model
meets
the
requirements
to
classify
as
a
general
-
pur
pose
AI
model
with
syste
mic
risk
,
which
previously
had
either
not
been
kno
wn
or
of
which
the
relevant
provider
has
failed
to
notify
the
Commission
,
the
Commission
should
be
empo
wered
to
designate
it
so
.
A
system
of
qualified
alerts
should
ensure
that
the
AI
Offi
ce
is
made
aware
by
the
scientifi
c
panel
of
general
-
pur
pose
AI
models
that
should
possibly
be
classified
as
general
-
pur
pose
AI
models
with
syste
mic
risk
,
in
addition
to
the
monitoring
activities
of
the
AI
Office
.
(
114
)
The
providers
of
general
-
pur
pose
AI
models
presenting
syste
mic
risks
should
be
subject
,
in
addition
to
the
oblig
ations
provid
ed
for
provid
ers
of
general
-
pur
pose
AI
models
,
to
oblig
ations
aimed
at
identifying
and
mitig
ating
those
risks
and
ensur
ing
an
adequate
level
of
cybersecurity
protection
,
regardless
of
whether
it
is
provided
as
a
standalone
model
or
embedded
in
an
AI
system
or
a
product
.
To
achi
eve
those
objectives
,
this
Regulation
should
require
provid
ers
to
perfo
rm
the
necessary
model
evaluations
,
in
particular
prior
to
its
first
placing
on
the
mark
et
,
including
conducting
and
documenting
adversar
ial
testing
of
models
,
also
,
as
appropriate
,
through
inter
nal
or
independent
exte
rnal
testing
.
In
addition
,
provider
s
of
general
-
pur
pose
AI
models
with
systemic
risks
should
continuously
assess
and
mitig
ate
systemic
risks
,
including
for
example
by
putting
in
place
risk
-
management
policies
,
such
as
accountability
and
gover
nance
processes
,
imp
lementing
post
-
mark
et
monitoring
,
taking
appropriate
measures
along
the
entire
model
’s
lifecycle
and
cooperating
with
relevant
actor
s
along
the
AI
value
chai
n.
(
115
)
Provi
ders
of
general
-
pur
pose
AI
models
with
systemic
risks
should
assess
and
mitiga
te
possible
systemic
risks
.
If
,
despite
efforts
to
identify
and
prevent
risks
related
to
a
general
-
pur
pose
AI
model
that
may
present
syste
mic
risks
,
the
development
or
use
of
the
model
causes
a
serious
incident
,
the
general
-
pur
pose
AI
model
provid
er
should
without
undue
dela
y
keep
track
of
the
incident
and
repor
t
any
relevant
information
and
possible
corrective
measures
to
the
Commission
and
national
competent
authorities
.
Further
more
,
provider
s
should
ensure
an
adequate
level
of
cybersecurity
protection
for
the
model
and
its
physical
infrastructure
,
if
appropriate
,
along
the
entire
model
lifecy
cle
.
Cybersecurity
protection
related
to
systemic
risks
associated
with
malicious
use
or
attacks
should
duly
consider
accidental
model
leakage
,
unauthorised
releases
,
circum
vention
of
safety
measures
,
and
defe
nce
against
cyberattacks
,
unauthorised
access
or
model
thef
t.
That
protection
could
be
facili
tated
by
secur
ing
model
weights
,
algor
ithms
,
servers
,
and
data
sets
,
such
as
through
operational
security
measures
for
information
security
,
specific
cybersecurity
policies
,
adequat
e
technical
and
established
solutions
,
and
cyber
and
phys
ical
access
controls
,
appropriate
to
the
relevant
circumstances
and
the
risks
involved
.
(
116
)
The
AI
Office
should
encourage
and
facilitate
the
drawing
up
,
review
and
adaptation
of
codes
of
practice
,
taking
into
account
inter
national
approaches
.
All
providers
of
gene
ral
-
pur
pose
AI
models
could
be
invit
ed
to
participate
.
To
ensure
that
the
codes
of
practice
reflect
the
state
of
the
art
and
duly
take
into
account
a
diverse
set
of
perspectives
,
the
AI
Office
should
collaborate
with
relevant
national
comp
etent
authorities
,
and
could
,
where
appropriate
,
consult
with
civil
society
organisations
and
other
relevant
stakeholders
and
exper
ts
,
including
the
Scientif
ic
Panel
,
for
the
drawing
up
of
such
codes
.
Codes
of
practice
should
cover
oblig
ations
for
provider
s
of
general
-
pur
pose
AI
models
and
of
general
-
pur
pose
AI
models
presenting
systemic
risks
.
In
addition
,
as
regard
s
systemic
risks
,
codes
of
practice
should
help
to
establish
a
risk
taxonomy
of
the
type
and
nature
of
the
syste
mic
risks
at
Union
level
,
including
their
sources
.
Codes
of
practice
should
also
be
focused
on
specific
risk
assessment
and
mitiga
tion
measures
.
(
117
)
The
codes
of
practice
should
represent
a
central
tool
for
the
proper
compliance
with
the
obligations
provided
for
under
this
Regulation
for
provider
s
of
general
-
pur
pose
AI
models
.
Provi
ders
should
be
able
to
rely
on
codes
of
practice
to
demonstrate
compliance
with
the
obligations
.
By
means
of
imp
lementing
acts
,
the
Commission
may
decide
to
approve
a
code
of
practice
and
give
it
a
general
validity
within
the
Union
,
or
,
alternatively
,
to
provide
common
rules
for
the
imp
lementation
of
the
relevant
obligations
,
if
,
by
the
time
this
Regulation
becomes
applicable
,
a
code
of
practice
can
not
be
final
ised
or
is
not
deemed
adequate
by
the
AI
Office
.
Once
a
harmonised
standard
is
and
assessed
as
suitable
to
cover
the
relevant
oblig
ations
by
the
AI
Office
,
compliance
with
a
European
harmonised
standard
should
grant
providers
the
presump
tion
of
conf
ormity
.
Provi
ders
of
general
-
pur
pose
AI
models
should
further
more
be
able
to
demonstrate
compliance
using
alter
native
adequate
means
,
if
codes
of
practice
or
harmonised
standards
are
not
available
,
or
they
choose
not
to
rely
on
those
.
(
118
)
This
Regulation
regulate
s
AI
systems
and
AI
models
by
imposing
certain
requirements
and
obligations
for
relevant
mark
et
actor
s
that
are
placing
them
on
the
market
,
putting
into
service
or
use
in
the
Union
,
thereby
complementing
oblig
ations
for
provid
ers
of
inter
mediar
y
services
that
embed
such
systems
or
models
into
their
services
regulate
d
by
Regulation
(
EU
)
.
To
the
extent
that
such
systems
or
models
are
embedded
into
designated
very
large
online
platforms
or
very
large
online
search
engines
,
they
are
subject
to
the
risk
-
management
framework
provid
ed
for
in
Regulation
(
EU
)
.
Consequently
,
the
corresponding
obligations
of
this
Regulation
should
be
presumed
to
be
fulfilled
,
unless
significant
syste
mic
risks
not
covered
by
Regulation
(
EU
)
emerg
e
and
are
identified
in
such
models
.
Within
this
framework
,
providers
of
very
large
online
platforms
and
very
large
online
searc
h
engines
are
oblige
d
to
assess
potential
systemic
risks
stemming
from
the
design
,
functioning
and
use
of
their
services
,
including
how
the
design
of
algor
ithmic
systems
used
in
the
service
may
contribut
e
to
such
risks
,
as
well
as
syste
mic
risks
stemming
from
potent
ial
misuses
.
Those
provid
ers
are
also
oblige
d
to
take
appropriate
mitig
ating
measures
in
obser
vance
of
fundamental
rights
.
(
119
)
Consider
ing
the
quick
pace
of
innovation
and
the
technological
evolution
of
digital
services
in
scope
of
different
instr
uments
of
Union
law
in
particular
having
in
mind
the
usage
and
the
percep
tion
of
their
recipients
,
the
AI
systems
subject
to
this
Regulation
may
be
provid
ed
as
interm
ediar
y
services
or
parts
thereof
within
the
meaning
of
Regulation
(
EU
)
,
which
should
be
inter
prete
d
in
a
technology
-
neutral
manner
.
For
example
,
AI
systems
may
be
used
to
provide
online
search
engines
,
in
particular
,
to
the
extent
that
an
AI
syste
m
such
as
an
online
chatbot
perf
orms
search
es
of
,
in
principle
,
all
websites
,
then
incor
porates
the
results
into
its
existing
knowle
dge
and
uses
the
updat
ed
knowledg
e
to
generate
a
sing
le
output
that
combines
different
sources
of
information
.
(
120
)
Further
more
,
obligations
placed
on
provid
ers
and
deplo
yers
of
certain
AI
systems
in
this
Regulation
to
enable
the
detection
and
disclosure
that
the
outputs
of
those
systems
are
artificially
generat
ed
or
manipulated
are
particularly
relevant
to
facilitate
the
effective
imp
lementation
of
Regulation
(
EU
)
.
This
applies
in
particular
as
regard
s
the
obligations
of
provid
ers
of
very
large
online
platforms
or
very
large
online
search
engines
to
identify
and
mitig
ate
syste
mic
risks
that
may
arise
from
the
dissemination
of
content
that
has
been
artificially
generated
or
manipulat
ed
,
in
particular
risk
of
the
actual
or
foreseeable
negati
ve
effects
on
democratic
processes
,
civic
discourse
and
elect
oral
processes
,
including
through
disinf
ormation
.
(
121
)
Standardisation
should
play
a
key
role
to
provid
e
technical
solutions
to
provider
s
to
ensure
compliance
with
this
Regulation
,
in
line
with
the
state
of
the
art
,
to
promote
innovation
as
well
as
competitiveness
and
growth
in
the
sing
le
market
.
compliance
with
harmonised
standards
as
defined
in
Article
2
,
point(c
)
,
of
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
,
which
are
normally
expected
to
reflect
the
state
of
the
art
,
should
be
a
means
for
providers
to
demonstrate
conf
ormity
with
the
requirements
of
this
Regulation
.
A
balanced
representation
of
interests
involving
all
relevant
stak
eholders
in
the
development
of
standards
,
in
particular
SMEs
,
consumer
organisations
and
environmental
and
social
stakeholders
in
accordance
with
Articles
5
and
6
of
Regulation
(
EU
)
No
should
theref
ore
be
encouraged
.
In
order
to
facilitate
compliance
,
the
standardisation
requests
should
be
issued
by
the
Commission
without
undue
dela
y.
When
prepar
ing
the
standardisation
request
,
the
Commission
should
consult
the
advisor
y
forum
and
the
Board
in
order
to
collect
relevant
exper
tise
.
However
,
in
the
absence
of
relevant
refere
nces
to
harmonised
standards
,
the
Commission
should
be
able
to
establish
,
via
implementing
acts
,
and
after
consultation
of
the
advisor
y
forum
,
common
specifications
for
certain
requirements
under
this
Regulation
.
The
common
specification
should
be
an
exceptional
fall
back
solution
to
facilitate
the
provid
er
’s
obligation
to
comp
ly
with
the
requirements
of
this
Regulation
,
when
the
standardisation
request
has
not
been
accept
ed
by
any
of
the
European
standardisation
organisations
,
or
when
the
relevant
harmonised
standards
insufficiently
address
fundamental
rights
concer
ns
,
or
when
the
harmonised
standards
do
not
com
ply
with
the
request
,
or
when
there
are
dela
ys
in
the
adoption
of
an
appropriate
harmonised
standard
.
Where
such
a
dela
y
in
the
adoption
of
a
harmonised
standard
is
due
to
the
technical
complexity
of
that
standard
,
this
should
(
41
)
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
of
25
October
2012
on
European
standardisation
,
amending
Council
Directives
/EEC
and
/EEC
and
Directives
/EC
,
/EC
,
/EC
,
/EC
,
/EC
,
/EC
,
/EC
,
/EC
and
/EC
of
the
European
Parliament
and
of
the
Council
and
repealing
Council
Decision
/EEC
and
Decision
No
/EC
of
the
European
Parliament
and
of
the
Council
(
11.2012
,
p.
12).be
considered
by
the
Commission
before
contem
plating
the
establishment
of
common
specifications
.
When
developing
common
specific
ations
,
the
Commission
is
encourage
d
to
cooperate
with
international
partners
and
international
standardisation
bodies
.
(
122
)
It
is
appropriate
that
,
without
prejudice
to
the
use
of
harmonised
standards
and
common
specifications
,
providers
of
a
higher
isk
AI
syste
m
that
has
been
trained
and
tested
on
data
reflecting
the
specific
geographical
,
behavi
oural
,
cont
extual
or
functional
setting
within
which
the
AI
system
is
intended
to
be
used
,
should
be
presumed
to
comp
ly
with
the
relevant
measure
provided
for
under
the
requirement
on
data
gove
rnance
set
out
in
this
Regulation
.
Without
prejudice
to
the
requirements
related
to
robustness
and
accuracy
set
out
in
this
Regulation
,
in
accordance
with
Article
54(3
)
of
Regulation
(
EU
)
,
higher
isk
AI
systems
that
have
been
certified
or
for
which
a
statement
of
conf
ormity
has
been
issued
under
a
cybersecurity
scheme
pursuant
to
that
Regulation
and
the
references
of
which
have
been
published
in
the
of
the
European
Union
should
be
presumed
to
comply
with
the
cybersecurity
requirement
of
this
Regulation
in
so
far
as
the
cybersecurity
certificate
or
statem
ent
of
conf
ormity
or
parts
thereof
cover
the
cybersecurity
requirement
of
this
Regulation
.
This
remains
without
prejudice
to
the
voluntar
y
nature
of
that
cybersecurity
scheme
.
(
123
)
In
order
to
ensure
a
high
level
of
trustw
orthiness
of
higher
isk
AI
systems
,
those
systems
should
be
subject
to
a
conf
ormity
assessment
prior
to
their
placing
on
the
market
or
putting
into
service
.
(
124
)
It
is
appropriate
that
,
in
order
to
minimise
the
burden
on
operators
and
avoid
any
possible
duplication
,
for
higher
isk
AI
systems
related
to
products
which
are
cover
ed
by
existing
Union
harmonisation
legislation
based
on
the
New
Legislative
Framewo
rk
,
the
compliance
of
those
AI
systems
with
the
requirements
of
this
Regulation
should
be
assessed
as
part
of
the
conf
ormity
assessment
already
provided
for
in
that
law
.
The
applicability
of
the
requirements
of
this
Regulation
should
thus
not
affect
the
specific
logic
,
methodology
or
general
structure
of
conf
ormity
assessment
under
the
relevant
Union
harmonisation
legislation
.
(
125
)
Given
the
comp
lexity
of
higher
isk
AI
systems
and
the
risks
that
are
associate
d
with
them
,
it
is
impor
tant
to
develop
an
adequate
conf
ormity
assessment
procedure
for
higher
isk
AI
systems
involving
notified
bodies
,
so
-
called
third
party
conf
ormity
assessment
.
However
,
given
the
current
exper
ience
of
professional
pre
-
market
certifiers
in
the
field
of
product
safety
and
the
different
nature
of
risks
involved
,
it
is
appropriate
to
limit
,
at
least
in
an
initial
phase
of
application
of
this
Regulation
,
the
scope
of
application
of
third
-
par
ty
conf
ormity
assessment
for
higher
isk
AI
systems
other
than
those
related
to
products
.
Theref
ore
,
the
conf
ormity
assessment
of
such
systems
should
be
carried
out
as
a
general
rule
by
the
provider
under
its
own
responsibility
,
with
the
only
excep
tion
of
AI
systems
intended
to
be
used
for
biometrics
.
(
126
)
In
order
to
carry
out
third
-
par
ty
conf
ormity
assessments
when
so
required
,
notified
bodies
should
be
notif
ied
under
this
Regulation
by
the
national
comp
etent
authorities
,
provided
that
they
comp
ly
with
a
set
of
requirements
,
in
particular
on
independence
,
compet
ence
,
absence
of
conf
licts
of
interests
and
suitable
cybersecurity
requirements
.
Notification
of
those
bodies
should
be
sent
by
national
competent
authorities
to
the
Commission
and
the
other
Member
States
by
means
of
the
electronic
notif
ication
tool
developed
and
managed
by
the
Commission
pursuant
to
Article
R23
of
Annex
I
to
Decision
No
/EC
.
(
127
)
In
line
with
Union
commitments
under
the
World
Trade
Organization
Agreement
on
Technical
Barriers
to
Trade
,
it
is
adequat
e
to
facilitate
the
mutual
recognition
of
conf
ormity
assessment
results
produced
by
comp
etent
conf
ormity
assessment
bodies
,
independent
of
the
territory
in
which
they
are
established
,
provid
ed
that
those
conf
ormity
assessment
bodies
established
under
the
law
of
a
third
countr
y
meet
the
applicable
requirements
of
this
Regulation
and
the
Union
has
concluded
an
agreement
to
that
extent
.
In
this
context
,
the
Commission
should
actively
explore
possible
international
instr
uments
for
that
purpose
and
in
particular
pursue
the
conclusion
of
mutual
recognition
agreements
with
third
countr
ies
.
(
128
)
In
line
with
the
commonly
established
notion
of
substantial
modif
ication
for
products
regulate
d
by
Union
harmonisation
legislation
,
it
is
appropriate
that
whenever
a
chang
e
occurs
which
may
affect
the
compliance
of
a
higher
isk
AI
system
with
this
Regulation
(
e.g.
change
of
operating
syste
m
or
software
arch
itecture
)
,
or
when
the
intended
purpose
of
the
syste
m
change
s
,
that
AI
syste
m
should
be
considered
to
be
a
new
AI
system
which
should
undergo
a
new
conf
ormity
assessment
.
How
ever
,
changes
occur
ring
to
the
algor
ithm
and
the
perf
ormance
of
AI
systems
which
continue
to
‘
lear
n
’
after
being
placed
on
the
market
or
put
into
service
,
namely
automat
ically
adap
ting
how
functions
are
carried
out
,
should
not
constitute
a
substantial
modifi
cation
,
provid
ed
that
those
change
s
have
been
pre
-
dete
rmined
by
the
provider
and
assessed
at
the
moment
of
the
conf
ormity
assessment
.
higher
isk
AI
systems
should
bear
the
CE
marking
to
indicate
their
conf
ormity
with
this
Regulation
so
that
they
can
move
freely
within
the
intern
al
mark
et
.
For
higher
isk
AI
systems
embedded
in
a
product
,
a
physica
l
CE
marking
should
be
affixed
,
and
may
be
comp
lemented
by
a
digital
CE
marking
.
For
higher
isk
AI
systems
only
provid
ed
digitally
,
a
digital
CE
marking
should
be
used
.
Member
States
should
not
create
unjustified
obstacles
to
the
placing
on
the
market
or
the
putting
into
service
of
higher
isk
AI
systems
that
comp
ly
with
the
requirements
laid
down
in
this
Regulation
and
bear
the
CE
marking
.
(
130
)
Und
er
certain
conditions
,
rapid
availability
of
inno
vative
technologies
may
be
crucial
for
health
and
safety
of
persons
,
the
protection
of
the
environment
and
climate
change
and
for
society
as
a
whole
.
It
is
thus
appropriate
that
under
exceptional
reasons
of
public
security
or
protection
of
life
and
health
of
natural
persons
,
environmental
protection
and
the
protection
of
key
industr
ial
and
infrastr
uctural
assets
,
market
surveillance
authorities
could
author
ise
the
placing
on
the
market
or
the
putting
into
service
of
AI
systems
which
have
not
undergone
a
conf
ormity
assessment
.
In
duly
justified
situations
,
as
provided
for
in
this
Regulation
,
law
enforcement
authorities
or
civil
protection
authorities
may
put
a
specific
higher
isk
AI
syste
m
into
service
without
the
author
isation
of
the
mark
et
surveillance
author
ity
,
provided
that
such
author
isation
is
request
ed
during
or
after
the
use
without
undue
dela
y.
(
131
)
In
order
to
facilitate
the
work
of
the
Commission
and
the
Member
States
in
the
AI
field
as
well
as
to
increase
the
transparency
towards
the
public
,
provider
s
of
higher
isk
AI
systems
other
than
those
related
to
products
falling
within
the
scope
of
relevant
existing
Union
harmonisation
legislation
,
as
well
as
providers
who
consider
that
an
AI
system
listed
in
the
higher
isk
use
cases
in
an
annex
to
this
Regulation
is
not
higher
isk
on
the
basis
of
a
deroga
tion
,
should
be
required
to
register
themselves
and
information
about
their
AI
syste
m
in
an
EU
database
,
to
be
established
and
manag
ed
by
the
Commission
.
Before
using
an
AI
syste
m
listed
in
the
higher
isk
use
cases
in
an
annex
to
this
Regulation
,
deplo
yers
of
higher
isk
AI
systems
that
are
public
authorities
,
agencies
or
bodies
,
should
regist
er
themselves
in
such
database
and
select
the
syste
m
that
they
envisag
e
to
use
.
Other
deplo
yers
should
be
entitled
to
do
so
voluntar
ily
.
This
section
of
the
EU
database
should
be
publicly
accessible
,
free
of
charg
e
,
the
information
should
be
easily
navig
able
,
understandable
and
machine
-
r
eadable
.
The
EU
database
should
also
be
user
-friendly
,
for
example
by
providing
search
functionalities
,
including
through
keywo
rds
,
allowi
ng
the
general
public
to
find
relevant
information
to
be
submitted
upon
the
registration
of
higher
isk
AI
systems
and
on
the
use
case
of
higher
isk
AI
systems
,
set
out
in
an
annex
to
this
Regulation
,
to
which
the
higher
isk
AI
systems
correspond
.
Any
substantial
modification
of
higher
isk
AI
systems
should
also
be
registered
in
the
EU
database
.
For
higher
isk
AI
systems
in
the
area
of
law
enforcement
,
migration
,
asylum
and
border
control
managem
ent
,
the
registration
obligations
should
be
fulfilled
in
a
secure
non
-
public
section
of
the
EU
database
.
Access
to
the
secure
non
-
public
section
should
be
strictly
limit
ed
to
the
Commission
as
well
as
to
market
surveillance
authorities
with
regard
to
their
national
section
of
that
database
.
higher
isk
AI
systems
in
the
area
of
critical
infrastructure
should
only
be
register
ed
at
national
level
.
The
Commission
should
be
the
controller
of
the
EU
database
,
in
accordance
with
Regulation
(
EU
)
.
In
order
to
ensure
the
full
functionality
of
the
EU
database
,
when
deployed
,
the
procedure
for
setting
the
database
should
include
the
development
of
functional
specifications
by
the
Commission
and
an
independent
audit
repor
t.
The
Commission
should
take
into
account
cybersecurity
risks
when
carrying
out
its
tasks
as
data
controller
on
the
EU
database
.
In
order
to
maximise
the
availability
and
use
of
the
EU
database
by
the
public
,
the
EU
database
,
including
the
information
made
available
through
it
,
should
comply
with
requirements
under
the
Directive
(
EU
)
.
(
132
)
Certain
AI
systems
intended
to
interac
t
with
natural
persons
or
to
generate
cont
ent
may
pose
specific
risks
of
impersonation
or
decept
ion
irrespective
of
whether
they
qualify
as
higher
isk
or
not
.
In
certain
circumstances
,
the
use
of
these
systems
should
theref
ore
be
subject
to
specific
transparency
obligations
without
prejudice
to
the
requirements
and
obligations
for
higher
isk
AI
systems
and
subject
to
target
ed
exceptions
to
take
into
account
the
special
need
of
law
enforcement
.
In
particular
,
natural
persons
should
be
notified
that
they
are
interacting
with
an
AI
syste
m
,
unless
this
is
obvious
from
the
point
of
view
of
a
natural
person
who
is
reasonably
well
-
inf
ormed
,
obser
vant
and
circumspect
taking
into
account
the
circumstances
and
the
context
of
use
.
When
implementing
that
obligation
,
the
character
istics
of
natural
persons
belonging
to
vulnerable
groups
due
to
their
age
or
disability
should
be
take
n
into
account
to
the
extent
the
AI
system
is
intended
to
interact
with
those
groups
as
well
.
Moreove
r
,
natural
persons
should
be
notified
when
they
are
exposed
to
AI
systems
that
,
by
processing
their
biometric
data
,
can
identify
or
infer
the
emotions
or
intentions
of
those
persons
or
assign
them
to
specific
cate
gories
.
Such
specific
categor
ies
can
relate
to
aspects
such
as
sex
,
age
,
hair
colour
,
eye
colour
,
tattoos
,
personal
traits
,
ethnic
origin
,
personal
pref
erences
and
interests
.
Such
information
and
notif
ications
should
be
provided
in
accessible
formats
for
persons
with
disabilities
.
(
133
)
A
variety
of
AI
systems
can
generate
large
quantities
of
synthetic
cont
ent
that
becomes
increasing
ly
hard
for
humans
to
distinguish
from
human
-
generat
ed
and
authentic
cont
ent
.
The
wide
availa
bility
and
increasing
capabilities
of
those
systems
have
a
significant
impact
on
the
integr
ity
and
trust
in
the
information
ecosystem
,
raising
new
risks
of
misinf
ormation
and
manipulation
at
scale
,
fraud
,
impersonation
and
consumer
decep
tion
.
In
light
of
those
impacts
,
the
fast
technological
pace
and
the
need
for
new
methods
and
techniques
to
trace
origin
of
information
,
it
is
appropriate
to
require
providers
of
those
systems
to
embed
technical
solutions
that
enable
marking
in
a
machi
ne
readable
format
and
detect
ion
that
the
output
has
been
generated
or
manipulate
d
by
an
AI
syste
m
and
not
a
human
.
Such
techniques
and
methods
should
be
sufficiently
reliable
,
interoperable
,
effective
and
robust
as
far
as
this
is
technically
feasible
,
taking
into
account
available
techniques
or
a
combination
of
such
techniques
,
such
as
watermarks
,
metadata
identifica
tions
,
cryptographic
methods
for
provin
g
prove
nance
and
authenticity
of
cont
ent
,
logging
methods
,
finger
prints
or
other
techniques
,
as
may
be
appropriate
.
When
imp
lementing
this
obligation
,
providers
should
also
take
into
account
the
specificities
and
the
limitations
of
the
different
types
of
cont
ent
and
the
relevant
technological
and
mark
et
developments
in
the
field
,
as
reflected
in
the
generally
ackno
wledged
state
of
the
art
.
Such
techniques
and
methods
can
be
implemented
at
the
level
of
the
AI
syste
m
or
at
the
level
of
the
AI
model
,
including
general
-
pur
pose
AI
models
generating
content
,
thereby
facilitating
fulfilment
of
this
obligation
by
the
downstream
provid
er
of
the
AI
system
.
To
remain
propor
tionate
,
it
is
appropriate
to
envisage
that
this
marking
oblig
ation
should
not
cover
AI
systems
perf
orming
primar
ily
an
assistive
function
for
standard
editing
or
AI
systems
not
substantially
alter
ing
the
input
data
provided
by
the
deplo
yer
or
the
semantics
thereof
.
(
134
)
Further
to
the
technical
solutions
emp
loyed
by
the
provider
s
of
the
AI
system
,
deplo
yers
who
use
an
AI
syste
m
to
generate
or
manipulate
imag
e
,
audio
or
video
content
that
appreciably
resembles
existing
persons
,
objects
,
places
,
entities
or
events
and
would
falsely
appear
to
a
person
to
be
authentic
or
truthful
(
deep
fakes
)
,
should
also
clearly
and
distinguishably
disclose
that
the
cont
ent
has
been
artificially
create
d
or
manipulated
by
labelling
the
AI
output
according
ly
and
disclosing
its
artificial
origin
.
Compliance
with
this
transparency
obligation
should
not
be
interpreted
as
indicating
that
the
use
of
the
AI
syste
m
or
its
output
impedes
the
right
to
freedom
of
expression
and
the
right
to
freedom
of
the
arts
and
sciences
guarante
ed
in
the
Charter
,
in
particular
where
the
content
is
part
of
an
evidently
creative
,
satir
ical
,
artistic
,
fictional
or
analogous
work
or
programme
,
subject
to
appropriate
safeguards
for
the
rights
and
freedoms
of
third
parties
.
In
those
cases
,
the
transparency
obligation
for
deep
fakes
set
out
in
this
Regulation
is
limited
to
disclosure
of
the
exist
ence
of
such
generated
or
manipulate
d
cont
ent
in
an
appropriate
manner
that
does
not
hamper
the
displa
y
or
enjo
yment
of
the
work
,
including
its
normal
exploitation
and
use
,
while
maintaining
the
utility
and
quality
of
the
work
.
In
addition
,
it
is
also
appropriate
to
envisag
e
a
similar
disclosure
oblig
ation
in
relation
to
AI
-
g
enerated
or
manipulate
d
text
to
the
extent
it
is
published
with
the
purpose
of
informing
the
public
on
matters
of
public
interest
unless
the
AI
-
g
enerated
content
has
undergone
a
process
of
human
review
or
editorial
control
and
a
natural
or
legal
person
holds
editor
ial
responsibility
for
the
publication
of
the
content
.
(
135
)
Without
prejudice
to
the
mandat
ory
nature
and
full
applicability
of
the
transparency
oblig
ations
,
the
Commission
may
also
encourage
and
facilitate
the
drawing
up
of
codes
of
practice
at
Union
level
to
facilitate
the
effective
implementation
of
the
obligations
regarding
the
detection
and
labelling
of
artificially
generated
or
manipulated
cont
ent
,
including
to
support
practical
arrang
ements
for
making
,
as
appropriate
,
the
detection
mec
hanisms
accessible
and
facilitating
cooperation
with
other
actor
s
along
the
value
chain
,
disseminating
content
or
chec
king
its
authenticity
and
provenance
to
enable
the
public
to
effectively
distinguish
AI
-
g
enerated
cont
ent
.
(
136
)
The
obligations
placed
on
provid
ers
and
deplo
yers
of
certain
AI
systems
in
this
Regulation
to
enable
the
detection
and
disclosure
that
the
outputs
of
those
systems
are
artificially
generat
ed
or
manipulat
ed
are
particularly
relevant
to
facilitate
the
effective
imp
lementation
of
Regulation
(
EU
)
.
This
applies
in
particular
as
rega
rds
the
oblig
ations
of
providers
of
very
large
online
platforms
or
very
large
online
search
engines
to
identify
and
mitigat
e
syste
mic
risks
that
may
arise
from
the
dissemination
of
cont
ent
that
has
been
artificially
generated
or
manipulate
d
,
in
particular
the
risk
of
the
actual
or
foreseeable
nega
tive
effects
on
democratic
processes
,
civic
discourse
and
elect
oral
processes
,
including
through
disinf
ormation
.
The
requirement
to
label
content
generated
by
AI
systems
under
this
Regulation
is
without
prejudice
to
the
obligation
in
Article
16(6
)
of
Regulation
(
EU
)
for
providers
of
hosting
services
to
process
notices
on
illegal
content
received
pursuant
to
Article
16(1
)
of
that
Regulation
and
should
not
influence
the
assessment
and
the
decision
on
the
illegality
of
the
specific
content
.
That
assessment
should
be
perform
ed
solely
with
reference
to
the
rules
governing
the
lega
lity
of
the
cont
ent
.
compliance
with
the
transparency
obligations
for
the
AI
systems
cover
ed
by
this
Regulation
should
not
be
interpreted
as
indicating
that
the
use
of
the
AI
syste
m
or
its
output
is
lawful
under
this
Regulation
or
other
Union
and
Member
State
law
and
should
be
without
prejudice
to
other
transparency
obligations
for
deplo
yers
of
AI
systems
laid
down
in
Union
or
national
law
.
(
138
)
AI
is
a
rapidly
developing
family
of
technologies
that
requires
regulatory
oversight
and
a
safe
and
controlled
space
for
exper
imentation
,
while
ensur
ing
responsible
innovation
and
integration
of
appropriate
safegua
rds
and
risk
mitigation
measures
.
To
ensure
a
lega
l
framework
that
promotes
innovation
,
is
future
-
proof
and
resilient
to
disruption
,
Member
States
should
ensure
that
their
national
competent
authorities
establish
at
least
one
AI
regulat
ory
sandbo
x
at
national
level
to
facilitate
the
development
and
testing
of
inno
vative
AI
systems
under
strict
regulat
ory
oversight
before
these
systems
are
placed
on
the
market
or
other
wise
put
into
service
.
Member
States
could
also
fulfil
this
obligation
through
participating
in
already
existing
regulator
y
sandbo
xes
or
establishing
jointly
a
sandbo
x
with
one
or
more
Member
States
’
competent
authorities
,
insofar
as
this
participation
provid
es
equivalent
level
of
national
coverag
e
for
the
participating
Member
States
.
AI
regulator
y
sandbo
xes
could
be
established
in
phys
ical
,
digital
or
hybri
d
form
and
may
accommodate
physica
l
as
well
as
digital
products
.
Establishing
authorities
should
also
ensure
that
the
AI
regulatory
sandbo
xes
have
the
adequate
resources
for
their
functioning
,
including
financial
and
human
resources
.
(
139
)
The
objectives
of
the
AI
regulato
ry
sandbo
xes
should
be
to
foster
AI
innovation
by
establishing
a
controlled
exper
imentation
and
testing
environment
in
the
development
and
pre
-
marketing
phase
with
a
view
to
ensur
ing
compliance
of
the
inno
vative
AI
systems
with
this
Regulation
and
other
relevant
Union
and
national
law
.
Moreove
r
,
the
AI
regulato
ry
sandbo
xes
should
aim
to
enhance
legal
certainty
for
inno
vators
and
the
competent
authorities
’
oversight
and
understanding
of
the
oppor
tunities
,
emerging
risks
and
the
impacts
of
AI
use
,
to
facilitate
regulator
y
learning
for
authorities
and
undertakings
,
including
with
a
view
to
future
adap
tions
of
the
lega
l
framework
,
to
support
cooperation
and
the
shar
ing
of
best
practices
with
the
authorities
involved
in
the
AI
regulator
y
sandbo
x
,
and
to
accelerate
access
to
markets
,
including
by
removing
barriers
for
SMEs
,
including
start
-
ups
.
AI
regulator
y
sandbo
xes
should
be
widely
available
throughout
the
Union
,
and
particular
attention
should
be
given
to
their
accessibility
for
SMEs
,
including
start
-
ups
.
The
participation
in
the
AI
regulato
ry
sandbo
x
should
focus
on
issues
that
raise
lega
l
uncer
tainty
for
provid
ers
and
prospective
provider
s
to
inno
vate
,
exper
iment
with
AI
in
the
Union
and
contribut
e
to
evidence
-
based
regulator
y
learning
.
The
super
vision
of
the
AI
systems
in
the
AI
regulator
y
sandbo
x
should
theref
ore
cover
their
development
,
training
,
testing
and
validation
before
the
systems
are
placed
on
the
mark
et
or
put
into
service
,
as
well
as
the
notion
and
occur
rence
of
substantial
modifi
cation
that
may
require
a
new
conf
ormity
assessment
procedure
.
Any
signifi
ca
nt
risks
identified
during
the
development
and
testing
of
such
AI
systems
should
result
in
adequat
e
mitiga
tion
and
,
failing
that
,
in
the
suspension
of
the
development
and
testing
process
.
Where
appropriate
,
national
com
petent
authorities
establishing
AI
regulator
y
sandbo
xes
should
cooperate
with
other
relevant
authorities
,
including
those
super
vising
the
protection
of
fundamental
rights
,
and
could
allow
for
the
involvement
of
other
actor
s
within
the
AI
ecosystem
such
as
national
or
European
standardisation
organisations
,
notified
bodies
,
testing
and
exper
imentation
facilities
,
research
and
exper
imentation
labs
,
European
Digital
innovation
Hubs
and
relevant
stak
eholder
and
civil
society
organisations
.
To
ensure
uniform
implementation
across
the
Union
and
economies
of
scale
,
it
is
appropriate
to
establish
common
rules
for
the
AI
regulator
y
sandbo
xes
’
implementation
and
a
framework
for
cooperation
between
the
relevant
authorities
involved
in
the
super
vision
of
the
sandbo
xes
.
AI
regulatory
sandbo
xes
established
under
this
Regulation
should
be
without
prejudice
to
other
law
allowing
for
the
establishment
of
other
sandbo
xes
aiming
to
ensure
compliance
with
law
other
than
this
Regulation
.
Where
appropriate
,
relevant
comp
etent
authorities
in
charge
of
those
other
regulato
ry
sandbo
xes
should
consider
the
benefits
of
using
those
sandbo
xes
also
for
the
purpose
of
ensur
ing
compliance
of
AI
systems
with
this
Regulation
.
Upon
agreement
between
the
national
competent
authorities
and
the
participants
in
the
AI
regulator
y
sandbo
x
,
testing
in
real
world
conditions
may
also
be
operated
and
super
vised
in
the
framework
of
the
AI
regulator
y
sandbo
x.
(
140
)
This
Regulation
should
provid
e
the
lega
l
basis
for
the
provid
ers
and
prospective
provid
ers
in
the
AI
regulator
y
sandbo
x
to
use
personal
data
collect
ed
for
other
purposes
for
developing
certain
AI
systems
in
the
public
interest
within
the
AI
regulator
y
sandbo
x
,
only
under
specif
ied
conditions
,
in
accordance
with
Article
6(4
)
and
Article
9(2
)
,
point
(
g
)
,
of
Regulation
(
EU
)
,
and
Articles
5
,
6
and
10
of
Regulation
(
EU
)
,
and
without
prejudice
to
Article
4(2
)
and
Article
10
of
Directive
(
EU
)
.
All
other
obligations
of
data
controllers
and
rights
of
data
subjects
under
Regulations
(
EU
)
and
(
EU
)
and
Directive
(
EU
)
remain
applicable
.
In
particular
,
this
Regulation
should
not
provide
a
lega
l
basis
in
the
meaning
of
Article
22(2
)
,
point
(
b
)
of
Regulation
(
EU
)
and
Article
24(2
)
,
point
(
b
)
of
Regulation
(
EU
)
.
Providers
and
prospective
authorities
,
including
by
following
their
guidance
and
acting
expeditiously
and
in
good
faith
to
adequat
ely
mitigat
e
any
identifie
d
signifi
ca
nt
risks
to
safety
,
health
,
and
fundamental
rights
that
may
arise
during
the
development
,
testing
and
exper
imentation
in
that
sandbo
x.
(
141
)
In
order
to
accelerate
the
process
of
development
and
the
placing
on
the
market
of
the
higher
isk
AI
systems
listed
in
an
annex
to
this
Regulation
,
it
is
impor
tant
that
provid
ers
or
prospective
providers
of
such
systems
may
also
benefit
from
a
specific
regime
for
testing
those
systems
in
real
world
conditions
,
without
participating
in
an
AI
regulator
y
sandbo
x.
However
,
in
such
cases
,
taking
into
account
the
possible
consequences
of
such
testing
on
individuals
,
it
should
be
ensured
that
appropriate
and
sufficient
guarantees
and
conditions
are
introduced
by
this
Regulation
for
providers
or
prospective
provider
s.
Such
guarantees
should
include
,
inter
alia
,
requesting
informed
consent
of
natural
persons
to
participate
in
testing
in
real
world
conditions
,
with
the
exception
of
law
enforcement
where
the
seeking
of
informed
consent
would
prevent
the
AI
syste
m
from
being
tested
.
Consent
of
subjects
to
participate
in
such
testing
under
this
Regulation
is
distinct
from
,
and
without
prejudice
to
,
consent
of
data
subjects
for
the
processing
of
their
personal
data
under
the
relevant
data
protection
law
.
It
is
also
imp
ortant
to
minimise
the
risks
and
enable
oversight
by
competent
authorities
and
theref
ore
require
prospective
provid
ers
to
have
a
real
-
wo
rld
testing
plan
submitted
to
comp
etent
mark
et
surveillance
author
ity
,
regist
er
the
testing
in
dedicated
sections
in
the
EU
database
subject
to
some
limited
exceptions
,
set
limitations
on
the
period
for
which
the
testing
can
be
done
and
require
additional
safegua
rds
for
persons
belonging
to
certain
vulnerable
groups
,
as
well
as
a
written
agreement
defining
the
roles
and
responsibilities
of
prospective
providers
and
deplo
yers
and
effective
overs
ight
by
competent
personnel
involved
in
the
real
world
testing
.
Further
more
,
it
is
appropriate
to
envisag
e
additional
safeguards
to
ensure
that
the
predictions
,
recommendations
or
decisions
of
the
AI
system
can
be
effectively
reversed
and
disreg
arded
and
that
personal
data
is
protected
and
is
deleted
when
the
subjects
have
withdra
wn
their
consent
to
participate
in
the
testing
without
prejudice
to
their
rights
as
data
subjects
under
the
Union
data
protection
law
.
As
rega
rds
transfer
of
data
,
it
is
also
appropriate
to
envis
age
that
data
collect
ed
and
processed
for
the
purpose
of
testing
in
real
-
world
conditions
should
be
transf
erred
to
third
countr
ies
only
where
appropriate
and
applicable
safeguards
under
Union
law
are
implemented
,
in
particular
in
accordance
with
bases
for
transf
er
of
personal
data
under
Union
law
on
data
protection
,
while
for
non
-
personal
data
appropriate
safeguards
are
put
in
place
in
accordance
with
Union
law
,
such
as
Regulations
(
EU
)
and
(
EU
)
of
the
European
Parliame
nt
and
of
the
Council
.
(
142
)
To
ensure
that
AI
leads
to
socially
and
environmentally
beneficial
outcomes
,
Member
States
are
encouraged
to
support
and
promote
research
and
development
of
AI
solutions
in
support
of
socially
and
environmentally
beneficial
outcomes
,
such
as
AI
-
based
solutions
to
increase
accessibility
for
persons
with
disabilities
,
tackle
socio
-
economic
inequalities
,
or
meet
environmental
targets
,
by
allocating
sufficient
resources
,
including
public
and
Union
funding
,
and
,
where
appropriate
and
provided
that
the
eligibility
and
selection
criteria
are
fulfilled
,
consider
ing
in
particular
projects
which
pursue
such
objectives
.
Such
projects
should
be
based
on
the
principle
of
interdisciplinar
y
cooperation
between
AI
developers
,
exper
ts
on
inequality
and
non
-
discr
imination
,
accessibility
,
consumer
,
environmental
,
and
digital
rights
,
as
well
as
academics
.
(
143
)
In
order
to
promot
e
and
protect
innovation
,
it
is
impor
tant
that
the
intere
sts
of
SMEs
,
including
start
-
ups
,
that
are
providers
or
deplo
yers
of
AI
systems
are
take
n
into
particular
account
.
To
that
end
,
Member
States
should
develop
initiatives
,
which
are
targe
ted
at
those
operators
,
including
on
awareness
raising
and
information
communication
.
Member
States
should
provide
SMEs
,
including
start
-
ups
,
that
have
a
regist
ered
office
or
a
branch
in
the
Union
,
with
priority
access
to
the
AI
regulator
y
sandbo
xes
provided
that
they
fulfil
the
eligibility
conditions
and
selection
criteria
and
without
precluding
other
providers
and
prospective
providers
to
access
the
sandbo
xes
provided
the
same
conditions
and
criteria
are
fulfilled
.
Member
States
should
utilise
existing
channels
and
where
appropriate
,
establish
new
dedicated
channels
for
communication
with
SMEs
,
including
start
-
ups
,
deplo
yers
,
other
innovat
ors
and
,
as
appropriate
,
local
public
authorities
,
to
support
SMEs
throughout
their
development
path
by
providing
guidance
and
responding
to
quer
ies
about
the
imp
lementation
of
this
Regulation
.
Where
appropriate
,
these
channels
should
work
together
to
create
synergies
and
ensure
homog
eneity
in
their
guidance
to
SMEs
,
including
start
-
ups
,
and
deplo
yers
.
Additionally
,
Member
States
should
facilitate
the
participation
of
SMEs
and
other
relevant
stakeholders
in
the
standardisation
development
processes
.
Moreover
,
the
specific
interests
and
needs
of
provid
ers
that
are
SMEs
,
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
30
May
2022
on
European
data
gover
nance
and
amending
Regulation
(
EU
)
(
Data
Governanc
e
Act
)
(
6.2022
,
p.
1
)
.
(
43
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
December
2023
on
harmonised
rules
on
fair
access
to
and
use
of
data
and
amending
Regulation
(
EU
)
and
Directive
(
EU
)
(
Data
Act).including
start
-
ups
,
should
be
take
n
into
account
when
notified
bodies
set
conf
ormity
assessment
fees
.
The
Commission
should
regularly
assess
the
certification
and
compliance
costs
for
SMEs
,
including
start
-
ups
,
through
transparent
consultations
and
should
work
with
Member
States
to
lower
such
costs
.
For
example
,
translation
costs
relate
d
to
mandat
ory
documentation
and
communication
with
authorities
may
constitute
a
signif
icant
cost
for
providers
and
other
operators
,
in
particular
those
of
a
smaller
scale
.
Member
States
should
possibly
ensure
that
one
of
the
languag
es
determin
ed
and
accep
ted
by
them
for
relevant
provider
s
’
documentation
and
for
communication
with
operators
is
one
which
is
broadly
understood
by
the
largest
possible
number
of
cross
-
border
deplo
yers
.
In
order
to
address
the
specific
needs
of
SMEs
,
including
start
-
ups
,
the
Commission
should
provid
e
standardised
templat
es
for
the
areas
covered
by
this
Regulation
,
upon
request
of
the
Board
.
Additiona
lly
,
the
Commission
should
comp
lement
Member
States
’
efforts
by
providing
a
sing
le
information
platform
with
easy
-
t
o
-
use
information
with
regard
s
to
this
Regulation
for
all
provid
ers
and
deplo
yers
,
by
organising
appropriate
communication
cam
paigns
to
raise
awareness
about
the
obligations
arising
from
this
Regulation
,
and
by
evaluating
and
promoting
the
conve
rgence
of
best
practices
in
public
procurement
procedures
in
relation
to
AI
systems
.
Medium
-
sized
enterprises
which
until
recently
qualified
as
small
enterprises
within
the
meaning
of
the
Annex
to
Commission
Recommendation
/EC
should
have
access
to
those
support
measures
,
as
those
new
medium
-
sized
enter
prises
may
sometimes
lack
the
legal
resources
and
training
necessary
to
ensure
proper
understanding
of
,
and
compliance
with
,
this
Regulation
.
(
144
)
In
order
to
promot
e
and
prot
ect
innovation
,
the
AI
-
on
-
demand
platform
,
all
relevant
Union
funding
programmes
and
projects
,
such
as
Digital
Europe
Programme
,
Hor
izon
Europe
,
implemented
by
the
Commission
and
the
Member
States
at
Union
or
national
level
should
,
as
appropriate
,
contribute
to
the
achievement
of
the
objectives
of
this
Regulation
.
(
145
)
In
order
to
minimise
the
risks
to
imp
lementation
resulting
from
lack
of
knowledg
e
and
exper
tise
in
the
market
as
well
as
to
facilitat
e
compliance
of
provid
ers
,
in
particular
SMEs
,
including
start
-
ups
,
and
notif
ied
bodies
with
their
oblig
ations
under
this
Regulation
,
the
AI
-
on
-
demand
platform
,
the
European
Digital
innovation
Hubs
and
the
testing
and
exper
imentation
facilities
established
by
the
Commission
and
the
Member
States
at
Union
or
national
level
should
contribute
to
the
implementation
of
this
Regulation
.
Within
their
respective
mission
and
fields
of
com
petence
,
the
AI
-
on
-
demand
platform
,
the
European
Digital
innovation
Hubs
and
the
testing
and
exper
imentation
Facilities
are
able
to
provide
in
particular
technical
and
scientific
support
to
provider
s
and
notified
bodies
.
(
146
)
Moreo
ver
,
in
light
of
the
very
small
size
of
some
operators
and
in
order
to
ensure
propor
tionality
regard
ing
costs
of
innovation
,
it
is
appropriate
to
allow
microenterp
rises
to
fulfil
one
of
the
most
costly
oblig
ations
,
namely
to
establish
a
quality
management
syste
m
,
in
a
simplified
manner
which
would
reduce
the
administrative
burden
and
the
costs
for
those
enterprises
without
affecting
the
level
of
protection
and
the
need
for
compliance
with
the
requirements
for
higher
isk
AI
systems
.
The
Commission
should
develop
guidelines
to
specify
the
elements
of
the
quality
managem
ent
syste
m
to
be
fulfilled
in
this
simplified
manner
by
microent
erprises
.
(
147
)
It
is
appropriate
that
the
Commission
facilitates
,
to
the
extent
possible
,
access
to
testing
and
exper
imentation
facilities
to
bodies
,
groups
or
laborat
ories
established
or
accredit
ed
pursuant
to
any
relevant
Union
harmonisation
legislation
and
which
fulfil
tasks
in
the
context
of
conf
ormity
assessment
of
products
or
devices
covered
by
that
Union
harmonisation
legislation
.
This
is
,
in
particular
,
the
case
as
rega
rds
exper
t
panels
,
exper
t
laborator
ies
and
reference
laborator
ies
in
the
field
of
medical
devices
pursuant
to
Regulations
(
EU
)
and
(
EU
)
.
(
148
)
This
Regulation
should
establish
a
gover
nance
framework
that
both
allows
to
coordinate
and
support
the
application
of
this
Regulation
at
national
level
,
as
well
as
build
capabilities
at
Union
level
and
integrat
e
stakeholders
in
the
field
of
AI
.
The
effective
implementation
and
enforcement
of
this
Regulation
require
a
gove
rnance
framework
that
allows
to
coordinat
e
and
build
up
central
exper
tise
at
Union
level
.
The
AI
Office
was
established
by
Commission
Decision
and
has
as
its
mission
to
develop
Union
exper
tise
and
capabilities
in
the
field
of
AI
and
to
contribute
to
the
imp
lementation
of
Union
law
on
AI
.
Member
States
should
facilitate
the
tasks
of
the
AI
Offi
ce
with
a
view
to
support
the
development
of
Union
exper
tise
and
capabilities
at
Union
level
and
to
strengthen
the
functioning
of
the
digital
sing
le
market
.
Further
more
,
a
Board
composed
of
representatives
of
the
Member
States
,
a
scientific
panel
to
integrate
the
scientific
community
and
an
advisor
y
forum
to
contribute
stakeh
older
input
to
the
imp
lementation
of
this
Regulation
,
at
Union
and
national
level
,
should
be
established
.
The
development
of
Union
exper
tise
and
(
44
)
Commission
Recommendation
of
6
May
2003
concerning
the
definition
of
micro
,
small
and
medium
-
sized
enterprises
.
(
45
)
Commission
Decision
of
24.1.2024
establishing
the
European
Artificial
Intellig
ence
Office
C(2024
)
390.capabilities
should
also
include
making
use
of
existing
resources
and
exper
tise
,
in
particular
through
synergies
with
structures
built
up
in
the
cont
ext
of
the
Union
level
enforcement
of
other
law
and
synergies
with
related
initiatives
at
Union
level
,
such
as
the
EuroHPC
Joint
Und
ertaking
and
the
AI
testing
and
exper
imentation
facilities
under
the
Digital
Europe
Programme
.
(
149
)
In
order
to
facilitate
a
smooth
,
effective
and
harmonised
implementation
of
this
Regulation
a
Board
should
be
established
.
The
Board
should
reflect
the
various
interests
of
the
AI
eco
-
system
and
be
comp
osed
of
representatives
of
the
Member
States
.
The
Board
should
be
responsible
for
a
number
of
advisor
y
tasks
,
including
issuing
opinions
,
recommendations
,
advice
or
contributing
to
guidance
on
matt
ers
related
to
the
imp
lementation
of
this
Regulation
,
including
on
enforcement
matt
ers
,
technical
specifications
or
existing
standards
regarding
the
requirements
established
in
this
Regulation
and
providing
advice
to
the
Commission
and
the
Member
States
and
their
national
com
petent
authorities
on
specific
questions
related
to
AI
.
In
order
to
give
some
flexibility
to
Member
States
in
the
designation
of
their
representatives
in
the
Board
,
such
representatives
may
be
any
persons
belonging
to
public
entities
who
should
have
the
relevant
compet
ences
and
powers
to
facilitat
e
coordination
at
national
level
and
contribut
e
to
the
achi
evement
of
the
Board
’s
tasks
.
The
Board
should
establish
two
standing
sub
-
groups
to
provide
a
platform
for
cooperation
and
exchang
e
among
market
surveillance
authorities
and
notifying
authorities
on
issues
relate
d
,
respectively
,
to
market
surveillance
and
notified
bodies
.
The
standing
subgroup
for
market
surveillance
should
act
as
the
administrative
cooperation
group
(
ADCO
)
for
this
Regulation
within
the
meaning
of
Article
30
of
Regulation
(
EU
)
.
In
accordance
with
Article
33
of
that
Regulation
,
the
Commission
should
support
the
activities
of
the
standing
subgroup
for
market
surveillance
by
under
taking
market
evaluations
or
studies
,
in
particular
with
a
view
to
identifying
aspects
of
this
Regulation
requir
ing
specific
and
urgent
coordination
among
mark
et
surveillance
authorities
.
The
Board
may
establish
other
standing
or
temporar
y
sub
-
groups
as
appropriate
for
the
purpose
of
examining
specific
issues
.
The
Board
should
also
cooperate
,
as
appropriate
,
with
relevant
Union
bodies
,
exper
ts
groups
and
networks
active
in
the
context
of
relevant
Union
law
,
including
in
particular
those
active
under
relevant
Union
law
on
data
,
digital
products
and
services
.
(
150
)
With
a
view
to
ensur
ing
the
involvement
of
stakeholders
in
the
imp
lementation
and
application
of
this
Regulation
,
an
advisor
y
forum
should
be
established
to
advise
and
provide
technical
exper
tise
to
the
Board
and
the
Commission
.
To
ensure
a
varied
and
balanced
stak
eholder
representation
between
commercial
and
non
-
commercial
interest
and
,
within
the
category
of
commercial
interests
,
with
rega
rds
to
SMEs
and
other
undertakings
,
the
advisor
y
forum
should
compr
ise
inter
alia
industr
y
,
start
-
ups
,
SMEs
,
academia
,
civil
society
,
including
the
social
partners
,
as
well
as
the
Fundamental
Rights
Agency
,
ENISA
,
the
European
Committee
for
Standardization
(
CEN
)
,
the
European
Committ
ee
for
Electrotec
hnical
Standardization
(
CENELEC
)
and
the
European
Telecommunications
Standards
Institut
e
(
ETSI
)
.
(
151
)
To
support
the
imp
lementation
and
enforcement
of
this
Regulation
,
in
particular
the
monitoring
activities
of
the
AI
Office
as
regards
general
-
pur
pose
AI
models
,
a
scientific
panel
of
independent
exper
ts
should
be
established
.
The
independent
exper
ts
constituting
the
scientific
panel
should
be
selected
on
the
basis
of
up
-
to
-
da
te
scientific
or
technical
exper
tise
in
the
field
of
AI
and
should
perf
orm
their
tasks
with
imp
artiality
,
objectivity
and
ensure
the
confidentiality
of
information
and
data
obtained
in
carrying
out
their
tasks
and
activities
.
To
allow
the
reinf
orcement
of
national
capacities
necessary
for
the
effective
enforcement
of
this
Regulation
,
Member
States
should
be
able
to
request
support
from
the
pool
of
exper
ts
constituting
the
scientific
panel
for
their
enforcement
activities
.
(
152
)
In
order
to
support
adequate
enforcement
as
regard
s
AI
systems
and
reinf
orce
the
capacities
of
the
Member
States
,
Union
AI
testing
support
structures
should
be
established
and
made
available
to
the
Member
States
.
(
153
)
Member
States
hold
a
key
role
in
the
application
and
enforcement
of
this
Regulation
.
In
that
respect
,
each
Member
State
should
designate
at
least
one
notifying
author
ity
and
at
least
one
market
surveillance
author
ity
as
national
com
petent
authorities
for
the
purpose
of
super
vising
the
application
and
implementation
of
this
Regulation
.
Member
States
may
decide
to
appoint
any
kind
of
public
entity
to
perform
the
tasks
of
the
national
competent
authorities
within
the
meaning
of
this
Regulation
,
in
accordance
with
their
specific
national
organisational
characteristics
and
needs
.
In
order
to
increase
organisation
efficiency
on
the
side
of
Member
States
and
to
set
a
sing
le
point
of
contact
vis
-
à
-
vis
the
public
and
other
counte
rparts
at
Member
State
and
Union
levels
,
each
Member
State
should
designate
a
market
surveillance
author
ity
to
act
as
a
sing
le
point
of
contact
.
The
national
competent
authorities
should
exercise
their
powers
independently
,
imp
artially
and
without
bias
,
so
as
to
safeguard
the
principles
of
objectivity
of
their
activities
and
tasks
and
to
ensure
the
application
and
implementation
of
this
Regulation
.
The
members
of
these
authorities
should
refrain
from
any
action
incom
patible
with
their
duties
and
should
be
subject
to
conf
identiality
rules
under
this
Regulation
.
(
155
)
In
order
to
ensure
that
providers
of
higher
isk
AI
systems
can
take
into
account
the
exper
ience
on
the
use
of
higher
isk
AI
systems
for
impro
ving
their
systems
and
the
design
and
development
process
or
can
take
any
possible
corrective
action
in
a
timely
manner
,
all
providers
should
have
a
post
-
mark
et
monitoring
syste
m
in
place
.
Where
relevant
,
post
-
mark
et
monitoring
should
include
an
analysis
of
the
interaction
with
other
AI
systems
including
other
devices
and
software
.
Post
-
market
monitoring
should
not
cover
sensitive
operational
data
of
deplo
yers
which
are
law
enforcement
authorities
.
This
system
is
also
key
to
ensure
that
the
possible
risks
emerging
from
AI
systems
which
continue
to
‘
lear
n
’
after
being
placed
on
the
market
or
put
into
service
can
be
more
efficiently
and
timely
addressed
.
In
this
cont
ext
,
providers
should
also
be
required
to
have
a
syste
m
in
place
to
repor
t
to
the
relevant
authorities
any
serious
incidents
resulting
from
the
use
of
their
AI
systems
,
meaning
incident
or
malfunctioning
leading
to
death
or
serious
damage
to
health
,
serious
and
irreversible
disruption
of
the
managem
ent
and
operation
of
critical
infrastructure
,
infringements
of
obligations
under
Union
law
intended
to
prot
ect
fundamental
rights
or
serious
damage
to
proper
ty
or
the
environment
.
(
156
)
In
order
to
ensure
an
appropriate
and
effective
enforcement
of
the
requirements
and
obligations
set
out
by
this
Regulation
,
which
is
Union
harmonisation
legislation
,
the
system
of
market
surveillance
and
compliance
of
products
established
by
Regulation
(
EU
)
should
apply
in
its
entirety
.
market
surveillance
authorities
designated
pursuant
to
this
Regulation
should
have
all
enforcement
powers
laid
down
in
this
Regulation
and
in
Regulation
(
EU
)
and
should
exercise
their
powers
and
carry
out
their
duties
independently
,
impar
tially
and
without
bias
.
Although
the
majorit
y
of
AI
systems
are
not
subject
to
specific
requirements
and
obligations
under
this
Regulation
,
mark
et
surveillance
authorities
may
take
measures
in
relation
to
all
AI
systems
when
they
present
a
risk
in
accordance
with
this
Regulation
.
Due
to
the
specific
nature
of
Union
institutions
,
agencies
and
bodies
falling
within
the
scope
of
this
Regulation
,
it
is
appropriate
to
designate
the
European
Data
Protect
ion
Super
visor
as
a
competent
mark
et
surveillance
author
ity
for
them
.
This
should
be
without
prejudice
to
the
designation
of
national
competent
authorities
by
the
Member
States
.
Market
surveillance
activities
should
not
affect
the
ability
of
the
super
vised
entities
to
carry
out
their
tasks
independently
,
when
such
independence
is
required
by
Union
law
.
(
157
)
This
Regulation
is
without
prejudice
to
the
compet
ences
,
tasks
,
powers
and
independence
of
relevant
national
public
authorities
or
bodies
which
super
vise
the
application
of
Union
law
protect
ing
fundamental
rights
,
including
equality
bodies
and
data
protection
authorities
.
Where
necessary
for
their
mandat
e
,
those
national
public
authorities
or
bodies
should
also
have
access
to
any
documentation
create
d
under
this
Regulation
.
A
specific
safeguard
procedure
should
be
set
for
ensur
ing
adequat
e
and
timely
enforcement
against
AI
systems
presenting
a
risk
to
health
,
safety
and
fundamental
rights
.
The
procedure
for
such
AI
systems
presenting
a
risk
should
be
applied
to
higher
isk
AI
systems
presenting
a
risk
,
prohibited
systems
which
have
been
placed
on
the
market
,
put
into
service
or
used
in
violation
of
the
prohibited
practices
laid
down
in
this
Regulation
and
AI
systems
which
have
been
made
available
in
violation
of
the
transparency
requirements
laid
down
in
this
Regulation
and
present
a
risk
.
(
158
)
Union
financ
ial
services
law
includes
internal
govern
ance
and
risk
-
managem
ent
rules
and
requirements
which
are
applicable
to
regulate
d
financial
institutions
in
the
course
of
provision
of
those
services
,
including
when
they
make
use
of
AI
systems
.
In
order
to
ensure
coherent
application
and
enforcement
of
the
oblig
ations
under
this
Regulation
and
relevant
rules
and
requirements
of
the
Union
financial
services
legal
acts
,
the
competent
authorities
for
the
super
vision
and
enforcement
of
those
legal
acts
,
in
particular
competent
authorities
as
defined
in
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
and
Directives
/EC
,
/EC
,
(
46
)
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
of
26
June
2013
on
prudential
requirements
for
credit
institutions
and
investment
firms
and
amending
Regulation
(
EU
)
No
(
6.2013
,
p.
1
)
.
(
47
)
Directive
/EC
of
the
European
Parliament
and
of
the
Council
of
23
Apr
il
2008
on
credit
agreements
for
consumers
and
repealing
Council
Directive
/EEC
(
5.2008
,
p.
66
)
.
(
48
)
Directive
/EC
of
the
European
Parliament
and
of
the
Council
of
25
November
2009
on
the
taking
-
up
and
pursuit
of
the
business
of
Insurance
and
Reinsurance
(
Solvency
II
)
(
12.2009
,
p.
1)./EU
,
/EU
and
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
should
be
designate
d
,
within
their
respective
comp
etences
,
as
competent
authorities
for
the
purpose
of
super
vising
the
implementation
of
this
Regulation
,
including
for
market
surveillance
activities
,
as
regard
s
AI
systems
provid
ed
or
used
by
regulated
and
super
vised
financial
institutions
unless
Member
States
decide
to
designate
another
author
ity
to
fulfil
these
market
surveillance
tasks
.
Those
comp
etent
authorities
should
have
all
powers
under
this
Regulation
and
Regulation
(
EU
)
to
enforce
the
requirements
and
obligations
of
this
Regulation
,
including
powers
to
carry
our
ex
post
market
surveillance
activities
that
can
be
integrat
ed
,
as
appropriate
,
into
their
existing
supervisory
mec
hanisms
and
procedures
under
the
relevant
Union
financial
services
law
.
It
is
appropriate
to
envisag
e
that
,
when
acting
as
market
surveillance
authorities
under
this
Regulation
,
the
national
authorities
responsible
for
the
super
vision
of
credit
institutions
regulate
d
under
Directive
/EU
,
which
are
participating
in
the
Sing
le
supervisory
Mec
hanism
established
by
Council
Regulation
(
EU
)
No
,
should
repor
t
,
without
dela
y
,
to
the
European
Central
Bank
any
information
identified
in
the
course
of
their
mark
et
surveillance
activities
that
may
be
of
potent
ial
interest
for
the
European
Central
Bank
’s
prudential
supervisory
tasks
as
specified
in
that
Regulation
.
To
further
enhance
the
consistency
between
this
Regulation
and
the
rules
applicable
to
credit
institutions
regulated
under
Directive
/EU
,
it
is
also
appropriate
to
integrate
some
of
the
provid
ers
’
procedural
obligations
in
relation
to
risk
managem
ent
,
post
marketing
monitorin
g
and
documentation
into
the
existing
obligations
and
procedures
under
Directive
/EU
.
In
order
to
avoid
overlaps
,
limit
ed
deroga
tions
should
also
be
envisaged
in
relation
to
the
quality
management
system
of
provid
ers
and
the
monitori
ng
obligation
placed
on
deplo
yers
of
higher
isk
AI
systems
to
the
extent
that
these
apply
to
credit
institutions
regulated
by
Directive
/EU
.
The
same
regime
should
apply
to
insurance
and
re
-
insurance
undertakings
and
insurance
holding
companies
under
Directive
/EC
and
the
insurance
inter
mediar
ies
under
Directive
(
EU
)
and
other
types
of
financial
institutions
subject
to
requirements
regarding
inter
nal
gover
nance
,
arrang
ements
or
processes
established
pursuant
to
the
relevant
Union
financial
services
law
to
ensure
consistency
and
equal
treatment
in
the
financ
ial
sector
.
(
159
)
Each
market
surveillance
author
ity
for
higher
isk
AI
systems
in
the
area
of
biometrics
,
as
listed
in
an
annex
to
this
Regulation
insofa
r
as
those
systems
are
used
for
the
purposes
of
law
enforcement
,
migration
,
asylum
and
border
control
management
,
or
the
administration
of
justice
and
democratic
processes
,
should
have
effective
invest
igative
and
corrective
powers
,
including
at
least
the
power
to
obtain
access
to
all
personal
data
that
are
being
processed
and
to
all
information
necessary
for
the
perf
ormance
of
its
tasks
.
The
market
surveillance
authorities
should
be
able
to
exercise
their
powers
by
acting
with
complet
e
independence
.
Any
limitations
of
their
access
to
sensitive
operational
data
under
this
Regulation
should
be
without
prejudice
to
the
powers
confe
rred
to
them
by
Directive
(
EU
)
.
No
exclusion
on
disclosing
data
to
national
data
protection
authorities
under
this
Regulation
should
affect
the
current
or
future
powers
of
those
authorities
beyond
the
scope
of
this
Regulation
.
(
160
)
The
mark
et
surveillance
authorities
and
the
Commission
should
be
able
to
propose
joint
activities
,
including
joint
investig
ations
,
to
be
conducted
by
market
surveillance
authorities
or
market
surveillance
authorities
jointly
with
the
Commission
,
that
have
the
aim
of
promoting
compliance
,
identifying
non
-
compliance
,
raising
awareness
and
providing
guidance
in
relation
to
this
Regulation
with
respect
to
specific
categor
ies
of
higher
isk
AI
systems
that
are
found
to
present
a
serious
risk
across
two
or
more
Member
States
.
Joint
activities
to
promote
compliance
should
be
carried
out
in
accordance
with
Article
9
of
Regulation
(
EU
)
.
The
AI
Office
should
provid
e
coordination
support
for
joint
investig
ations
.
(
161
)
It
is
necessary
to
clarify
the
responsibilities
and
comp
etences
at
Union
and
national
level
as
regard
s
AI
systems
that
are
built
on
general
-
pur
pose
AI
models
.
To
avoid
overlapping
comp
etences
,
where
an
AI
syste
m
is
based
on
a
general
-
pur
pose
AI
model
and
the
model
and
system
are
provid
ed
by
the
same
provider
,
the
super
vision
should
Directive
/EU
of
the
European
Parliament
and
of
the
Council
of
26
June
2013
on
access
to
the
activity
of
credit
institutions
and
the
prudential
super
vision
of
credit
institutions
and
investment
firms
,
amending
Directive
/EC
and
repealing
Directives
/EC
and
/EC
(
6.2013
,
p.
338
)
.
(
50
)
Directive
/EU
of
the
European
Parliament
and
of
the
Council
of
4
Febr
uary
2014
on
credit
agreements
for
consumers
relating
to
residential
immov
able
proper
ty
and
amending
Directives
/EC
and
/EU
and
Regulation
(
EU
)
No
(
2.2014
,
p.
34
)
.
(
51
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
20
Januar
y
2016
on
insurance
distr
ibution
.
(
52
)
Council
Regulation
(
EU
)
No
of
15
October
2013
conf
erring
specific
tasks
on
the
European
Central
Bank
concerning
policies
relating
to
the
prudential
super
vision
of
credit
institutions
(
10.2013
,
p.
63).take
place
at
Union
level
through
the
AI
Office
,
which
should
have
the
powers
of
a
market
surveillance
author
ity
within
the
meaning
of
Regulation
(
EU
)
for
this
purpose
.
In
all
other
cases
,
national
market
surveillance
authorities
remain
responsible
for
the
super
vision
of
AI
systems
.
How
ever
,
for
general
-
pur
pose
AI
systems
that
can
be
used
directly
by
deplo
yers
for
at
least
one
purpose
that
is
classified
as
higher
isk
,
market
surveillance
authorities
should
cooperate
with
the
AI
Office
to
carry
out
evaluations
of
compliance
and
inform
the
Board
and
other
market
surveillance
authorities
according
ly
.
Further
more
,
market
surveillance
authorities
should
be
able
to
request
assistance
from
the
AI
Office
where
the
market
surveillance
author
ity
is
unable
to
conclude
an
invest
igation
on
a
higher
isk
AI
syste
m
because
of
its
inability
to
access
certain
information
related
to
the
general
-
pur
pose
AI
model
on
which
the
higher
isk
AI
syste
m
is
built
.
In
such
cases
,
the
procedure
regarding
mutual
assistance
in
cross
-
border
cases
in
Chapt
er
VI
of
Regulation
(
EU
)
should
apply
mutatis
mutandis
.
(
162
)
To
mak
e
best
use
of
the
centralised
Union
exper
tise
and
synergies
at
Union
level
,
the
powers
of
super
vision
and
enforcement
of
the
obligations
on
provid
ers
of
general
-
pur
pose
AI
models
should
be
a
comp
etence
of
the
Commission
.
The
AI
Office
should
be
able
to
carry
out
all
necessary
actions
to
monitor
the
effective
implementation
of
this
Regulation
as
rega
rds
general
-
pur
pose
AI
models
.
It
should
be
able
to
investigat
e
possible
infringements
of
the
rules
on
providers
of
general
-
pur
pose
AI
models
both
on
its
own
initiative
,
following
the
results
of
its
monitoring
activities
,
or
upon
request
from
market
surveillance
authorities
in
line
with
the
conditions
set
out
in
this
Regulation
.
To
support
effective
monitoring
of
the
AI
Offi
ce
,
it
should
provide
for
the
possibility
that
downstream
providers
lodg
e
complaints
about
possible
infringements
of
the
rules
on
provid
ers
of
general
-
pur
pose
AI
models
and
systems
.
(
163
)
With
a
view
to
comp
lementing
the
governance
systems
for
general
-
pur
pose
AI
models
,
the
scientific
panel
should
support
the
monitoring
activities
of
the
AI
Offi
ce
and
may
,
in
certain
cases
,
provide
qualified
alerts
to
the
AI
Office
which
trigger
follow
-
up
s
,
such
as
invest
igations
.
This
should
be
the
case
where
the
scientific
panel
has
reason
to
suspect
that
a
general
-
purpo
se
AI
model
poses
a
concrete
and
identifi
able
risk
at
Union
level
.
Further
more
,
this
should
be
the
case
where
the
scientific
panel
has
reason
to
suspect
that
a
general
-
pur
pose
AI
model
meets
the
criteria
that
would
lead
to
a
classif
ication
as
general
-
pur
pose
AI
model
with
systemic
risk
.
To
equip
the
scientific
panel
with
the
information
necessary
for
the
perf
ormance
of
those
tasks
,
there
should
be
a
mec
hanism
whereby
the
scientific
panel
can
request
the
Commission
to
require
documentation
or
information
from
a
provid
er
.
(
164
)
The
AI
Offi
ce
should
be
able
to
take
the
necessary
actions
to
monitor
the
effective
imp
lementation
of
and
compliance
with
the
obligations
for
provid
ers
of
general
-
pur
pose
AI
models
laid
down
in
this
Regulation
.
The
AI
Office
should
be
able
to
invest
igate
possible
infringements
in
accordance
with
the
powers
provided
for
in
this
Regulation
,
including
by
requesting
documentation
and
information
,
by
conducting
evaluations
,
as
well
as
by
requesting
measures
from
provid
ers
of
general
-
pur
pose
AI
models
.
When
conducting
evaluations
,
in
order
to
make
use
of
independent
exper
tise
,
the
AI
Office
should
be
able
to
involve
independent
exper
ts
to
carry
out
the
evaluations
on
its
behalf
.
Compliance
with
the
obligations
should
be
enforceable
,
inter
alia
,
through
requests
to
take
appropriate
measures
,
including
risk
mitiga
tion
measures
in
the
case
of
identifie
d
syste
mic
risks
as
well
as
restr
icting
the
making
availa
ble
on
the
market
,
withdra
wing
or
recalling
the
model
.
As
a
safeguard
,
where
needed
beyond
the
procedural
rights
provided
for
in
this
Regulation
,
provid
ers
of
general
-
pur
pose
AI
models
should
have
the
procedural
rights
provid
ed
for
in
Article
18
of
Regulation
(
EU
)
,
which
should
apply
mutatis
mutandis
,
without
prejudice
to
more
specific
procedural
rights
provid
ed
for
by
this
Regulation
.
(
165
)
The
development
of
AI
systems
other
than
higher
isk
AI
systems
in
accordance
with
the
requirements
of
this
Regulation
may
lead
to
a
larger
uptak
e
of
ethical
and
trustw
orthy
AI
in
the
Union
.
Providers
of
AI
systems
that
are
not
higher
isk
should
be
encouraged
to
create
codes
of
conduct
,
including
related
gover
nance
mechanisms
,
intended
to
foste
r
the
voluntar
y
application
of
some
or
all
of
the
mandator
y
requirements
applicable
to
higher
isk
AI
systems
,
adap
ted
in
light
of
the
intended
purpose
of
the
systems
and
the
lower
risk
involved
and
taking
into
account
the
available
technical
solutions
and
industr
y
best
practices
such
as
model
and
data
cards
.
Provi
ders
and
,
as
appropriate
,
deplo
yers
of
all
AI
systems
,
higher
isk
or
not
,
and
AI
models
should
also
be
encourage
d
to
apply
on
a
voluntar
y
basis
additional
requirements
relate
d
,
for
example
,
to
the
elements
of
the
Union
’s
Ethics
Guidelines
for
Trustw
orthy
AI
,
including
attention
to
vulnerable
persons
and
accessibility
to
persons
with
disability
,
stakeholders
’
participation
with
the
involvement
,
as
appropriate
,
of
relevant
stak
eholders
such
as
business
and
civil
society
organisations
,
academia
,
researc
h
organisations
,
trade
unions
and
consumer
protection
organisations
in
the
design
and
development
of
AI
systems
,
and
diversity
of
the
development
teams
,
including
gender
balance
.
To
ensure
that
the
voluntar
y
codes
of
conduct
are
effective
,
they
should
be
based
on
clear
objectives
and
key
perf
ormance
indicators
to
measure
the
achievement
of
those
objectives
.
They
should
also
be
developed
in
an
inclusive
way
,
as
appropriate
,
with
the
involvement
of
relevant
stakeholders
such
as
business
and
civil
society
organisations
,
academia
,
research
organisations
,
trade
unions
and
consumer
protection
organisation
.
The
Commission
may
develop
initiatives
,
including
of
a
sectoral
nature
,
to
facilitate
the
lowering
of
technical
barriers
hinder
ing
cross
-
border
exchang
e
of
data
for
AI
development
,
including
on
data
access
infrastructure
,
semantic
and
technical
interoperability
of
different
types
of
data
.
(
166
)
It
is
important
that
AI
systems
relate
d
to
products
that
are
not
higher
isk
in
accordance
with
this
Regulation
and
thus
are
not
required
to
comp
ly
with
the
requirements
set
out
for
higher
isk
AI
systems
are
never
theless
safe
when
placed
on
the
market
or
put
into
service
.
To
contribut
e
to
this
objective
,
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
would
apply
as
a
safety
net
.
(
167
)
In
order
to
ensure
trustful
and
constr
uctive
cooperation
of
comp
etent
authorities
on
Union
and
national
level
,
all
parties
involved
in
the
application
of
this
Regulation
should
respect
the
conf
identiality
of
information
and
data
obtained
in
carrying
out
their
tasks
,
in
accordance
with
Union
or
national
law
.
They
should
carry
out
their
tasks
and
activities
in
such
a
manner
as
to
prot
ect
,
in
particular
,
intellectual
proper
ty
rights
,
confi
dential
business
information
and
trade
secrets
,
the
effective
implementation
of
this
Regulation
,
public
and
national
security
interests
,
the
integr
ity
of
criminal
and
administrative
proceedings
,
and
the
integr
ity
of
classif
ied
information
.
(
168
)
compliance
with
this
Regulation
should
be
enforceable
by
means
of
the
imp
osition
of
penalties
and
other
enforcement
measures
.
Member
States
should
take
all
necessary
measures
to
ensure
that
the
provisions
of
this
Regulation
are
implement
ed
,
including
by
laying
down
effective
,
propor
tionate
and
dissuasive
penalties
for
their
infringement
,
and
to
respect
the
ne
bis
in
idem
principle
.
In
order
to
strengthen
and
harmonise
administrative
penalties
for
infringem
ent
of
this
Regulation
,
the
upper
limits
for
setting
the
administrative
fines
for
certain
specific
infringements
should
be
laid
down
.
When
assessing
the
amount
of
the
fines
,
Member
States
should
,
in
each
individual
case
,
take
into
account
all
relevant
circumstances
of
the
specific
situation
,
with
due
regard
in
particular
to
the
nature
,
gravity
and
duration
of
the
infringement
and
of
its
consequences
and
to
the
size
of
the
provid
er
,
in
particular
if
the
provider
is
an
SME
,
including
a
start
-
up
.
The
European
Data
Protection
Super
visor
should
have
the
power
to
impose
fines
on
Union
institutions
,
agencies
and
bodies
falling
within
the
scope
of
this
Regulation
.
(
169
)
compliance
with
the
obligations
on
providers
of
general
-
pur
pose
AI
models
imposed
under
this
Regulation
should
be
enforceable
,
inter
alia
,
by
means
of
fines
.
To
that
end
,
appropriate
levels
of
fines
should
also
be
laid
down
for
infringement
of
those
obligati
ons
,
including
the
failure
to
comply
with
measures
request
ed
by
the
Commission
in
accordance
with
this
Regulation
,
subject
to
appropriate
limitation
periods
in
accordance
with
the
principle
of
propor
tionality
.
All
decisions
taken
by
the
Commission
under
this
Regulation
are
subject
to
review
by
the
Cour
t
of
Justice
of
the
European
Union
in
accordance
with
the
TFEU
,
including
the
unlimit
ed
jurisdiction
of
the
Cour
t
of
Justice
with
rega
rd
to
penalties
pursuant
to
Article
261
TFEU
.
(
170
)
Union
and
national
law
already
provide
effective
remedies
to
natural
and
legal
persons
whose
rights
and
freedoms
are
adversely
affected
by
the
use
of
AI
systems
.
Without
prejudice
to
those
remedies
,
any
natural
or
lega
l
person
that
has
grounds
to
consider
that
there
has
been
an
infringem
ent
of
this
Regulation
should
be
entitled
to
lodg
e
a
complaint
to
the
relevant
mark
et
surveillance
author
ity
.
(
171
)
Affecte
d
persons
should
have
the
right
to
obtain
an
explanation
where
a
deplo
yer
’s
decision
is
based
mainly
upon
the
output
from
certain
higher
isk
AI
systems
that
fall
within
the
scope
of
this
Regulation
and
where
that
decision
produces
legal
effects
or
similarly
significantly
affects
those
persons
in
a
way
that
they
consider
to
have
an
adverse
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
10
May
2023
on
general
product
safety
,
amending
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
and
Directive
(
EU
)
of
the
European
Parliament
and
the
Council
,
and
repealing
Directive
/EC
of
the
European
Parliament
and
of
the
Council
and
Council
Directive
/EEC
(
5.2023
,
p.
1).impact
on
their
health
,
safety
or
fundamental
rights
.
That
explanation
should
be
clear
and
meaningful
and
should
provide
a
basis
on
which
the
affected
persons
are
able
to
exercise
their
rights
.
The
right
to
obtain
an
explanation
should
not
apply
to
the
use
of
AI
systems
for
which
exceptions
or
restrictions
follow
from
Union
or
national
law
and
should
apply
only
to
the
extent
this
right
is
not
already
provid
ed
for
under
Union
law
.
(
172
)
Persons
acting
as
whistleblo
wers
on
the
infringements
of
this
Regulation
should
be
prot
ected
under
the
Union
law
.
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
should
theref
ore
apply
to
the
repor
ting
of
infringements
of
this
Regulation
and
the
protect
ion
of
persons
repor
ting
such
infringements
.
(
173
)
In
order
to
ensure
that
the
regulato
ry
framework
can
be
adap
ted
where
necessary
,
the
power
to
adopt
acts
in
accordance
with
Article
290
TFEU
should
be
deleg
ated
to
the
Commission
to
amend
the
conditions
under
which
an
AI
system
is
not
to
be
considered
to
be
higher
isk
,
the
list
of
higher
isk
AI
systems
,
the
provisions
regarding
technical
documentation
,
the
cont
ent
of
the
EU
declaration
of
conf
ormity
the
provisions
regard
ing
the
conf
ormity
assessment
procedures
,
the
provisions
establishing
the
higher
isk
AI
systems
to
which
the
conf
ormity
assessment
procedure
based
on
assessment
of
the
quality
management
syste
m
and
assessment
of
the
technical
documentation
should
apply
,
the
threshold
,
bench
mark
s
and
indicators
,
including
by
supplementing
those
bench
mark
s
and
indicator
s
,
in
the
rules
for
the
classification
of
general
-
pur
pose
AI
models
with
syste
mic
risk
,
the
criteria
for
the
designation
of
general
-
pur
pose
AI
models
with
syste
mic
risk
,
the
technical
documentation
for
providers
of
general
-
pur
pose
AI
models
and
the
transparency
information
for
providers
of
general
-
pur
pose
AI
models
.
It
is
of
particular
imp
ortance
that
the
Commission
carry
out
appropriate
consultations
during
its
preparatory
work
,
including
at
exper
t
level
,
and
that
those
consultations
be
conducte
d
in
accordance
with
the
principles
laid
down
in
the
Inter
institutional
Agreement
of
13
Apr
il
2016
on
Better
Law
-Making
.
In
particular
,
to
ensure
equal
participation
in
the
preparation
of
delegat
ed
acts
,
the
European
Parliament
and
the
Council
receive
all
documents
at
the
same
time
as
Member
States
’
exper
ts
,
and
their
exper
ts
syste
matically
have
access
to
meetings
of
Commission
exper
t
groups
dealing
with
the
preparation
of
delegat
ed
acts
.
(
174
)
Given
the
rapid
technological
developments
and
the
technical
exper
tise
required
to
effectively
apply
this
Regulation
,
the
Commission
should
evaluate
and
review
this
Regulation
by
2
Augu
st
2029
and
ever
y
four
years
thereaf
ter
and
repor
t
to
the
European
Parliament
and
the
Council
.
In
addition
,
taking
into
account
the
implications
for
the
scope
of
this
Regulation
,
the
Commission
should
carry
out
an
assessment
of
the
need
to
amend
the
list
of
higher
isk
AI
systems
and
the
list
of
prohibited
practices
once
a
year
.
Moreove
r
,
by
2
August
2028
and
ever
y
four
years
thereaf
ter
,
the
Commission
should
evaluate
and
repor
t
to
the
European
Parliament
and
to
the
Council
on
the
need
to
amend
the
list
of
higher
isk
areas
headings
in
the
annex
to
this
Regulation
,
the
AI
systems
within
the
scope
of
the
transparency
obligations
,
the
effectiveness
of
the
super
vision
and
gover
nance
syste
m
and
the
progress
on
the
development
of
standardisation
deliverables
on
energy
efficient
development
of
general
-
pur
pose
AI
models
,
including
the
need
for
further
measures
or
actions
.
Finally
,
by
2
August
2028
and
ever
y
three
years
thereaf
ter
,
the
Commission
should
evaluate
the
impact
and
effectiveness
of
voluntar
y
codes
of
conduct
to
foste
r
the
application
of
the
requirements
provided
for
higher
isk
AI
systems
in
the
case
of
AI
systems
other
than
higher
isk
AI
systems
and
possibly
other
additional
requirements
for
such
AI
systems
.
(
175
)
In
order
to
ensure
uniform
conditions
for
the
imp
lementation
of
this
Regulation
,
impl
ementing
powers
should
be
conf
erred
on
the
Commission
.
Those
powers
should
be
exercised
in
accordance
with
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
.
(
176
)
Since
the
objective
of
this
Regulation
,
namely
to
imp
rove
the
functioning
of
the
internal
mark
et
and
to
promot
e
the
uptake
of
human
centr
ic
and
trustw
orthy
AI
,
while
ensur
ing
a
high
level
of
protection
of
health
,
safety
,
fundamental
rights
enshrined
in
the
Charter
,
including
democracy
,
the
rule
of
law
and
environmental
protection
against
harmful
effects
of
AI
systems
in
the
Union
and
supporting
innovation
,
can
not
be
sufficiently
achieved
by
the
Member
States
and
can
rather
,
by
reason
of
the
scale
or
effects
of
the
action
,
be
better
achieved
at
Union
level
,
the
Union
may
adop
t
(
54
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
23
October
2019
on
the
protection
of
persons
who
repor
t
breaches
of
Union
law
(
11.2019
,
p.
17
)
.
(
55
)
5.2016
,
p.
1
.
(
56
)
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
of
16
Febr
uary
2011
laying
down
the
rules
and
general
principles
concerning
mechanisms
for
control
by
Member
States
of
the
Commission
’s
exerci
se
of
implementing
powers
(
2.2011
,
p.
13).measures
in
accordance
with
the
principle
of
subsidiar
ity
as
set
out
in
Article
5
TEU
.
In
accordance
with
the
principle
of
propor
tionality
as
set
out
in
that
Article
,
this
Regulation
does
not
go
beyond
what
is
necessary
in
order
to
achieve
that
objective
.
(
177
)
In
order
to
ensure
lega
l
certainty
,
ensure
an
appropriate
adaptation
period
for
operators
and
avoid
disruption
to
the
mark
et
,
including
by
ensur
ing
continuity
of
the
use
of
AI
systems
,
it
is
appropriate
that
this
Regulation
applies
to
the
higher
isk
AI
systems
that
have
been
placed
on
the
market
or
put
into
service
before
the
general
date
of
application
thereof
,
only
if
,
from
that
date
,
those
systems
are
subject
to
signifi
ca
nt
chang
es
in
their
design
or
intende
d
purpose
.
It
is
appropriate
to
clarify
that
,
in
this
respect
,
the
concept
of
significant
chang
e
should
be
understood
as
equivalent
in
substance
to
the
notion
of
substantial
modif
ication
,
which
is
used
with
regard
only
to
higher
isk
AI
systems
pursuant
to
this
Regulation
.
On
an
exceptiona
l
basis
and
in
light
of
public
accountability
,
operators
of
AI
systems
which
are
comp
onents
of
the
large
-
scale
IT
systems
established
by
the
lega
l
acts
listed
in
an
annex
to
this
Regulation
and
operators
of
higher
isk
AI
systems
that
are
intended
to
be
used
by
public
authorities
should
,
respectively
,
take
the
necessary
steps
to
comp
ly
with
the
requirements
of
this
Regulation
by
end
of
2030
and
by
2
August
2030
.
(
178
)
Provi
ders
of
higher
isk
AI
systems
are
encourage
d
to
start
to
comply
,
on
a
voluntar
y
basis
,
with
the
relevant
oblig
ations
of
this
Regulation
already
during
the
transitional
period
.
(
179
)
This
Regulation
should
apply
from
2
August
2026
.
How
ever
,
taking
into
account
the
unaccepta
ble
risk
associated
with
the
use
of
AI
in
certain
ways
,
the
prohibitions
as
well
as
the
general
provis
ions
of
this
Regulation
should
already
apply
from
2
Febr
uary
2025
.
While
the
full
effect
of
those
prohibitions
follows
with
the
establishment
of
the
gove
rnance
and
enforcement
of
this
Regulation
,
anticipating
the
application
of
the
prohibitions
is
imp
ortant
to
take
account
of
unaccepta
ble
risks
and
to
have
an
effect
on
other
procedures
,
such
as
in
civil
law
.
Moreove
r
,
the
infrastructure
related
to
the
gove
rnance
and
the
conf
ormity
assessment
syste
m
should
be
operational
before
for
provid
ers
of
general
-
pur
pose
AI
models
should
apply
from
2
August
2025
.
Codes
of
practice
should
be
ready
by
classification
rules
and
procedures
are
up
to
date
in
light
of
technological
developments
.
In
addition
,
Member
States
should
lay
down
and
notify
to
the
Commission
the
rules
on
penalties
,
including
administrative
fines
,
and
ensure
that
they
are
properly
and
effectively
implemented
by
the
date
of
application
of
this
Regulation
.
Theref
ore
the
provis
ions
on
penalties
should
apply
from
2
August
2025
.
(
180
)
The
European
Data
Protection
Super
visor
and
the
European
Data
protection
Board
were
consult
ed
in
accordance
with
Article
42(1
)
and
of
Regulation
(
EU
)
and
delivered
their
joint
opinion
on
18
June
2021
,
HAVE
ADOPTED
THIS
REGULATION
:
CHAPTER
I
GENERAL
PROVISIONS
Article
1
Subject
matter
`
human
-
centr
ic
and
trustw
orthy
artificial
intellig
ence
(
AI
)
,
while
ensur
ing
a
high
level
of
protection
of
health
,
safety
,
fundamental
rights
enshrined
in
the
Charter
,
including
democracy
,
the
rule
of
law
and
environmental
protection
,
against
the
harmful
effects
of
AI
systems
in
the
Union
and
supporting
innovation
.
of
certain
AI
practices
;
(
c)specific
requirements
for
higher
isk
AI
systems
and
oblig
ations
for
operators
of
such
systems
;
(
d)harmonised
transparency
rules
for
certain
AI
systems
;
(
e)harmonised
rules
for
the
placing
on
the
market
of
general
-
pur
pose
AI
models
;
(
f)rules
on
mark
et
monitoring
,
market
surveillance
,
gove
rnance
and
enforcement
;
(
g)measures
to
support
innovation
,
with
a
particular
focus
on
SMEs
,
including
start
-
ups
.
Article
2
Scope
in
the
Union
,
irrespective
of
whether
those
providers
are
established
or
locat
ed
within
the
Union
or
in
a
third
countr
y
;
(
b)deplo
yers
of
AI
systems
that
have
their
place
of
establishment
or
are
located
within
the
Union
;
(
c)provid
ers
and
deplo
yers
of
AI
systems
that
have
their
place
of
establishment
or
are
locate
d
in
a
third
countr
y
,
where
the
output
produced
by
the
AI
system
is
used
in
the
Union
;
(
d)imp
orters
and
distr
ibut
ors
of
AI
systems
;
(
e)product
manufacturers
placing
on
the
market
or
putting
into
service
an
AI
system
together
with
their
product
and
under
their
own
name
or
trademark
;
(
f)authorised
representatives
of
provider
s
,
which
are
not
established
in
the
Union
;
(
g)affected
persons
that
are
located
in
the
Union
.
Union
harmonisation
legislation
listed
in
Section
B
of
Annex
I
,
only
Article
6(1
)
,
Articles
102
to
109
and
Article
112
apply
.
Article
57
applies
only
in
so
far
as
the
requirements
for
higher
isk
AI
systems
under
this
Regulation
have
been
integrat
ed
in
that
Union
harmonisation
legislation
.
compet
ences
of
the
Member
States
concerning
national
security
,
regardless
of
the
type
of
entity
entr
uste
d
by
the
Member
States
with
carrying
out
tasks
in
relation
to
those
comp
etences
.
This
Regulation
does
not
apply
to
AI
systems
where
and
in
so
far
they
are
placed
on
the
market
,
put
into
service
,
or
used
with
or
without
modif
ication
exclusively
for
military
,
defe
nce
or
national
security
purposes
,
rega
rdless
of
the
type
of
entity
carrying
out
those
activities
.
This
Regulation
does
not
apply
to
AI
systems
which
are
not
placed
on
the
market
or
put
into
service
in
the
Union
,
where
the
output
is
used
in
the
Union
exclusively
for
military
,
defe
nce
or
national
security
purposes
,
regardless
of
the
type
of
entity
carrying
out
those
activities
.
within
the
scope
of
this
Regulation
pursuant
to
paragraph
1
,
where
those
authorities
or
organisations
use
AI
systems
in
the
framework
of
international
cooperation
or
agreements
for
law
enforcement
and
judicial
cooperation
with
the
Union
or
with
one
or
more
Member
States
,
provided
that
such
a
third
countr
y
or
intern
ational
organisation
provides
adequate
safeguards
with
respect
to
the
protection
of
fundamental
rights
and
freedoms
of
individuals
.
as
set
out
in
Chapt
er
II
of
Regulation
(
EU
)
.
.
This
Regulation
does
not
apply
to
AI
systems
or
AI
models
,
including
their
output
,
specifically
developed
and
put
into
service
for
the
sole
purpose
of
scientifi
c
researc
h
and
development
.
data
processed
in
connection
with
the
rights
and
obligations
laid
down
in
this
Regulation
.
This
Regulation
shall
not
affect
Regulation
(
EU
)
or
(
EU
)
,
or
Directive
/EC
or
(
EU
)
,
without
prejudice
to
Article
prior
to
their
being
placed
on
the
market
or
put
into
service
.
Such
activities
shall
be
conducted
in
accordance
with
applicable
Union
law
.
Testing
in
real
world
conditions
shall
not
be
covered
by
that
exclusion
.
and
product
safety
.
a
purely
personal
non
-
professional
activity
.
administrative
provis
ions
which
are
more
favourable
to
workers
in
terms
of
protect
ing
their
rights
in
respect
of
the
use
of
AI
systems
by
employers
,
or
from
encouraging
or
allowi
ng
the
application
of
collective
agreements
which
are
more
favourable
to
workers
.
the
mark
et
or
put
into
service
as
higher
isk
AI
systems
or
as
an
AI
system
that
falls
under
Article
5
or
50
.
Article
3
Def
initions
For
the
purposes
of
this
Regulation
,
the
following
definitions
apply
:
‘
AI
syste
m
’
means
a
machine
-
based
system
that
is
designed
to
operat
e
with
varying
levels
of
autonom
y
and
that
may
exhibit
adap
tiveness
after
deplo
yment
,
and
that
,
for
explicit
or
implicit
objectives
,
infers
,
from
the
input
it
receives
,
how
to
generate
outputs
such
as
predictions
,
content
,
recommendations
,
or
decisions
that
can
influence
phys
ical
or
virtual
environments
;
‘
risk
’
means
the
combination
of
the
probability
of
an
occur
rence
of
harm
and
the
sever
ity
of
that
harm
;
‘
provi
der
’
means
a
natural
or
lega
l
person
,
public
author
ity
,
agency
or
other
body
that
develops
an
AI
system
or
a
general
-
pur
pose
AI
model
or
that
has
an
AI
syste
m
or
a
general
-
pur
pose
AI
model
developed
and
places
it
on
the
mark
et
or
puts
the
AI
syste
m
into
service
under
its
own
name
or
trademark
,
whether
for
payment
or
free
of
charg
e
;
‘
deplo
yer
’
means
a
natural
or
lega
l
person
,
public
author
ity
,
agency
or
other
body
using
an
AI
system
under
its
author
ity
except
where
the
AI
system
is
used
in
the
course
of
a
personal
non
-
professional
activity
;
‘
authorised
representative
’
means
a
natural
or
legal
person
located
or
established
in
the
Union
who
has
received
and
accep
ted
a
written
mandate
from
a
provider
of
an
AI
system
or
a
general
-
pur
pose
AI
model
to
,
respectively
,
perf
orm
and
carry
out
on
its
behalf
the
obligations
and
procedures
established
by
this
Regulation
;
‘
imp
orter
’
means
a
natural
or
lega
l
person
locate
d
or
established
in
the
Union
that
places
on
the
market
an
AI
system
that
bears
the
name
or
trademark
of
a
natural
or
lega
l
person
established
in
a
third
countr
y
;
‘
distr
ibutor
’
means
a
natural
or
legal
person
in
the
supply
chain
,
other
than
the
provider
or
the
importer
,
that
mak
es
an
AI
syste
m
available
on
the
Union
market
;
‘
operat
or
’
means
a
provid
er
,
product
manufa
cturer
,
deplo
yer
,
authorised
representative
,
imp
orter
or
distr
ibut
or
;
‘
placing
on
the
market
’
means
the
first
making
available
of
an
AI
system
or
a
general
-
purpo
se
AI
model
on
the
Union
mark
et
;
‘
making
available
on
the
market
’
means
the
supply
of
an
AI
syste
m
or
a
general
-
pur
pose
AI
model
for
distr
ibution
or
use
on
the
Union
market
in
the
course
of
a
commercial
activity
,
whether
in
retur
n
for
payment
or
free
of
charge
;
‘
putting
into
service
’
means
the
supply
of
an
AI
system
for
first
use
directly
to
the
deplo
yer
or
for
own
use
in
the
Union
for
its
intended
purpose
;
‘
intended
purpose
’
means
the
use
for
which
an
AI
syste
m
is
intended
by
the
provider
,
including
the
specific
cont
ext
and
conditions
of
use
,
as
specif
ied
in
the
information
supplied
by
the
provid
er
in
the
instr
uctions
for
use
,
promotional
or
sales
materials
and
statements
,
as
well
as
in
the
technical
documentation
;
‘
reasonably
foreseeable
misuse
’
means
the
use
of
an
AI
syste
m
in
a
way
that
is
not
in
accordance
with
its
intended
purpose
,
but
which
may
result
from
reasonably
foreseeable
human
behavi
our
or
interaction
with
other
systems
,
including
other
AI
systems
;
‘
safety
component
’
means
a
component
of
a
product
or
of
an
AI
system
which
fulfils
a
safety
function
for
that
product
or
AI
system
,
or
the
failure
or
malfunctioning
of
which
endang
ers
the
health
and
safety
of
persons
or
proper
ty
;
‘
instr
uctions
for
use
’
means
the
information
provided
by
the
provider
to
inform
the
deplo
yer
of
,
in
particular
,
an
AI
syste
m
’s
intended
purpose
and
proper
use
;
‘
recall
of
an
AI
syste
m
’
means
any
measure
aiming
to
achieve
the
retur
n
to
the
provider
or
taking
out
of
service
or
disabling
the
use
of
an
AI
system
made
available
to
deplo
yers
;
‘
withdra
wal
of
an
AI
system
’
means
any
measure
aiming
to
prevent
an
AI
system
in
the
supply
chain
being
made
available
on
the
mark
et
;
‘
perf
ormance
of
an
AI
syste
m
’
means
the
ability
of
an
AI
syste
m
to
achieve
its
intended
purpose
;
‘
notifying
author
ity
’
means
the
national
author
ity
responsible
for
setting
up
and
carrying
out
the
necessary
procedures
for
the
assessment
,
designation
and
notification
of
conf
ormity
assessment
bodies
and
for
their
monitoring
;
‘
conf
ormity
assessment
’
means
the
process
of
demonstrating
whether
the
requirements
set
out
in
Chapt
er
III
,
Section
2
relating
to
a
higher
isk
AI
system
have
been
fulfilled
;
‘
conf
ormity
assessment
body
’
means
a
body
that
perfo
rms
third
-
par
ty
conf
ormity
assessment
activities
,
including
testing
,
certification
and
inspection
;
‘
notified
body
’
means
a
conf
ormity
assessment
body
notif
ied
in
accordance
with
this
Regulation
and
other
relevant
Union
harmonisation
legislation
;
‘
substantial
modifi
cation
’
means
a
change
to
an
AI
syste
m
after
its
placing
on
the
market
or
putting
into
service
which
is
not
foreseen
or
planned
in
the
initial
conf
ormity
assessment
carried
out
by
the
provid
er
and
as
a
result
of
which
the
compliance
of
the
AI
system
with
the
requirements
set
out
in
Chap
ter
III
,
Section
2
is
affected
or
results
in
a
modif
ication
to
the
intended
purpose
for
which
the
AI
syste
m
has
been
assessed
;
‘
CE
marking
’
means
a
marking
by
which
a
provid
er
indicates
that
an
AI
system
is
in
conf
ormity
with
the
requirements
set
out
in
Chapt
er
III
,
Section
2
and
other
applicable
Union
harmonisation
legislation
providing
for
its
affixing
;
‘
post
-
mark
et
monitoring
syste
m
’
means
all
activities
carried
out
by
provid
ers
of
AI
systems
to
collect
and
review
exper
ience
gained
from
the
use
of
AI
systems
they
place
on
the
market
or
put
into
service
for
the
purpose
of
identifying
any
need
to
immediate
ly
apply
any
necessary
corrective
or
preventive
actions
;
‘
market
surveillance
author
ity
’
means
the
national
author
ity
carrying
out
the
activities
and
taking
the
measures
pursuant
to
Regulation
(
EU
)
;
(
27
)
‘
harmonised
standard
’
means
a
harmonised
standard
as
defined
in
Article
2(1
)
,
point
(
c
)
,
of
Regulation
(
EU
)
No
;
‘
common
specification
’
means
a
set
of
technical
specifications
as
defined
in
Article
2
,
point
of
Regulation
(
EU
)
No
,
providing
means
to
compl
y
with
certain
requirements
established
under
this
Regulation
;
‘
training
data
’
means
data
used
for
training
an
AI
system
through
fitting
its
learnable
paramet
ers
;
‘
validation
data
’
means
data
used
for
providing
an
evaluation
of
the
trained
AI
system
and
for
tuning
its
non
-
lear
nable
paramet
ers
and
its
learning
process
in
order
,
inter
alia
,
to
prevent
under
fitting
or
overfitting
;
‘
validation
data
set
’
means
a
separate
data
set
or
part
of
the
training
data
set
,
either
as
a
fixed
or
variable
split
;
‘
testing
data
’
means
data
used
for
providing
an
independent
evaluation
of
the
AI
system
in
order
to
conf
irm
the
expect
ed
perform
ance
of
that
syste
m
before
its
placing
on
the
market
or
putting
into
service
;
‘
input
data
’
means
data
provided
to
or
directly
acquired
by
an
AI
syste
m
on
the
basis
of
which
the
syste
m
produces
an
output
;
‘
biometric
data
’
means
personal
data
resulting
from
specific
technical
processing
relating
to
the
physical
,
physiolo
gical
or
behavi
oural
character
istics
of
a
natural
person
,
such
as
facial
images
or
dactyloscopic
data
;
‘
biometric
identifica
tion
’
means
the
automat
ed
recognition
of
physical
,
physiolo
gical
,
behavioural
,
or
psychological
human
features
for
the
purpose
of
establishing
the
identity
of
a
natural
person
by
comp
aring
biometric
data
of
that
individual
to
biometric
data
of
individuals
stored
in
a
database
;
‘
biometric
verificati
on
’
means
the
automa
ted
,
one
-
to
-
one
verification
,
including
authentication
,
of
the
identity
of
natural
persons
by
compari
ng
their
biometric
data
to
previously
provided
biometric
data
;
‘
special
categori
es
of
personal
data
’
means
the
categori
es
of
personal
data
refer
red
to
in
Article
9(1
)
of
Regulation
(
EU
)
,
Article
10
of
Directive
(
EU
)
and
Article
10(1
)
of
Regulation
(
EU
)
;
‘
sensitive
operational
data
’
means
operational
data
related
to
activities
of
prevention
,
detection
,
investig
ation
or
prosecution
of
criminal
offences
,
the
disclosure
of
which
could
jeopardise
the
integrity
of
criminal
proceedings
;
‘
emotion
recognition
syste
m
’
means
an
AI
syste
m
for
the
purpose
of
identifying
or
inferr
ing
emotions
or
intentions
of
natural
persons
on
the
basis
of
their
biometric
data
;
‘
biometric
categor
isation
syste
m
’
means
an
AI
system
for
the
purpose
of
assigning
natural
persons
to
specific
cate
gories
on
the
basis
of
their
biometric
data
,
unless
it
is
ancillar
y
to
another
commercial
service
and
strictly
necessary
for
objective
technical
reasons
;
‘
remot
e
biometric
identification
system
’
means
an
AI
system
for
the
purpose
of
identifying
natural
persons
,
without
their
active
involvement
,
typically
at
a
distance
through
the
comp
arison
of
a
person
’s
biometric
data
with
the
biometric
data
contained
in
a
refere
nce
database
;
‘
real
-
time
remote
biometric
identifica
tion
syste
m
’
means
a
remot
e
biometric
identification
system
,
whereby
the
capt
uring
of
biometric
data
,
the
comp
arison
and
the
identification
all
occur
without
a
signifi
ca
nt
dela
y
,
compr
ising
not
only
instant
identifica
tion
,
but
also
limited
shor
t
dela
ys
in
order
to
avoid
circum
vention
;
‘
post
-
remot
e
biometric
identifica
tion
syste
m
’
means
a
remot
e
biometric
identification
system
other
than
a
real
-
time
remot
e
biometric
identification
system
;
‘
publicly
accessible
space
’
means
any
publicly
or
privately
owned
physica
l
place
accessible
to
an
undet
ermined
number
of
natural
persons
,
regardless
of
whether
certain
conditions
for
access
may
apply
,
and
regard
less
of
the
pote
ntial
capacity
restrictions
;
‘
law
enforcement
author
ity
’
means
:
(
a)any
public
author
ity
comp
etent
for
the
prevention
,
invest
igation
,
detection
or
prosecution
of
criminal
offences
or
the
execution
of
criminal
penalties
,
including
the
safeguarding
against
and
the
prevention
of
threats
to
public
security
;
or
(
b)any
other
body
or
entity
entr
uste
d
by
Member
State
law
to
exercise
public
author
ity
and
public
powers
for
the
purposes
of
the
prevention
,
investig
ation
,
detect
ion
or
prosecution
of
criminal
offences
or
the
execution
of
criminal
penalties
,
including
the
safeguarding
against
and
the
prevention
of
threats
to
public
security
;
‘
law
enforcement
’
means
activities
carried
out
by
law
enforcement
authorities
or
on
their
behalf
for
the
prevention
,
investig
ation
,
detection
or
prosecution
of
criminal
offences
or
the
execution
of
criminal
penalties
,
including
safeguarding
against
and
preventing
threats
to
public
security
;
‘
AI
Office
’
means
the
Commission
’s
function
of
contributing
to
the
implementation
,
monitorin
g
and
super
vision
of
AI
systems
and
general
-
pur
pose
AI
models
,
and
AI
gover
nance
,
provid
ed
for
in
Commission
Decision
of
24
Januar
y
put
into
service
or
used
by
Union
institutions
,
agencies
,
offices
and
bodies
,
references
to
national
competent
authorities
or
market
surveillance
authorities
in
this
Regulation
shall
be
constr
ued
as
references
to
the
European
Data
protection
Super
visor
;
‘
serious
incident
’
means
an
incident
or
malfunctioning
of
an
AI
syste
m
that
directly
or
indirectly
leads
to
any
of
the
following
:
(
a)the
death
of
a
person
,
or
serious
harm
to
a
person
’s
health
;
(
b)a
serious
and
irreversible
disruption
of
the
managem
ent
or
operation
of
critical
infrastructure
;
(
c)the
infringement
of
obligations
under
Union
law
intended
to
protect
fundamental
rights
;
(
d)serious
harm
to
proper
ty
or
the
environment
;
‘
personal
data
’
means
personal
data
as
defined
in
Article
4
,
point
,
of
Regulation
(
EU
)
;
‘
non
-
personal
data
’
means
data
other
than
personal
data
as
defined
in
Article
4
,
point
,
of
Regulation
(
EU
)
;
‘
profiling
’
means
prof
iling
as
defined
in
Article
4
,
point
,
of
Regulation
(
EU
)
;
‘
real
-
w
orld
testing
plan
’
means
a
document
that
descr
ibes
the
objectives
,
methodology
,
geographical
,
population
and
temporal
scope
,
monitorin
g
,
organisation
and
conduct
of
testing
in
real
-
world
conditions
;
‘
sandbo
x
plan
’
means
a
document
agreed
between
the
participating
provid
er
and
the
comp
etent
author
ity
descr
ibing
the
objectives
,
conditions
,
timeframe
,
methodology
and
requirements
for
the
activities
carried
out
within
the
sandbo
x
;
‘
AI
regulato
ry
sandbo
x
’
means
a
controlled
framework
set
up
by
a
comp
etent
author
ity
which
offers
provid
ers
or
prospective
provider
s
of
AI
systems
the
possibility
to
develop
,
train
,
validate
and
test
,
where
appropriate
in
real
-
wo
rld
conditions
,
an
innovative
AI
system
,
pursuant
to
a
sandbo
x
plan
for
a
limited
time
under
regulator
y
super
vision
;
‘
AI
literacy
’
means
skills
,
knowledg
e
and
understanding
that
allow
provider
s
,
deplo
yers
and
affect
ed
persons
,
taking
into
account
their
respective
rights
and
obligations
in
the
context
of
this
Regulation
,
to
make
an
informed
deplo
yment
of
AI
systems
,
as
well
as
to
gain
awareness
about
the
oppor
tunities
and
risks
of
AI
and
possible
harm
it
can
cause
;
(
57
)
‘
testing
in
real
-
world
conditions
’
means
the
temporar
y
testing
of
an
AI
syste
m
for
its
intended
purpose
in
real
-
wo
rld
conditions
outside
a
laborato
ry
or
other
wise
simulated
environment
,
with
a
view
to
gather
ing
reliable
and
robust
data
and
to
assessing
and
verifying
the
conf
ormity
of
the
AI
system
with
the
requirements
of
this
Regulation
and
it
does
not
qualify
as
placing
the
AI
system
on
the
market
or
putting
it
into
service
within
the
meaning
of
this
Regulation
,
provided
that
all
the
conditions
laid
down
in
Article
57
or
60
are
fulfilled
;
‘
subject
’
,
for
the
purpose
of
real
-
world
testing
,
means
a
natural
person
who
participate
s
in
testing
in
real
-
wo
rld
conditions
;
‘
informed
consent
’
means
a
subject
’s
freely
given
,
specific
,
unambiguous
and
voluntar
y
expression
of
his
or
her
willingness
to
participate
in
a
particular
testing
in
real
-
world
conditions
,
after
having
been
informed
of
all
aspects
of
the
testing
that
are
relevant
to
the
subject
’s
decision
to
participate
;
‘
deep
fake
’
means
AI
-
g
enerated
or
manipulate
d
imag
e
,
audio
or
video
cont
ent
that
resembles
existing
persons
,
objects
,
places
,
entities
or
events
and
would
falsely
appear
to
a
person
to
be
authentic
or
truthful
;
‘
widespread
infringement
’
means
any
act
or
omission
contrar
y
to
Union
law
prot
ecting
the
interest
of
individuals
,
which
:
(
a)has
harmed
or
is
likely
to
harm
the
collective
interests
of
individuals
residing
in
at
least
two
Member
States
other
than
the
Member
State
in
which
:
(
i)the
act
or
omission
originated
or
took
place
;
(
ii)the
provider
concer
ned
,
or
,
where
applicable
,
its
authorised
representative
is
located
or
established
;
or
(
iii
)
the
deplo
yer
is
established
,
when
the
infringement
is
committed
by
the
deplo
yer
;
(
b)has
caused
,
causes
or
is
likel
y
to
cause
harm
to
the
collective
intere
sts
of
individuals
and
has
common
features
,
including
the
same
unlawful
practice
or
the
same
interest
being
infringed
,
and
is
occur
ring
concur
rently
,
committ
ed
by
the
same
operat
or
,
in
at
least
three
Member
States
;
‘
critical
infrastructure
’
means
critical
infrastructure
as
defined
in
Article
2
,
point
,
of
Directive
(
EU
)
;
‘
general
-
pur
pose
AI
model
’
means
an
AI
model
,
including
where
such
an
AI
model
is
trained
with
a
large
amount
of
data
using
self
-
super
vision
at
scale
,
that
displa
ys
significant
generality
and
is
capable
of
competently
perfo
rming
a
wide
rang
e
of
distinct
tasks
regardless
of
the
way
the
model
is
placed
on
the
market
and
that
can
be
integrat
ed
into
a
variety
of
downstream
systems
or
applications
,
except
AI
models
that
are
used
for
research
,
development
or
prot
otyping
activities
before
they
are
placed
on
the
mark
et
;
‘
high
-
imp
act
capabilities
’
means
capabilities
that
matc
h
or
exceed
the
capabilities
recorded
in
the
most
advanced
general
-
pur
pose
AI
models
;
‘
syst
emic
risk
’
means
a
risk
that
is
specific
to
the
high
-
im
pact
capabilities
of
general
-
pur
pose
AI
models
,
having
a
significant
impact
on
the
Union
market
due
to
their
reac
h
,
or
due
to
actual
or
reasonably
foreseeable
negative
effects
on
public
health
,
safety
,
public
security
,
fundamental
rights
,
or
the
society
as
a
whole
,
that
can
be
propagat
ed
at
scale
across
the
value
chain
;
‘
general
-
pur
pose
AI
syste
m
’
means
an
AI
syste
m
which
is
based
on
a
general
-
pur
pose
AI
model
and
which
has
the
capability
to
serve
a
variety
of
purposes
,
both
for
direct
use
as
well
as
for
integrat
ion
in
other
AI
systems
;
‘
floating
-
point
operation
’
means
any
mathematical
operation
or
assignment
involving
floating
-
point
numbers
,
which
are
a
subset
of
the
real
numbers
typically
represente
d
on
comput
ers
by
an
integ
er
of
fixed
precision
scaled
by
an
integer
exponent
of
a
fixed
base
;
‘
downstream
provider
’
means
a
provid
er
of
an
AI
system
,
including
a
general
-
pur
pose
AI
system
,
which
integrat
es
an
AI
model
,
rega
rdless
of
whether
the
AI
model
is
provided
by
themselves
and
vertically
integrat
ed
or
provid
ed
by
another
entity
based
on
contractual
relations
.
AI
literacy
Provi
ders
and
deplo
yers
of
AI
systems
shall
take
measures
to
ensure
,
to
their
best
extent
,
a
suffi
cient
level
of
AI
literacy
of
their
staff
and
other
persons
dealing
with
the
operation
and
use
of
AI
systems
on
their
behalf
,
taking
into
account
their
technical
kno
wledge
,
exper
ience
,
education
and
training
and
the
context
the
AI
systems
are
to
be
used
in
,
and
consider
ing
the
persons
or
groups
of
persons
on
whom
the
AI
systems
are
to
be
used
.
CHAPTER
II
PROHIBITED
AI
PRA
CTICES
Article
5
Prohibited
AI
practices
a
person
’s
consciousness
or
purposefully
manipulative
or
decep
tive
techniques
,
with
the
objective
,
or
the
effect
of
materially
distor
ting
the
behaviour
of
a
person
or
a
group
of
persons
by
appreciably
imp
airing
their
ability
to
make
an
informed
decision
,
thereby
causing
them
to
take
a
decision
that
they
would
not
have
other
wise
take
n
in
a
manner
that
causes
or
is
reasonably
likely
to
cause
that
person
,
another
person
or
group
of
persons
significant
harm
;
(
b)the
placing
on
the
market
,
the
putting
into
service
or
the
use
of
an
AI
system
that
exploits
any
of
the
vulnerabilities
of
a
natural
person
or
a
specific
group
of
persons
due
to
their
age
,
disability
or
a
specific
social
or
economic
situation
,
with
the
objective
,
or
the
effect
,
of
materi
ally
distor
ting
the
behavio
ur
of
that
person
or
a
person
belonging
to
that
group
in
a
manner
that
causes
or
is
reasonably
likely
to
cause
that
person
or
another
person
significant
harm
;
(
c)the
placing
on
the
market
,
the
putting
into
service
or
the
use
of
AI
systems
for
the
evaluation
or
classif
ication
of
natural
persons
or
groups
of
persons
over
a
certain
period
of
time
based
on
their
social
behavio
ur
or
known
,
inferr
ed
or
predicted
personal
or
personality
characteristics
,
with
the
social
score
leading
to
either
or
both
of
the
follo
wing
:
(
i)detr
imental
or
unfa
vourable
treatment
of
certain
natural
persons
or
groups
of
persons
in
social
contexts
that
are
unrelat
ed
to
the
cont
exts
in
which
the
data
was
originally
generat
ed
or
collect
ed
;
(
ii)detr
imental
or
unfa
vourable
treatment
of
certain
natural
persons
or
groups
of
persons
that
is
unjustifie
d
or
dispropor
tionate
to
their
social
behavio
ur
or
its
gravity
;
(
d)the
placing
on
the
market
,
the
putting
into
service
for
this
specific
purpose
,
or
the
use
of
an
AI
system
for
making
risk
assessments
of
natural
persons
in
order
to
assess
or
predict
the
risk
of
a
natural
person
committing
a
criminal
offence
,
based
solely
on
the
profiling
of
a
natural
person
or
on
assessing
their
personality
traits
and
character
istics
;
this
prohibition
shall
not
apply
to
AI
systems
used
to
support
the
human
assessment
of
the
involvement
of
a
person
in
a
criminal
activity
,
which
is
already
based
on
objective
and
verifiable
facts
directly
linke
d
to
a
criminal
activity
;
(
e)the
placing
on
the
mark
et
,
the
putting
into
service
for
this
specific
purpose
,
or
the
use
of
AI
systems
that
create
or
expand
facial
recognition
databases
through
the
untarget
ed
scraping
of
facial
images
from
the
inter
net
or
CCT
V
footage
;
(
f)the
placing
on
the
market
,
the
putting
into
service
for
this
specific
purpose
,
or
the
use
of
AI
systems
to
infer
emotions
of
a
natural
person
in
the
areas
of
workplace
and
education
institutions
,
except
where
the
use
of
the
AI
system
is
intende
d
to
be
put
in
place
or
into
the
mark
et
for
medical
or
safety
reasons
;
(
g)the
placing
on
the
market
,
the
putting
into
service
for
this
specific
purpose
,
or
the
use
of
biometric
categor
isation
systems
that
catego
rise
individually
natural
persons
based
on
their
biometric
data
to
deduce
or
infer
their
race
,
political
opinions
,
trade
union
membership
,
religious
or
philosophical
beliefs
,
sex
life
or
sexual
orientation
;
this
prohibition
does
not
cover
any
labelling
or
filter
ing
of
lawfully
acquired
biometric
datasets
,
such
as
imag
es
,
based
on
biometric
data
or
categori
zing
of
biometric
data
in
the
area
of
law
enforcement
;
(
h)the
use
of
‘
real
-
time
’
remote
biometric
identification
systems
in
publicly
accessible
spaces
for
the
purposes
of
law
enforcement
,
unless
and
in
so
far
as
such
use
is
strictly
necessary
for
one
of
the
follo
wing
objectives
:
(
i)the
targe
ted
search
for
specific
victims
of
abduction
,
trafficking
in
human
beings
or
sexual
exploitation
of
human
beings
,
as
well
as
the
search
for
missing
persons
;
(
ii)the
prevention
of
a
specific
,
substantial
and
imminent
threat
to
the
life
or
physica
l
safety
of
natural
persons
or
a
genuine
and
present
or
genuine
and
foreseeable
threat
of
a
terrorist
attack
;
(
iii
)
the
localisation
or
identification
of
a
person
suspect
ed
of
having
committed
a
criminal
offence
,
for
the
purpose
of
conducting
a
criminal
invest
igation
or
prosecution
or
executing
a
criminal
penalty
for
offences
referred
to
in
Annex
II
and
punishable
in
the
Member
State
concer
ned
by
a
custodial
sente
nce
or
a
detention
order
for
a
maximum
period
of
at
least
four
years
.
Point
(
h
)
of
the
first
subparagraph
is
without
prejudice
to
Article
9
of
Regulation
(
EU
)
for
the
processing
of
biometric
data
for
purposes
other
than
law
enforcement
.
enforcement
for
any
of
the
objectives
refer
red
to
in
paragraph
1
,
first
subparagraph
,
point
(
h
)
,
shall
be
deployed
for
the
purposes
set
out
in
that
point
only
to
confir
m
the
identity
of
the
specifically
targe
ted
individual
,
and
it
shall
take
into
account
the
follo
wing
elements
:
(
a)the
nature
of
the
situation
giving
rise
to
the
possible
use
,
in
particular
the
seriousness
,
probability
and
scale
of
the
harm
that
would
be
caused
if
the
system
were
not
used
;
(
b)the
consequences
of
the
use
of
the
system
for
the
rights
and
freedoms
of
all
persons
concer
ned
,
in
particular
the
seriousness
,
probability
and
scale
of
those
consequences
.
In
addition
,
the
use
of
‘
real
-
time
’
remot
e
biometric
identification
systems
in
publicly
accessible
spaces
for
the
purposes
of
law
enforcement
for
any
of
the
objectives
referred
to
in
paragraph
1
,
first
subparagraph
,
point
(
h
)
,
of
this
Article
shall
comply
with
necessary
and
propor
tionate
safegua
rds
and
conditions
in
relation
to
the
use
in
accordance
with
the
national
law
author
ising
the
use
thereof
,
in
particular
as
regard
s
the
temporal
,
geographic
and
personal
limitations
.
The
use
of
the
‘
real
-
time
’
remot
e
biometric
identifica
tion
syste
m
in
publicly
accessible
spaces
shall
be
authorised
only
if
the
law
enforcement
author
ity
has
complet
ed
a
fundamental
rights
impact
assessment
as
provid
ed
for
in
Article
27
and
has
register
ed
the
system
in
the
EU
database
according
to
Article
49
.
However
,
in
duly
justified
cases
of
urgency
,
the
use
of
such
systems
may
be
commenced
without
the
registration
in
the
EU
database
,
provid
ed
that
such
registration
is
complet
ed
without
undue
dela
y.
enforcement
of
a
‘
real
-
time
’
remot
e
biometric
identifica
tion
syste
m
in
publicly
accessible
spaces
shall
be
subject
to
a
prior
author
isation
granted
by
a
judicial
author
ity
or
an
independent
administrative
author
ity
whose
decision
is
binding
of
the
Member
State
in
which
the
use
is
to
take
place
,
issued
upon
a
reasoned
request
and
in
accordance
with
the
detailed
rules
of
national
law
referred
to
in
paragraph
5
.
How
ever
,
in
a
duly
justified
situation
of
urgency
,
the
use
of
such
system
may
be
commenced
without
an
author
isation
provided
that
such
author
isation
is
request
ed
without
undue
dela
y
,
at
the
latest
within
24
hours
.
If
such
author
isation
is
rejected
,
the
use
shall
be
stopped
with
immediate
effect
and
all
the
data
,
as
well
as
the
results
and
outputs
of
that
use
shall
be
immediately
discarded
and
deleted
.
The
comp
etent
judicial
author
ity
or
an
independent
administrative
author
ity
whose
decision
is
binding
shall
grant
the
author
isation
only
where
it
is
satisfied
,
on
the
basis
of
objective
evidence
or
clear
indications
presented
to
it
,
that
the
use
of
the
‘
real
-
time
’
remote
biometric
identifi
cation
system
concer
ned
is
necessary
for
,
and
propor
tionate
to
,
achieving
one
of
the
specified
in
paragraph
1
,
first
subparagraph
,
point
(
h
)
,
as
identified
in
the
request
and
,
in
particular
,
remains
limited
to
what
is
strictly
necessary
concerning
the
period
of
time
as
well
as
the
geographic
and
personal
scope
.
In
deciding
on
the
request
,
that
author
ity
shall
take
into
account
the
elements
referred
to
in
paragraph
2
.
No
decision
that
produces
an
adverse
legal
effect
on
a
person
may
be
taken
based
solely
on
the
output
of
the
‘
real
-
time
’
remote
biometric
identification
syste
m.
accessible
spaces
for
law
enforcement
purposes
shall
be
notif
ied
to
the
relevant
market
surveillance
author
ity
and
the
national
data
protection
author
ity
in
accordance
with
the
national
rules
refer
red
to
in
paragraph
5
.
The
notif
ication
shall
,
as
a
minimum
,
contain
the
information
specified
under
paragraph
6
and
shall
not
include
sensitive
operational
data
.
biometric
identifica
tion
systems
in
publicly
accessible
spaces
for
the
purposes
of
law
enforcement
within
the
limits
and
under
the
conditions
listed
in
paragraph
1
,
first
subparagraph
,
point
(
h
)
,
and
paragraphs
2
and
3
.
Member
States
concer
ned
shall
lay
down
in
their
national
law
the
necessary
detailed
rules
for
the
request
,
issuance
and
exercise
of
,
as
well
as
super
vision
and
repor
ting
relating
to
,
the
author
isations
referred
to
in
paragraph
3
.
Those
rules
shall
also
specify
in
respect
of
which
of
the
objectives
listed
in
paragraph
1
,
first
subparagraph
,
point
(
h
)
,
including
which
of
the
criminal
offences
referred
to
in
point
(
h)(iii
)
thereof
,
the
competent
authorities
may
be
authorised
to
use
those
systems
for
the
purposes
of
law
enforcement
.
Member
States
shall
notify
those
rules
to
the
Commission
at
the
latest
30
days
follo
wing
the
adoption
thereof
.
Member
States
may
introduce
,
in
accordance
with
Union
law
,
more
restr
ictive
laws
on
the
use
of
remote
biometric
identifica
tion
systems
.
notif
ied
of
the
use
of
‘
real
-
time
’
remote
biometric
identifi
cation
systems
in
publicly
accessible
spaces
for
law
enforcement
purposes
pursuant
to
paragraph
4
shall
submit
to
the
Commission
annual
repor
ts
on
such
use
.
For
that
purpose
,
the
Commission
shall
provide
Member
States
and
national
market
surveillance
and
data
protection
authorities
with
a
template
,
including
information
on
the
number
of
the
decisions
taken
by
competent
judicial
authorities
or
an
independent
administrative
author
ity
whose
decision
is
binding
upon
requests
for
author
isations
in
accordance
with
paragraph
3
and
their
result
.
publicly
accessible
spaces
for
law
enforcement
purposes
,
based
on
aggregat
ed
data
in
Member
States
on
the
basis
of
the
annual
repor
ts
refer
red
to
in
paragraph
6
.
Those
annual
repor
ts
shall
not
include
sensitive
operational
data
of
the
related
law
enforcement
activities
.
CHAPTER
III
higherISK
AI
SYSTEMS
SECTION
1
Classification
of
AI
systems
as
higher
isk
Article
6
Classif
ication
rules
for
higher
isk
AI
systems
referred
to
in
points
(
a
)
and
(
b
)
,
that
AI
system
shall
be
considered
to
be
higher
isk
where
both
of
the
follo
wing
conditions
are
fulfilled
:
(
a)the
AI
syste
m
is
intended
to
be
used
as
a
safety
compo
nent
of
a
product
,
or
the
AI
syste
m
is
itself
a
product
,
cover
ed
by
the
Union
harmonisation
legislation
listed
in
Annex
I
;
(
b)the
product
whose
safety
compo
nent
pursuant
to
point
(
a
)
is
the
AI
syste
m
,
or
the
AI
system
itself
as
a
product
,
is
required
to
undergo
a
third
-
par
ty
conf
ormity
assessment
,
with
a
view
to
the
placing
on
the
market
or
the
putting
into
service
of
that
product
pursuant
to
the
Union
harmonisation
legislation
listed
in
Annex
I.
considered
to
be
higher
isk
.
.
By
derogat
ion
from
paragraph
2
,
an
AI
system
referred
to
in
Annex
III
shall
not
be
considered
to
be
higher
isk
where
it
does
not
pose
a
significant
risk
of
harm
to
the
health
,
safety
or
fundamental
rights
of
natural
persons
,
including
by
not
materially
influencing
the
outcome
of
decision
making
.
The
first
subparagraph
shall
apply
where
any
of
the
follo
wing
conditions
is
fulfilled
:
(
a)the
AI
syste
m
is
intende
d
to
perform
a
narrow
procedural
task
;
(
b)the
AI
syste
m
is
intende
d
to
impro
ve
the
result
of
a
previously
comp
leted
human
activity
;
(
c)the
AI
system
is
intende
d
to
detect
decision
-
making
patt
erns
or
deviations
from
prior
decision
-
making
patterns
and
is
not
meant
to
replace
or
influence
the
previously
comp
leted
human
assessment
,
without
proper
human
review
;
or
(
d)the
AI
system
is
intended
to
perfor
m
a
preparat
ory
task
to
an
assessment
relevant
for
the
purposes
of
the
use
cases
listed
in
Annex
III
.
Notwithstanding
the
first
subparagraph
,
an
AI
system
refer
red
to
in
Annex
III
shall
alwa
ys
be
considered
to
be
higher
isk
where
the
AI
system
perfor
ms
profiling
of
natural
persons
.
before
that
system
is
placed
on
the
market
or
put
into
service
.
Such
provid
er
shall
be
subject
to
the
registration
obligation
set
out
in
Article
49(2
)
.
Upon
request
of
national
comp
etent
authorities
,
the
provider
shall
provide
the
documentation
of
the
assessment
.
with
a
comp
rehensive
list
of
practical
examples
of
use
cases
of
AI
systems
that
are
higher
isk
and
not
higher
isk
.
second
subparagraph
,
of
this
Article
by
adding
new
conditions
to
those
laid
down
therein
,
or
by
modifying
them
,
where
there
is
concrete
and
reliable
evidence
of
the
existe
nce
of
AI
systems
that
fall
under
the
scope
of
Annex
III
,
but
do
not
pose
a
significant
risk
of
harm
to
the
health
,
safety
or
fundamental
rights
of
natural
persons
.
subparagraph
,
of
this
Article
by
deleting
any
of
the
conditions
laid
down
therein
,
where
there
is
concrete
and
reliable
evidence
that
this
is
necessary
to
maintain
the
level
of
protection
of
health
,
safety
and
fundamental
rights
provided
for
by
this
Regulation
.
paragraphs
6
and
7
of
this
Article
shall
not
decrease
the
overall
level
of
protect
ion
of
health
,
safety
and
fundamental
rights
provid
ed
for
by
this
Regulation
and
shall
ensure
consiste
ncy
with
the
deleg
ated
acts
adop
ted
pursuant
to
Article
7(1
)
,
and
take
account
of
market
and
technological
developments
.
Article
7
Amendments
to
Annex
III
or
modifying
use
-
cases
of
higher
isk
AI
systems
where
both
of
the
following
conditions
are
fulfilled
:
(
a)the
AI
systems
are
intende
d
to
be
used
in
any
of
the
areas
listed
in
Annex
III
;
(
b)the
AI
systems
pose
a
risk
of
harm
to
health
and
safety
,
or
an
adverse
impact
on
fundamental
rights
,
and
that
risk
is
equivalent
to
,
or
great
er
than
,
the
risk
of
harm
or
of
adverse
imp
act
posed
by
the
higher
isk
AI
systems
already
referred
to
in
Annex
III
.
When
assessing
the
condition
under
paragraph
1
,
point
(
b
)
,
the
Commission
shall
take
into
account
the
following
criteria
:
(
a)the
intended
purpose
of
the
AI
system
;
(
b)the
extent
to
which
an
AI
syste
m
has
been
used
or
is
likely
to
be
used
;
(
c)the
nature
and
amount
of
the
data
processed
and
used
by
the
AI
system
,
in
particular
whether
special
categori
es
of
personal
data
are
processed
;
(
d)the
extent
to
which
the
AI
system
acts
autonomously
and
the
possibility
for
a
human
to
override
a
decision
or
recommendations
that
may
lead
to
pote
ntial
harm
;
(
e)the
extent
to
which
the
use
of
an
AI
syste
m
has
already
caused
harm
to
health
and
safety
,
has
had
an
adverse
impact
on
fundamental
rights
or
has
given
rise
to
significant
concer
ns
in
relation
to
the
likelihood
of
such
harm
or
adverse
impact
,
as
demonstrated
,
for
example
,
by
repor
ts
or
documente
d
alleg
ations
submitted
to
national
competent
authorities
or
by
other
repor
ts
,
as
appropriate
;
(
f)the
pote
ntial
extent
of
such
harm
or
such
adverse
impact
,
in
particular
in
terms
of
its
intens
ity
and
its
ability
to
affect
multiple
persons
or
to
dispropor
tionately
affect
a
particular
group
of
persons
;
(
g)the
extent
to
which
persons
who
are
potent
ially
harmed
or
suffer
an
adverse
impact
are
dependent
on
the
outcome
produced
with
an
AI
system
,
in
particular
because
for
practical
or
lega
l
reasons
it
is
not
reasonably
possible
to
opt
-
ou
t
from
that
outcome
;
(
h)the
extent
to
which
there
is
an
imbalance
of
power
,
or
the
persons
who
are
potentially
harmed
or
suffer
an
adverse
imp
act
are
in
a
vulnerable
position
in
relation
to
the
deplo
yer
of
an
AI
syste
m
,
in
particular
due
to
status
,
author
ity
,
kno
wledge
,
economic
or
social
circumstances
,
or
age
;
(
i)the
extent
to
which
the
outcome
produced
involving
an
AI
system
is
easily
corrigible
or
reversible
,
taking
into
account
the
technical
solutions
available
to
correct
or
reverse
it
,
whereby
outcomes
having
an
adverse
imp
act
on
health
,
safety
or
fundamental
rights
,
shall
not
be
considered
to
be
easily
corrigible
or
reversible
;
(
j)the
magnitude
and
likelihood
of
benefit
of
the
deplo
yment
of
the
AI
system
for
individuals
,
groups
,
or
society
at
large
,
including
possible
improvements
in
product
safety
;
(
k)the
extent
to
which
existing
Union
law
provid
es
for
:
(
i)effective
measures
of
redress
in
relation
to
the
risks
posed
by
an
AI
syste
m
,
with
the
exclusion
of
claims
for
damages
;
(
ii)effective
measures
to
prevent
or
substantially
minimise
those
risks
.
by
removi
ng
higher
isk
AI
systems
where
both
of
the
following
conditions
are
fulfilled
:
(
a)the
higher
isk
AI
syste
m
concer
ned
no
longer
poses
any
signifi
ca
nt
risks
to
fundamental
rights
,
health
or
safety
,
taking
into
account
the
criteria
listed
in
paragraph
2
;
(
b)the
deletion
does
not
decrease
the
overall
level
of
protection
of
health
,
safety
and
fundamental
rights
under
Union
law
.
SECTION
2
Requir
ements
for
higher
isk
AI
systems
Article
8
Compliance
with
the
requirements
purpose
as
well
as
the
generally
ackno
wledged
state
of
the
art
on
AI
and
AI
-
related
technologies
.
The
risk
management
syste
m
referred
to
in
Article
9
shall
be
taken
into
account
when
ensur
ing
compliance
with
those
requirements
.
.
Where
a
product
contains
an
AI
syste
m
,
to
which
the
requirements
of
this
Regulation
as
well
as
requirements
of
the
Union
harmonisation
legislation
listed
in
Section
A
of
Annex
I
apply
,
provid
ers
shall
be
responsible
for
ensur
ing
that
their
product
is
fully
comp
liant
with
all
applicable
requirements
under
applicable
Union
harmonisation
legislation
.
In
ensur
ing
the
compliance
of
higher
isk
AI
systems
referred
to
in
paragraph
1
with
the
requirements
set
out
in
this
Section
,
and
in
order
to
ensure
consistency
,
avoid
duplication
and
minimise
additional
burdens
,
provid
ers
shall
have
a
choice
of
integrat
ing
,
as
appropriate
,
the
necessary
testing
and
repor
ting
processes
,
information
and
documentation
they
provide
with
rega
rd
to
their
product
into
documentation
and
procedures
that
already
exist
and
are
required
under
the
Union
harmonisation
legislation
listed
in
Section
A
of
Annex
I.
Article
9
Risk
management
system
systems
.
entire
lifecycle
of
a
higher
isk
AI
system
,
requir
ing
regular
systematic
review
and
updating
.
It
shall
comp
rise
the
following
steps
:
(
a)the
identification
and
analysis
of
the
known
and
the
reasonably
foreseeable
risks
that
the
higher
isk
AI
system
can
pose
to
health
,
safety
or
fundamental
rights
when
the
higher
isk
AI
syste
m
is
used
in
accordance
with
its
intende
d
purpose
;
(
b)the
estimation
and
evaluation
of
the
risks
that
may
emerg
e
when
the
higher
isk
AI
syste
m
is
used
in
accordance
with
its
intende
d
purpose
,
and
under
conditions
of
reasonably
foreseeable
misuse
;
(
c)the
evaluation
of
other
risks
possibly
arising
,
based
on
the
analysis
of
data
gathered
from
the
post
-
mark
et
monitoring
syste
m
referred
to
in
Article
72
;
(
d)the
adoption
of
appropriate
and
target
ed
risk
management
measures
designed
to
address
the
risks
identified
pursuant
to
point
(
a
)
.
the
development
or
design
of
the
higher
isk
AI
syste
m
,
or
the
provision
of
adequat
e
technical
information
.
possible
interaction
resulting
from
the
combined
application
of
the
requirements
set
out
in
this
Section
,
with
a
view
to
minimising
risks
more
effectively
while
achieving
an
appropriate
balance
in
implementing
the
measures
to
fulfil
those
requirements
.
associate
d
with
each
hazard
,
as
well
as
the
overall
residual
risk
of
the
higher
isk
AI
systems
is
judged
to
be
accep
table
.
In
identifying
the
most
appropriate
risk
managem
ent
measures
,
the
following
shall
be
ensured
:
(
a)elimination
or
reduction
of
risks
identifie
d
and
evaluate
d
pursuant
to
paragraph
2
in
as
far
as
technically
feasible
through
adequate
design
and
development
of
the
higher
isk
AI
syste
m
;
(
b)where
appropriate
,
imp
lementation
of
adequate
mitiga
tion
and
control
measures
addressing
risks
that
can
not
be
eliminated
;
(
c)provision
of
information
required
pursuant
to
Article
13
and
,
where
appropriate
,
training
to
deplo
yers
.
With
a
view
to
eliminating
or
reducing
risks
relate
d
to
the
use
of
the
higher
isk
AI
system
,
due
consideration
shall
be
given
to
the
technical
kno
wledge
,
exper
ience
,
education
,
the
training
to
be
expected
by
the
deplo
yer
,
and
the
presumable
cont
ext
in
which
the
system
is
intended
to
be
used
.
higher
isk
AI
systems
shall
be
tested
for
the
purpose
of
identifying
the
most
appropriate
and
target
ed
risk
management
measures
.
Testing
shall
ensure
that
higher
isk
AI
systems
perfor
m
consist
ently
for
their
intended
purpose
and
that
they
are
in
complia
nce
with
the
requirements
set
out
in
this
Section
.
process
,
and
,
in
any
event
,
prior
to
their
being
placed
on
the
market
or
put
into
service
.
Testing
shall
be
carried
out
agains
t
prior
defined
metr
ics
and
probabilistic
thresholds
that
are
appropriate
to
the
intended
purpose
of
the
higher
isk
AI
syste
m.
consideration
to
whether
in
view
of
its
intende
d
purpose
the
higher
isk
AI
syste
m
is
likely
to
have
an
adverse
impact
on
persons
under
the
age
of
18
and
,
as
appropriate
,
other
vulnerable
groups
.
under
other
relevant
provisions
of
Union
law
,
the
aspects
provided
in
paragraphs
1
to
9
may
be
part
of
,
or
combined
with
,
the
risk
managem
ent
procedures
established
pursuant
to
that
law
.
Article
10
Data
and
data
governance
on
the
basis
of
training
,
validation
and
testing
data
sets
that
meet
the
quality
criteria
referred
to
in
paragraphs
2
to
5
whenever
such
data
sets
are
used
.
for
the
intended
purpose
of
the
higher
isk
AI
syste
m.
Those
practices
shall
concer
n
in
particular
:
(
a)the
relevant
design
choices
;
(
b)data
collection
processes
and
the
origin
of
data
,
and
in
the
case
of
personal
data
,
the
original
purpose
of
the
data
collection
;
(
c)relevant
data
-
preparation
processing
operations
,
such
as
annotation
,
labelling
,
cleaning
,
updating
,
enrichment
and
aggregati
on
;
(
d)the
formulation
of
assum
ptions
,
in
particular
with
respect
to
the
information
that
the
data
are
supposed
to
measure
and
represent
;
(
e)an
assessment
of
the
availability
,
quantity
and
suitability
of
the
data
sets
that
are
needed
;
(
f)examination
in
view
of
possible
biases
that
are
likel
y
to
affect
the
health
and
safety
of
persons
,
have
a
negative
impact
on
fundamental
rights
or
lead
to
discr
imination
prohibited
under
Union
law
,
especially
where
data
outputs
influence
input
s
for
future
operations
;
(
g)appropriate
measures
to
detect
,
prevent
and
mitigat
e
possible
biases
identifie
d
according
to
point
(
f
)
;
(
h)the
identifica
tion
of
relevant
data
gaps
or
shor
tcomings
that
prevent
compliance
with
this
Regulation
,
and
how
those
gaps
and
shor
tcomings
can
be
addressed
.
free
of
errors
and
comp
lete
in
view
of
the
intended
purpose
.
They
shall
have
the
appropriate
statistical
proper
ties
,
including
,
where
applicable
,
as
regards
the
persons
or
groups
of
persons
in
relation
to
whom
the
higher
isk
AI
system
is
intende
d
to
be
used
.
Those
charact
eristics
of
the
data
sets
may
be
met
at
the
level
of
individual
data
sets
or
at
the
level
of
a
combination
thereof
.
are
particular
to
the
specific
geographical
,
cont
extual
,
behavi
oural
or
functional
setting
within
which
the
higher
isk
AI
syste
m
is
intende
d
to
be
used
.
.
To
the
extent
that
it
is
strictly
necessary
for
the
purpose
of
ensur
ing
bias
detection
and
correction
in
relation
to
the
higher
isk
AI
systems
in
accordance
with
paragraph
,
points
(
f
)
and
(
g
)
of
this
Article
,
the
provid
ers
of
such
systems
may
exceptionally
process
special
categor
ies
of
personal
data
,
subject
to
appropriate
safeguards
for
the
fundamental
rights
and
freedoms
of
natural
persons
.
In
addition
to
the
provisions
set
out
in
Regulations
(
EU
)
and
(
EU
)
and
Directive
(
EU
)
,
all
the
following
conditions
must
be
met
in
order
for
such
processing
to
occur
:
(
a)the
bias
detection
and
correction
can
not
be
effectively
fulfilled
by
processing
other
data
,
including
synthetic
or
anonymised
data
;
(
b)the
special
cate
gories
of
personal
data
are
subject
to
technical
limitations
on
the
re
-
use
of
the
personal
data
,
and
state
-
of
-
t
he
-
ar
t
security
and
privacy
-
preser
ving
measures
,
including
pseudon
ymisation
;
(
c)the
special
cate
gories
of
personal
data
are
subject
to
measures
to
ensure
that
the
personal
data
processed
are
secured
,
protected
,
subject
to
suitable
safeguards
,
including
strict
controls
and
documentation
of
the
access
,
to
avoid
misuse
and
ensure
that
only
authorised
persons
have
access
to
those
personal
data
with
appropriate
conf
identiality
obligations
;
(
d)the
special
categori
es
of
personal
data
are
not
to
be
transmitte
d
,
transferre
d
or
other
wise
accessed
by
other
parties
;
(
e)the
special
categor
ies
of
personal
data
are
delete
d
once
the
bias
has
been
corrected
or
the
personal
data
has
reac
he
d
the
end
of
its
retention
period
,
whichever
comes
first
;
(
f)the
records
of
processing
activities
pursuant
to
Regulations
(
EU
)
and
(
EU
)
and
Directive
(
EU
)
include
the
reasons
why
the
processing
of
special
categor
ies
of
personal
data
was
strictly
necessary
to
detect
and
correct
biases
,
and
why
that
objective
could
not
be
achi
eved
by
processing
other
data
.
to
5
apply
only
to
the
testing
data
sets
.
Article
11
Technical
documentation
put
into
service
and
shall
be
kept
up
-
to
date
.
The
technical
documentation
shall
be
drawn
up
in
such
a
way
as
to
demonstrate
that
the
higher
isk
AI
system
complie
s
with
the
requirements
set
out
in
this
Section
and
to
provide
national
comp
etent
authorities
and
notified
bodies
with
the
necessary
information
in
a
clear
and
compre
hensive
form
to
assess
the
compliance
of
the
AI
syste
m
with
those
requirements
.
It
shall
contain
,
at
a
minimum
,
the
elements
set
out
in
Annex
IV
.
SMEs
,
including
start
-
ups
,
may
provid
e
the
elements
of
the
technical
documentation
specified
in
Annex
IV
in
a
simplified
manner
.
To
that
end
,
the
Commission
shall
establish
a
simplified
technical
documentation
form
target
ed
at
the
needs
of
small
and
microent
erprises
.
Where
an
SME
,
including
a
start
-
up
,
opts
to
provide
the
information
required
in
Annex
IV
in
a
simplified
manner
,
it
shall
use
the
form
referred
to
in
this
paragraph
.
Notified
bodies
shall
accep
t
the
form
for
the
purposes
of
the
conf
ormity
assessment
.
A
of
Annex
I
is
placed
on
the
market
or
put
into
service
,
a
sing
le
set
of
technical
documentation
shall
be
drawn
up
containing
all
the
information
set
out
in
paragraph
1
,
as
well
as
the
information
required
under
those
lega
l
acts
.
where
necessary
,
to
ensure
that
,
in
light
of
technical
progress
,
the
technical
documentation
provides
all
the
information
necessary
to
assess
the
compliance
of
the
system
with
the
requirements
set
out
in
this
Section
.
Record
-
ke
eping
syste
m.
purpose
of
the
syste
m
,
logging
capabilities
shall
enable
the
recording
of
events
relevant
for
:
(
a)identifying
situations
that
may
result
in
the
higher
isk
AI
system
presenting
a
risk
within
the
meaning
of
Article
79(1
)
or
in
a
substantial
modification
;
(
b)facilitating
the
post
-
market
monitoring
refer
red
to
in
Article
72
;
and
(
c)monitoring
the
operation
of
higher
isk
AI
systems
referred
to
in
Article
26(5
)
.
Article
13
Transparency
and
provision
of
information
to
deplo
yers
transparent
to
enable
deplo
yers
to
interpret
a
syste
m
’s
output
and
use
it
appropriately
.
An
appropriate
type
and
degree
of
transparency
shall
be
ensured
with
a
view
to
achieving
compliance
with
the
relevant
obligations
of
the
provid
er
and
deplo
yer
set
out
in
Section
3
.
include
concise
,
complet
e
,
correct
and
clear
information
that
is
relevant
,
accessible
and
comp
rehensible
to
deplo
yers
.
higher
isk
AI
syste
m
has
been
tested
and
validat
ed
and
which
can
be
expected
,
and
any
known
and
foreseeable
circumstances
that
may
have
an
imp
act
on
that
expected
level
of
accuracy
,
robustness
and
cybersecurity
;
(
iii
)
any
kno
wn
or
foreseeable
circumstance
,
relate
d
to
the
use
of
the
higher
isk
AI
syste
m
in
accordance
with
its
intended
purpose
or
under
conditions
of
reasonably
foreseeable
misuse
,
which
may
lead
to
risks
to
the
health
and
safety
or
fundamental
rights
refer
red
to
in
Article
9(2
)
;
(
iv
)
where
applicable
,
the
technical
capabilities
and
character
istics
of
the
higher
isk
AI
system
to
provid
e
information
that
is
relevant
to
explain
its
output
;
(
v)when
appropriate
,
its
perform
ance
rega
rding
specific
persons
or
groups
of
persons
on
which
the
system
is
intended
to
be
used
;
(
vi
)
when
appropriate
,
specifications
for
the
input
data
,
or
any
other
relevant
information
in
terms
of
the
training
,
validation
and
testing
data
sets
used
,
taking
into
account
the
intende
d
purpose
of
the
higher
isk
AI
system
;
(
vii
)
where
applicable
,
information
to
enable
deplo
yers
to
inter
pret
the
output
of
the
higher
isk
AI
system
and
use
it
appropriately
;
(
c)the
chang
es
to
the
higher
isk
AI
syste
m
and
its
perf
ormance
which
have
been
pre
-
dete
rmined
by
the
provider
at
the
moment
of
the
initial
conf
ormity
assessment
,
if
any
;
(
d)the
human
oversight
measures
referred
to
in
Article
14
,
including
the
technical
measures
put
in
place
to
facilitate
the
inter
pretation
of
the
outputs
of
the
higher
isk
AI
systems
by
the
deplo
yers
;
(
e)the
computa
tional
and
hardware
resources
needed
,
the
expect
ed
lifetime
of
the
higher
isk
AI
system
and
any
necessary
mainte
nance
and
care
measures
,
including
their
frequency
,
to
ensure
the
proper
functioning
of
that
AI
system
,
including
as
regards
software
update
s
;
(
f)where
relevant
,
a
descr
iption
of
the
mechanisms
included
within
the
higher
isk
AI
system
that
allows
deplo
yers
to
properly
collect
,
store
and
interpret
the
logs
in
accordance
with
Article
12
.
Article
14
Human
oversight
interfac
e
tools
,
that
they
can
be
effectively
oversee
n
by
natural
persons
during
the
period
in
which
they
are
in
use
.
when
a
higher
isk
AI
syste
m
is
used
in
accordance
with
its
intended
purpose
or
under
conditions
of
reasonably
foreseeable
misuse
,
in
particular
where
such
risks
persist
despite
the
application
of
other
requirements
set
out
in
this
Section
.
AI
system
,
and
shall
be
ensured
through
either
one
or
both
of
the
following
types
of
measures
:
(
a)measures
identified
and
built
,
when
technically
feasible
,
into
the
higher
isk
AI
system
by
the
provid
er
before
it
is
placed
on
the
mark
et
or
put
into
service
;
(
b)measures
identifie
d
by
the
provider
before
placing
the
higher
isk
AI
syste
m
on
the
market
or
putting
it
into
service
and
that
are
appropriate
to
be
implement
ed
by
the
deplo
yer
.
such
a
way
that
natural
persons
to
whom
human
oversight
is
assigned
are
enabled
,
as
appropriate
and
propor
tionate
:
(
a)to
properly
understand
the
relevant
capacities
and
limitations
of
the
higher
isk
AI
system
and
be
able
to
duly
monitor
its
operation
,
including
in
view
of
detecting
and
addressing
anomalies
,
dysfunctions
and
unexpect
ed
perfo
rmance
;
(
b)to
remain
aware
of
the
possible
tendency
of
automat
ically
relying
or
over
-relying
on
the
output
produced
by
a
higher
isk
AI
syste
m
(
automation
bias
)
,
in
particular
for
higher
isk
AI
systems
used
to
provide
information
or
recommendations
for
decisions
to
be
take
n
by
natural
persons
;
(
c)to
correctly
interpret
the
higher
isk
AI
system
’s
output
,
taking
into
account
,
for
example
,
the
interpretat
ion
tools
and
methods
available
;
decide
,
in
any
particular
situation
,
not
to
use
the
higher
isk
AI
syste
m
or
to
other
wise
disreg
ard
,
overr
ide
or
reverse
the
output
of
the
higher
isk
AI
syste
m
;
(
e)to
inter
vene
in
the
operation
of
the
higher
isk
AI
syste
m
or
inter
rupt
the
syste
m
through
a
‘
stop
’
button
or
a
similar
procedure
that
allows
the
syste
m
to
come
to
a
halt
in
a
safe
state
.
shall
be
such
as
to
ensure
that
,
in
addition
,
no
action
or
decision
is
take
n
by
the
deplo
yer
on
the
basis
of
the
identification
resulting
from
the
system
unless
that
identification
has
been
separate
ly
verified
and
confi
rmed
by
at
least
two
natural
persons
with
the
necessary
compet
ence
,
training
and
author
ity
.
The
requirement
for
a
separate
verificati
on
by
at
least
two
natural
persons
shall
not
apply
to
higher
isk
AI
systems
used
for
the
purposes
of
law
enforcement
,
migration
,
border
control
or
asylum
,
where
Union
or
national
law
considers
the
application
of
this
requirement
to
be
dispropor
tionate
.
Article
15
Accuracy
,
robustness
and
cybersecurity
robustness
,
and
cybersecurity
,
and
that
they
perf
orm
consiste
ntly
in
those
respects
throughout
their
lifecy
cle
.
paragraph
1
and
any
other
relevant
perform
ance
metr
ics
,
the
Commission
shall
,
in
cooperation
with
relevant
stakeholders
and
organisations
such
as
metrology
and
bench
marking
authorities
,
encourage
,
as
appropriate
,
the
development
of
bench
mark
s
and
measurement
methodologies
.
instr
uctions
of
use
.
the
syste
m
or
the
environment
in
which
the
syste
m
operat
es
,
in
particular
due
to
their
interaction
with
natural
persons
or
other
systems
.
Technical
and
organisational
measures
shall
be
take
n
in
this
regard
.
The
robustness
of
higher
isk
AI
systems
may
be
achieved
through
technical
redundancy
solutions
,
which
may
include
backu
p
or
fail
-
safe
plans
.
higher
isk
AI
systems
that
continue
to
learn
after
being
placed
on
the
market
or
put
into
service
shall
be
developed
in
such
a
way
as
to
eliminate
or
reduce
as
far
as
possible
the
risk
of
possibly
biased
outputs
influencing
input
for
future
operations
(
feedback
loops
)
,
and
as
to
ensure
that
any
such
feedback
loops
are
duly
addressed
with
appropriate
mitigati
on
measures
.
perfo
rmance
by
exploiting
syste
m
vulnerabilities
.
The
technical
solutions
aiming
to
ensure
the
cybersecurity
of
higher
isk
AI
systems
shall
be
appropriate
to
the
relevant
circumstances
and
the
risks
.
The
technical
solutions
to
address
AI
specific
vulnerabilities
shall
include
,
where
appropriate
,
measures
to
prevent
,
detect
,
respond
to
,
resolve
and
control
for
attac
ks
trying
to
manipulat
e
the
training
data
set
(
data
poisoning
)
,
or
pre
-
trained
compo
nents
used
in
training
(
model
poisoning
)
,
inputs
designed
to
cause
the
AI
model
to
make
a
mistake
(
adversar
ial
examples
or
model
evasion
)
,
confi
dentiality
attac
ks
or
model
flaws
.
Oblig
ations
of
providers
and
deploye
rs
of
higher
isk
AI
systems
and
other
parties
Article
16
Obligations
of
providers
of
higher
isk
AI
systems
Provi
ders
of
higher
isk
AI
systems
shall
:
(
a)ensure
that
their
higher
isk
AI
systems
are
compliant
with
the
requirements
set
out
in
Section
2
;
(
b)indicate
on
the
higher
isk
AI
syste
m
or
,
where
that
is
not
possible
,
on
its
packaging
or
its
accom
panying
documentation
,
as
applicable
,
their
name
,
regist
ered
trade
name
or
register
ed
trade
mark
,
the
address
at
which
they
can
be
contacted
;
(
c)have
a
quality
management
syste
m
in
place
which
comp
lies
with
Article
17
;
(
d)keep
the
documentation
refer
red
to
in
Article
18
;
(
e)when
under
their
control
,
keep
the
logs
auto
matically
generated
by
their
higher
isk
AI
systems
as
referred
to
in
Article
19
;
(
f)ensure
that
the
higher
isk
AI
system
undergoes
the
relevant
conf
ormity
assessment
procedure
as
refer
red
to
in
Article
43
,
prior
to
its
being
placed
on
the
mark
et
or
put
into
service
;
(
g)draw
up
an
EU
declaration
of
conf
ormity
in
accordance
with
Article
47
;
(
h)affix
the
CE
marking
to
the
higher
isk
AI
system
or
,
where
that
is
not
possible
,
on
its
packag
ing
or
its
accom
panying
documentation
,
to
indicate
conf
ormity
with
this
Regulation
,
in
accordance
with
Article
48
;
(
i)comply
with
the
registration
oblig
ations
refer
red
to
in
Article
49(1
)
;
(
j)take
the
necessary
corrective
actions
and
provide
information
as
required
in
Article
20
;
(
k)upon
a
reasoned
request
of
a
national
comp
etent
author
ity
,
demonstrate
the
conf
ormity
of
the
higher
isk
AI
system
with
the
requirements
set
out
in
Section
2
;
(
l)ensure
that
the
higher
isk
AI
system
com
plies
with
accessibility
requirements
in
accordance
with
Directives
(
EU
)
and
(
EU
)
.
Article
17
Quality
management
system
Regulation
.
That
system
shall
be
documente
d
in
a
syste
matic
and
orderly
manner
in
the
form
of
written
policies
,
procedures
and
instr
uctions
,
and
shall
include
at
least
the
following
aspects
:
(
a
)
a
strate
gy
for
regulator
y
compliance
,
including
compliance
with
conf
ormity
assessment
procedures
and
procedures
for
the
management
of
modifications
to
the
higher
isk
AI
system
;
(
b
)
techniques
,
procedures
and
syste
matic
actions
to
be
used
for
the
design
,
design
control
and
design
verification
of
the
higher
isk
AI
syste
m
;
(
c
)
techniques
,
procedures
and
systematic
actions
to
be
used
for
the
development
,
quality
control
and
quality
assurance
of
the
higher
isk
AI
system
;
(
d
)
examination
,
test
and
validation
procedures
to
be
carried
out
before
,
during
and
after
the
development
of
the
higher
isk
AI
system
,
and
the
frequency
with
which
they
have
to
be
carried
out
;
technical
specifications
,
including
standards
,
to
be
applied
and
,
where
the
relevant
harmonised
standards
are
not
applied
in
full
or
do
not
cover
all
of
the
relevant
requirements
set
out
in
Section
2
,
the
means
to
be
used
to
ensure
that
the
higher
isk
AI
system
complies
with
those
requirements
;
(
f
)
systems
and
procedures
for
data
managem
ent
,
including
data
acquisition
,
data
collection
,
data
analysis
,
data
labelling
,
data
storage
,
data
filtration
,
data
mining
,
data
aggregation
,
data
retention
and
any
other
operation
regard
ing
the
data
that
is
perfo
rmed
before
and
for
the
purpose
of
the
placing
on
the
mark
et
or
the
putting
into
service
of
higher
isk
AI
systems
;
(
g
)
the
risk
managem
ent
syste
m
referred
to
in
Article
9
;
(
h
)
the
setting
-
up
,
imp
lementation
and
maintenance
of
a
post
-
mark
et
monitoring
syste
m
,
in
accordance
with
Article
72
;
(
i
)
procedures
related
to
the
repor
ting
of
a
serious
incident
in
accordance
with
Article
73
;
(
j
)
the
handling
of
communication
with
national
competent
authorities
,
other
relevant
authorities
,
including
those
provid
ing
or
supporting
the
access
to
data
,
notif
ied
bodies
,
other
operators
,
custome
rs
or
other
intere
sted
parties
;
(
k
)
systems
and
procedures
for
record
-
keepi
ng
of
all
relevant
documentation
and
information
;
(
l
)
resource
management
,
including
security
-
of
-
supply
related
measures
;
(
m
)
an
accountability
framework
setting
out
the
responsibilities
of
the
management
and
other
staff
with
regard
to
all
the
aspects
listed
in
this
paragraph
.
organisation
.
Providers
shall
,
in
any
event
,
respect
the
degree
of
rigour
and
the
level
of
protection
required
to
ensure
the
complia
nce
of
their
higher
isk
AI
systems
with
this
Regulation
.
equivalent
function
under
relevant
sectoral
Union
law
may
include
the
aspects
listed
in
paragraph
1
as
part
of
the
quality
managem
ent
systems
pursuant
to
that
law
.
or
processes
under
Union
financial
services
law
,
the
obligation
to
put
in
place
a
quality
management
syste
m
,
with
the
exception
of
paragraph
1
,
points
(
g
)
,
(
h
)
and
(
i
)
of
this
Article
,
shall
be
deemed
to
be
fulfilled
by
comp
lying
with
the
rules
on
inter
nal
gover
nance
arrang
ements
or
processes
pursuant
to
the
relevant
Union
financ
ial
services
law
.
To
that
end
,
any
harmonised
standards
referred
to
in
Article
40
shall
be
take
n
into
account
.
Article
18
Documentation
keep
ing
into
service
,
keep
at
the
disposal
of
the
national
competent
authorities
:
(
a)the
technical
documentation
refer
red
to
in
Article
11
;
(
b)the
documentation
concerning
the
quality
management
syste
m
referred
to
in
Article
17
;
(
c)the
documentation
concerning
the
changes
approved
by
notif
ied
bodies
,
where
applicable
;
(
d)the
decisions
and
other
documents
issued
by
the
notified
bodies
,
where
applicable
;
(
e)the
EU
declaration
of
conf
ormity
referred
to
in
Article
47
.
.
Each
Member
State
shall
determine
conditions
under
which
the
documentation
referred
to
in
paragraph
1
remains
at
the
disposal
of
the
national
competent
authorities
for
the
period
indicated
in
that
paragraph
for
the
cases
when
a
provid
er
or
its
authorised
representative
established
on
its
territory
goes
bankr
upt
or
ceases
its
activity
prior
to
the
end
of
that
period
.
processes
under
Union
financ
ial
services
law
shall
maintain
the
technical
documentation
as
part
of
the
documentation
kept
under
the
relevant
Union
financial
services
law
.
Article
19
Aut
omatically
generated
logs
higher
isk
AI
systems
,
to
the
extent
such
logs
are
under
their
control
.
Without
prejudice
to
applicable
Union
or
national
law
,
the
logs
shall
be
kept
for
a
period
appropriate
to
the
intended
purpose
of
the
higher
isk
AI
syste
m
,
of
at
least
six
months
,
unless
provided
other
wise
in
the
applicable
Union
or
national
law
,
in
particular
in
Union
law
on
the
protect
ion
of
personal
data
.
processes
under
Union
financial
services
law
shall
maintain
the
logs
automa
tically
generated
by
their
higher
isk
AI
systems
as
part
of
the
documentation
kept
under
the
relevant
financial
services
law
.
Article
20
Cor
rectiv
e
actions
and
duty
of
infor
mation
placed
on
the
market
or
put
into
service
is
not
in
conf
ormity
with
this
Regulation
shall
immediate
ly
take
the
necessary
corrective
actions
to
bring
that
system
into
conf
ormity
,
to
withdraw
it
,
to
disable
it
,
or
to
recall
it
,
as
appropriate
.
They
shall
inform
the
distr
ibutors
of
the
higher
isk
AI
syste
m
concer
ned
and
,
where
applicable
,
the
deplo
yers
,
the
authorised
representative
and
imp
orters
according
ly
.
that
risk
,
it
shall
immediate
ly
investig
ate
the
causes
,
in
collaboration
with
the
repor
ting
deplo
yer
,
where
applicable
,
and
inform
the
market
surveillance
authorities
competent
for
the
higher
isk
AI
system
concer
ned
and
,
where
applicable
,
the
notif
ied
body
that
issued
a
certificate
for
that
higher
isk
AI
system
in
accordance
with
Article
44
,
in
particular
,
of
the
nature
of
the
non
-
compliance
and
of
any
relevant
corrective
action
taken
.
Article
21
Cooperation
with
competent
authorities
the
information
and
documentation
necessary
to
demonstrate
the
conf
ormity
of
the
higher
isk
AI
syste
m
with
the
requirements
set
out
in
Section
2
,
in
a
languag
e
which
can
be
easily
understood
by
the
author
ity
in
one
of
the
official
languag
es
of
the
institutions
of
the
Union
as
indicate
d
by
the
Member
State
concer
ned
.
applicable
,
access
to
the
automat
ically
generated
logs
of
the
higher
isk
AI
syste
m
refer
red
to
in
Article
12(1
)
,
to
the
extent
such
logs
are
under
their
control
.
confi
dentiality
oblig
ations
set
out
in
Article
78
.
authorised
representativ
es
of
providers
of
higher
isk
AI
systems
shall
,
by
written
mandate
,
appoint
an
authorised
representative
which
is
established
in
the
Union
.
provid
er
.
provid
e
a
copy
of
the
mandate
to
the
market
surveillance
authorities
upon
request
,
in
one
of
the
official
languages
of
the
institutions
of
the
Union
,
as
indicated
by
the
competent
author
ity
.
For
the
purposes
of
this
Regulation
,
the
mandate
shall
emp
ower
the
authorised
representative
to
carry
out
the
following
tasks
:
(
a)verify
that
the
EU
declaration
of
conf
ormity
refer
red
to
in
Article
47
and
the
technical
documentation
referred
to
in
Article
11
have
been
drawn
up
and
that
an
appropriate
conf
ormity
assessment
procedure
has
been
carried
out
by
the
provid
er
;
(
b)keep
at
the
disposal
of
the
comp
etent
authorities
and
national
authorities
or
bodies
referred
to
in
Article
74(10
)
,
for
a
period
of
10
years
after
the
higher
isk
AI
syste
m
has
been
placed
on
the
market
or
put
into
service
,
the
contact
details
of
the
provid
er
that
appoint
ed
the
authorised
representative
,
a
copy
of
the
EU
declaration
of
conf
ormity
refer
red
to
in
Article
47
,
the
technical
documentation
and
,
if
applicable
,
the
certificate
issued
by
the
notif
ied
body
;
(
c)provid
e
a
competent
author
ity
,
upon
a
reasoned
request
,
with
all
the
information
and
documentation
,
including
that
referred
to
in
point
(
b
)
of
this
subparagraph
,
necessary
to
demonstrate
the
conf
ormity
of
a
higher
isk
AI
system
with
the
requirements
set
out
in
Section
2
,
including
access
to
the
logs
,
as
referred
to
in
Article
12(1
)
,
auto
matically
generated
by
the
higher
isk
AI
system
,
to
the
extent
such
logs
are
under
the
control
of
the
provid
er
;
(
d)cooperate
with
competent
authorities
,
upon
a
reasoned
request
,
in
any
action
the
latter
take
in
relation
to
the
higher
isk
AI
system
,
in
particular
to
reduce
and
mitiga
te
the
risks
posed
by
the
higher
isk
AI
system
;
(
e)where
applicable
,
comply
with
the
registration
obligations
refer
red
to
in
Article
49(1
)
,
or
,
if
the
registration
is
carried
out
by
the
provid
er
itself
,
ensure
that
the
information
refer
red
to
in
point
3
of
Section
A
of
Annex
VIII
is
correct
.
The
mandate
shall
empo
wer
the
authorised
representative
to
be
addressed
,
in
addition
to
or
instead
of
the
provider
,
by
the
competent
authorities
,
on
all
issues
related
to
ensur
ing
compliance
with
this
Regulation
.
acting
contrar
y
to
its
obligations
pursuant
to
this
Regulation
.
In
such
a
case
,
it
shall
immediately
inform
the
relevant
market
surveillance
author
ity
,
as
well
as
,
where
applicable
,
the
relevant
notified
body
,
about
the
termination
of
the
mandat
e
and
the
reasons
theref
or
.
Article
23
Obligations
of
impor
ters
Regulation
by
verifying
that
:
(
a)the
relevant
conf
ormity
assessment
procedure
refer
red
to
in
Article
43
has
been
carried
out
by
the
provider
of
the
higher
isk
AI
syste
m
;
(
b)the
provid
er
has
drawn
up
the
technical
documentation
in
accordance
with
Article
11
and
Annex
IV
;
(
c)the
system
bears
the
required
CE
marking
and
is
accompanied
by
the
EU
declaration
of
conf
ormity
refer
red
to
in
Article
47
and
instr
uctions
for
use
;
(
d)the
provid
er
has
appoint
ed
an
authorised
representative
in
accordance
with
Article
22(1
)
.
.
Where
an
impor
ter
has
sufficient
reason
to
consider
that
a
higher
isk
AI
system
is
not
in
conf
ormity
with
this
Regulation
,
or
is
falsifi
ed
,
or
accompanie
d
by
falsifi
ed
documentation
,
it
shall
not
place
the
syste
m
on
the
market
until
it
has
been
brought
into
conf
ormity
.
Where
the
higher
isk
AI
syste
m
presents
a
risk
within
the
meaning
of
Article
79(1
)
,
the
imp
orter
shall
inform
the
provid
er
of
the
syste
m
,
the
authorised
representative
and
the
market
surveillance
authorities
to
that
effect
.
be
contacted
on
the
higher
isk
AI
syste
m
and
on
its
packaging
or
its
accom
panying
documentation
,
where
applicable
.
where
applicable
,
do
not
jeopardise
its
compliance
with
the
requirements
set
out
in
Section
2
.
service
,
a
copy
of
the
certificate
issued
by
the
notif
ied
body
,
where
applicable
,
of
the
instr
uctions
for
use
,
and
of
the
EU
declaration
of
conf
ormity
referred
to
in
Article
47
.
information
and
documentation
,
including
that
refer
red
to
in
paragraph
5
,
to
demonstrat
e
the
conf
ormity
of
a
higher
isk
AI
syste
m
with
the
requirements
set
out
in
Section
2
in
a
languag
e
which
can
be
easily
understoo
d
by
them
.
For
this
purpose
,
they
shall
also
ensure
that
the
technical
documentation
can
be
made
available
to
those
authorities
.
a
higher
isk
AI
syste
m
placed
on
the
market
by
the
importers
,
in
particular
to
reduce
and
mitig
ate
the
risks
posed
by
it
.
Article
24
Obligations
of
distr
ibutors
marking
,
that
it
is
accom
panied
by
a
copy
of
the
EU
declaration
of
conf
ormity
referred
to
in
Article
47
and
instr
uctions
for
use
,
and
that
the
provid
er
and
the
imp
orter
of
that
syste
m
,
as
applicable
,
have
complied
with
their
respective
obligations
as
laid
down
in
Article
16
,
points
(
b
)
and
(
c
)
and
Article
23(3
)
.
a
higher
isk
AI
system
is
not
in
conf
ormity
with
the
requirements
set
out
in
Section
2
,
it
shall
not
mak
e
the
higher
isk
AI
syste
m
available
on
the
market
until
the
syste
m
has
been
brought
into
conf
ormity
with
those
requirements
.
Further
more
,
where
the
higher
isk
AI
system
presents
a
risk
within
the
meaning
of
Article
79(1
)
,
the
distr
ibutor
shall
inform
the
provid
er
or
the
imp
orter
of
the
syste
m
,
as
applicable
,
to
that
effect
.
conditions
,
where
applicable
,
do
not
jeopardise
the
compliance
of
the
syste
m
with
the
requirements
set
out
in
Section
2
.
syste
m
which
it
has
made
available
on
the
market
not
to
be
in
conf
ormity
with
the
requirements
set
out
in
Section
2
,
shall
take
the
corrective
actions
necessary
to
bring
that
system
into
conf
ormity
with
those
requirements
,
to
withdra
w
it
or
recall
it
,
or
shall
ensure
that
the
provid
er
,
the
impor
ter
or
any
relevant
operat
or
,
as
appropriate
,
take
s
those
corrective
actions
.
Where
the
higher
isk
AI
system
presents
a
risk
within
the
meaning
of
Article
79(1
)
,
the
distr
ibut
or
shall
immediate
ly
inform
the
provid
er
or
impor
ter
of
the
syste
m
and
the
authorities
competent
for
the
higher
isk
AI
system
concer
ned
,
giving
details
,
in
particular
,
of
the
non
-
compliance
and
of
any
corrective
actions
taken
.
author
ity
with
all
the
information
and
documentation
regarding
their
actions
pursuant
to
paragraphs
1
to
4
necessary
to
demonstrate
the
conf
ormity
of
that
system
with
the
requirements
set
out
in
Section
2
.
a
higher
isk
AI
system
made
available
on
the
market
by
the
distr
ibutors
,
in
particular
to
reduce
or
mitigat
e
the
risk
posed
by
it
.
Responsibilities
along
the
AI
value
chain
for
the
purposes
of
this
Regulation
and
shall
be
subject
to
the
obligations
of
the
provider
under
Article
16
,
in
any
of
the
follo
wing
circumstances
:
(
a)they
put
their
name
or
trademark
on
a
higher
isk
AI
system
already
placed
on
the
market
or
put
into
service
,
without
prejudice
to
contractual
arrangements
stipulating
that
the
obligations
are
other
wise
allocat
ed
;
(
b)they
mak
e
a
substantial
modification
to
a
higher
isk
AI
system
that
has
already
been
placed
on
the
market
or
has
already
been
put
into
service
in
such
a
way
that
it
remains
a
higher
isk
AI
system
pursuant
to
Article
6
;
(
c)they
modify
the
intended
purpose
of
an
AI
system
,
including
a
general
-
pur
pose
AI
system
,
which
has
not
been
classif
ied
as
higher
isk
and
has
already
been
placed
on
the
market
or
put
into
service
in
such
a
way
that
the
AI
syste
m
concer
ned
becomes
a
higher
isk
AI
system
in
accordance
with
Article
6
.
market
or
put
it
into
service
shall
no
longe
r
be
considered
to
be
a
provider
of
that
specific
AI
syste
m
for
the
purposes
of
this
Regulation
.
That
initial
provider
shall
closely
cooperate
with
new
provid
ers
and
shall
make
available
the
necessary
information
and
provide
the
reasonably
expected
technical
access
and
other
assistance
that
are
required
for
the
fulfilment
of
the
oblig
ations
set
out
in
this
Regulation
,
in
particular
rega
rding
the
compliance
with
the
conf
ormity
assessment
of
higher
isk
AI
systems
.
This
paragraph
shall
not
apply
in
cases
where
the
initial
provider
has
clearly
specif
ied
that
its
AI
syste
m
is
not
to
be
changed
into
a
higher
isk
AI
system
and
theref
ore
does
not
fall
under
the
obligation
to
hand
over
the
documentation
.
legislation
listed
in
Section
A
of
Annex
I
,
the
product
manufa
cturer
shall
be
considered
to
be
the
provider
of
the
higher
isk
AI
system
,
and
shall
be
subject
to
the
obligations
under
Article
16
under
either
of
the
follo
wing
circumstances
:
(
a)the
higher
isk
AI
syste
m
is
placed
on
the
market
together
with
the
product
under
the
name
or
trademark
of
the
product
manufa
cturer
;
(
b)the
higher
isk
AI
system
is
put
into
service
under
the
name
or
trademark
of
the
product
manufacturer
after
the
product
has
been
placed
on
the
market
.
processes
that
are
used
or
integrat
ed
in
a
higher
isk
AI
system
shall
,
by
written
agreement
,
specify
the
necessary
information
,
capabilities
,
technical
access
and
other
assistance
based
on
the
generally
acknowledg
ed
state
of
the
art
,
in
order
to
enable
the
provider
of
the
higher
isk
AI
syste
m
to
fully
comply
with
the
obligations
set
out
in
this
Regulation
.
This
paragraph
shall
not
apply
to
third
parties
making
accessible
to
the
public
tools
,
services
,
processes
,
or
comp
onents
,
other
than
general
-
pur
pose
AI
models
,
under
a
free
and
open
-
source
licence
.
The
AI
Office
may
develop
and
recommend
voluntar
y
model
terms
for
contracts
between
provid
ers
of
higher
isk
AI
systems
and
third
parties
that
supply
tools
,
services
,
comp
onents
or
processes
that
are
used
for
or
integrated
into
higher
isk
AI
systems
.
When
developing
those
voluntar
y
model
terms
,
the
AI
Office
shall
take
into
account
possible
contractual
requirements
applicable
in
specific
sectors
or
business
cases
.
The
voluntar
y
model
terms
shall
be
published
and
be
available
free
of
charg
e
in
an
easily
usable
electronic
format
.
business
information
and
trade
secrets
in
accordance
with
Union
and
national
law
.
Article
26
Obligations
of
deplo
yers
of
higher
isk
AI
systems
such
systems
in
accordance
with
the
instr
uctions
for
use
accompan
ying
the
systems
,
pursuant
to
paragraphs
3
and
6
.
.
Deplo
yers
shall
assign
human
oversight
to
natural
persons
who
have
the
necessary
compet
ence
,
training
and
author
ity
,
as
well
as
the
necessary
support
.
national
law
and
to
the
deplo
yer
’s
freedom
to
organise
its
own
resources
and
activities
for
the
purpose
of
imp
lementing
the
human
overs
ight
measures
indicated
by
the
provid
er
.
deplo
yer
shall
ensure
that
input
data
is
relevant
and
suffi
ciently
representative
in
view
of
the
intende
d
purpose
of
the
higher
isk
AI
syste
m.
relevant
,
inform
provid
ers
in
accordance
with
Article
72
.
Where
deplo
yers
have
reason
to
consider
that
the
use
of
the
higher
isk
AI
system
in
accordance
with
the
instr
uctions
may
result
in
that
AI
syste
m
presenting
a
risk
within
the
meaning
of
Article
79(1
)
,
they
shall
,
without
undue
dela
y
,
inform
the
provider
or
distr
ibut
or
and
the
relevant
market
surveillance
author
ity
,
and
shall
suspend
the
use
of
that
system
.
Where
deplo
yers
have
identifie
d
a
serious
incident
,
they
shall
also
immediately
inform
first
the
provid
er
,
and
then
the
impor
ter
or
distr
ibut
or
and
the
relevant
market
surveillance
authorities
of
that
incident
.
If
the
deplo
yer
is
not
able
to
reac
h
the
provider
,
Article
73
shall
apply
mutatis
mutandis
.
This
obligation
shall
not
cover
sensitive
operational
data
of
deplo
yers
of
AI
systems
which
are
law
enforcement
authorities
.
For
deplo
yers
that
are
financ
ial
institutions
subject
to
requirements
regard
ing
their
inter
nal
gover
nance
,
arrangements
or
processes
under
Union
financial
services
law
,
the
monitorin
g
obligation
set
out
in
the
first
subparagraph
shall
be
deemed
to
be
fulfilled
by
complying
with
the
rules
on
inter
nal
govern
ance
arrangements
,
processes
and
mechanisms
pursuant
to
the
relevant
financial
service
law
.
such
logs
are
under
their
control
,
for
a
period
appropriate
to
the
intended
purpose
of
the
higher
isk
AI
system
,
of
at
least
six
months
,
unless
provid
ed
other
wise
in
applicable
Union
or
national
law
,
in
particular
in
Union
law
on
the
protect
ion
of
personal
data
.
Deplo
yers
that
are
financial
institutions
subject
to
requirements
regard
ing
their
inter
nal
gove
rnance
,
arrang
ements
or
processes
under
Union
financial
services
law
shall
maintain
the
logs
as
part
of
the
documentation
kept
pursuant
to
the
relevant
Union
financ
ial
service
law
.
workers
’
representatives
and
the
affect
ed
work
ers
that
they
will
be
subject
to
the
use
of
the
higher
isk
AI
system
.
This
information
shall
be
provided
,
where
applicable
,
in
accordance
with
the
rules
and
procedures
laid
down
in
Union
and
national
law
and
practice
on
information
of
work
ers
and
their
representatives
.
comply
with
the
registration
obligations
referred
to
in
Article
49
.
When
such
deplo
yers
find
that
the
higher
isk
AI
system
that
they
envisage
using
has
not
been
registered
in
the
EU
database
refer
red
to
in
Article
71
,
they
shall
not
use
that
system
and
shall
inform
the
provider
or
the
distr
ibutor
.
Regulation
to
comp
ly
with
their
obligation
to
carry
out
a
data
protection
imp
act
assessment
under
Article
35
of
Regulation
(
EU
)
or
Article
27
of
Directive
(
EU
)
.
a
person
suspect
ed
or
convict
ed
of
having
committed
a
criminal
offence
,
the
deplo
yer
of
a
higher
isk
AI
syste
m
for
post
-
remote
biometric
identifica
tion
shall
request
an
author
isation
,
ex
ante
,
or
without
undue
dela
y
and
no
later
than
48
hours
,
by
a
judicial
author
ity
or
an
administrative
author
ity
whose
decision
is
binding
and
subject
to
judicial
review
,
for
the
use
of
that
syste
m
,
excep
t
when
it
is
used
for
the
initial
identifica
tion
of
a
potential
suspect
based
on
objective
and
verifiable
facts
directly
linked
to
the
offence
.
Each
use
shall
be
limited
to
what
is
strictly
necessary
for
the
invest
igation
of
a
specific
criminal
offence
.
If
the
author
isation
requested
pursuant
to
the
first
subparagraph
is
rejected
,
the
use
of
the
post
-
remote
biometric
identifica
tion
syste
m
linked
to
that
requested
author
isation
shall
be
stopped
with
immediate
effect
and
the
personal
data
linked
to
the
use
of
the
higher
isk
AI
system
for
which
the
author
isation
was
request
ed
shall
be
delete
d.
no
case
shall
such
higher
isk
AI
syste
m
for
post
-
remote
biometric
identifica
tion
be
used
for
law
enforcement
purposes
in
an
untarget
ed
way
,
without
any
link
to
a
criminal
offence
,
a
criminal
proceeding
,
a
genuine
and
present
or
genuine
and
foreseeable
threat
of
a
criminal
offenc
e
,
or
the
searc
h
for
a
specific
missing
person
.
It
shall
be
ensured
that
no
decision
that
produces
an
adverse
legal
effect
on
a
person
may
be
taken
by
the
law
enforcement
authorities
based
solely
on
the
output
of
such
post
-
remote
biometric
identifi
cation
systems
.
This
paragraph
is
without
prejudice
to
Article
9
of
Regulation
(
EU
)
and
Article
10
of
Directive
(
EU
)
for
the
processing
of
biometric
data
.
Regardless
of
the
purpose
or
deplo
yer
,
each
use
of
such
higher
isk
AI
systems
shall
be
documented
in
the
relevant
police
file
and
shall
be
made
available
to
the
relevant
market
surveillance
author
ity
and
the
national
data
protection
author
ity
upon
request
,
excluding
the
disclosure
of
sensitive
operational
data
related
to
law
enforcement
.
This
subparagraph
shall
be
without
prejudice
to
the
powers
confe
rred
by
Directive
(
EU
)
on
supervisory
authorities
.
Deplo
yers
shall
submit
annual
repor
ts
to
the
relevant
market
surveillance
and
national
data
protection
authorities
on
their
use
of
post
-
remote
biometric
identifica
tion
systems
,
excluding
the
disclosure
of
sensitive
operational
data
related
to
law
enforcement
.
The
repor
ts
may
be
aggrega
ted
to
cover
more
than
one
deplo
yment
.
Member
States
may
introduce
,
in
accordance
with
Union
law
,
more
restr
ictive
laws
on
the
use
of
post
-
remote
biometric
identifica
tion
systems
.
make
decisions
or
assist
in
making
decisions
relate
d
to
natural
persons
shall
inform
the
natural
persons
that
they
are
subject
to
the
use
of
the
higher
isk
AI
syste
m.
For
higher
isk
AI
systems
used
for
law
enforcement
purposes
Article
13
of
Directive
(
EU
)
shall
apply
.
the
higher
isk
AI
system
in
order
to
imp
lement
this
Regulation
.
Article
27
Fundament
al
rights
impact
assessment
for
higher
isk
AI
systems
intende
d
to
be
used
in
the
area
listed
in
point
2
of
Annex
III
,
deplo
yers
that
are
bodies
gove
rned
by
public
law
,
or
are
private
entities
provid
ing
public
services
,
and
deplo
yers
of
higher
isk
AI
systems
refer
red
to
in
points
5
(
b
)
and
(
c
)
of
Annex
III
,
shall
perfo
rm
an
assessment
of
the
impact
on
fundamental
rights
that
the
use
of
such
syste
m
may
produce
.
For
that
purpose
,
deplo
yers
shall
perfo
rm
an
assessment
consisting
of
:
(
a)a
descr
iption
of
the
deplo
yer
’s
processes
in
which
the
higher
isk
AI
system
will
be
used
in
line
with
its
intende
d
purpose
;
(
b)a
descr
iption
of
the
period
of
time
within
which
,
and
the
frequency
with
which
,
each
higher
isk
AI
system
is
intended
to
be
used
;
(
c)the
cate
gories
of
natural
persons
and
groups
likely
to
be
affected
by
its
use
in
the
specific
cont
ext
;
(
d)the
specific
risks
of
harm
likely
to
have
an
imp
act
on
the
categori
es
of
natural
persons
or
groups
of
persons
identifie
d
pursuant
to
point
(
c
)
of
this
paragraph
,
taking
into
account
the
information
given
by
the
provider
pursuant
to
Article
13
;
(
e)a
descr
iption
of
the
imp
lementation
of
human
oversight
measures
,
according
to
the
instr
uctions
for
use
;
(
f)the
measures
to
be
taken
in
the
case
of
the
materialis
ation
of
those
risks
,
including
the
arrang
ements
for
internal
governance
and
comp
laint
mec
hanisms
.
.
The
obligation
laid
down
in
paragraph
1
applies
to
the
first
use
of
the
higher
isk
AI
system
.
The
deplo
yer
may
,
in
similar
cases
,
rely
on
previously
conducted
fundamental
rights
imp
act
assessments
or
existing
impact
assessments
carried
out
by
provid
er
.
If
,
during
the
use
of
the
higher
isk
AI
system
,
the
deplo
yer
considers
that
any
of
the
elements
listed
in
paragraph
1
has
changed
or
is
no
longe
r
up
to
date
,
the
deplo
yer
shall
take
the
necessary
steps
to
update
the
information
.
market
surveillance
author
ity
of
its
results
,
submitting
the
filled-
out
template
refer
red
to
in
paragraph
5
of
this
Article
as
part
of
the
notification
.
In
the
case
refer
red
to
in
Article
46(1
)
,
deplo
yers
may
be
exem
pt
from
that
obligation
to
notify
.
conducted
pursuant
to
Article
35
of
Regulation
(
EU
)
or
Article
27
of
Directive
(
EU
)
,
the
fundamental
rights
imp
act
assessment
referred
to
in
paragraph
1
of
this
Article
shall
comp
lement
that
data
protection
impact
assessment
.
in
com
plying
with
their
obligations
under
this
Article
in
a
simplified
manner
.
SECTION
4
Notifying
authorities
and
notified
bodies
Article
28
Notifying
authorities
out
the
necessary
procedures
for
the
assessment
,
designation
and
notif
ication
of
conf
ormity
assessment
bodies
and
for
their
monitoring
.
Those
procedures
shall
be
developed
in
cooperation
between
the
notifying
authorities
of
all
Member
States
.
a
national
accreditation
body
within
the
meaning
of
,
and
in
accordance
with
,
Regulation
(
EC
)
No
.
conf
ormity
assessment
bodies
,
and
that
the
objectivity
and
impar
tiality
of
their
activities
are
safeguarded
.
assessment
bodies
are
taken
by
competent
persons
diffe
rent
from
those
who
carried
out
the
assessment
of
those
bodies
.
consultancy
services
on
a
commercial
or
competitive
basis
.
Article
78
.
perfo
rmance
of
their
tasks
.
Comp
etent
personnel
shall
have
the
necessary
exper
tise
,
where
applicable
,
for
their
function
,
in
fields
such
as
information
technologi
es
,
AI
and
law
,
including
the
super
vision
of
fundamental
rights
.
Article
29
Application
of
a
confor
mity
assessment
body
for
notif
ication
State
in
which
they
are
established
.
The
application
for
notif
ication
shall
be
accompanied
by
a
descr
iption
of
the
conf
ormity
assessment
activities
,
the
conf
ormity
assessment
module
or
modules
and
the
types
of
AI
systems
for
which
the
conf
ormity
assessment
body
claims
to
be
competent
,
as
well
as
by
an
accreditation
certificate
,
where
one
exists
,
issued
by
a
national
accreditation
body
attesting
that
the
conf
ormity
assessment
body
fulfils
the
requirements
laid
down
in
Article
31
.
Any
valid
document
related
to
existing
designations
of
the
applicant
notified
body
under
any
other
Union
harmonisation
legislation
shall
be
added
.
notifying
author
ity
with
all
the
documentar
y
evidence
necessary
for
the
verification
,
recognition
and
regular
monitoring
of
its
compliance
with
the
requirements
laid
down
in
Article
31
.
certificates
linked
to
those
designations
may
be
used
to
support
their
designation
procedure
under
this
Regulation
,
as
appropriate
.
The
notified
body
shall
update
the
documentation
refer
red
to
in
paragraphs
2
and
3
of
this
Article
whenever
relevant
changes
occur
,
in
order
to
enable
the
author
ity
responsible
for
notified
bodies
to
monitor
and
verify
continuous
complia
nce
with
all
the
requirements
laid
down
in
Article
31
.
Article
30
Notif
ication
procedure
in
Article
31
.
developed
and
managed
by
the
Commission
,
of
each
conf
ormity
assessment
body
refer
red
to
in
paragraph
1
.
activities
,
the
conf
ormity
assessment
module
or
modules
,
the
types
of
AI
systems
concer
ned
,
and
the
relevant
attestation
of
compet
ence
.
Where
a
notification
is
not
based
on
an
accreditation
certificate
as
refer
red
to
in
Article
29(2
)
,
the
notifying
author
ity
shall
provide
the
Commission
and
the
other
Member
States
with
documentar
y
evidence
which
attests
to
the
compet
ence
of
the
conf
ormity
assessment
body
and
to
the
arrang
ements
in
place
to
ensure
that
that
body
will
be
monitor
ed
regularly
and
will
continue
to
satisfy
the
requirements
laid
down
in
Article
31
.
are
raised
by
the
Commission
or
the
other
Member
States
within
two
weeks
of
a
notification
by
a
notifying
author
ity
where
it
includes
an
accreditation
certificate
refer
red
to
in
Article
29(2
)
,
or
within
two
months
of
a
notification
by
the
notifying
author
ity
where
it
includes
documentar
y
evidence
refer
red
to
in
Article
29(3
)
.
States
and
the
conf
ormity
assessment
body
.
In
view
thereof
,
the
Commission
shall
decide
whether
the
author
isation
is
justified
.
The
Commission
shall
address
its
decision
to
the
Member
State
concer
ned
and
to
the
relevant
conf
ormity
assessment
body
.
Article
31
Requirements
relating
to
notif
ied
bodies
necessary
to
fulfil
their
tasks
,
as
well
as
suitable
cybersecurity
requirements
.
ensure
confi
dence
in
their
perf
ormance
,
and
in
the
results
of
the
conf
ormity
assessment
activities
that
the
notif
ied
bodies
conduct
.
.
Notifi
ed
bodies
shall
be
independent
of
the
provid
er
of
a
higher
isk
AI
syste
m
in
relation
to
which
they
perform
conf
ormity
assessment
activities
.
Notifi
ed
bodies
shall
also
be
independent
of
any
other
operat
or
having
an
economic
intere
st
in
higher
isk
AI
systems
assessed
,
as
well
as
of
any
competit
ors
of
the
provider
.
This
shall
not
preclude
the
use
of
assessed
higher
isk
AI
systems
that
are
necessary
for
the
operations
of
the
conf
ormity
assessment
body
,
or
the
use
of
such
higher
isk
AI
systems
for
personal
purposes
.
conf
ormity
assessment
tasks
shall
be
directly
involved
in
the
design
,
development
,
marketing
or
use
of
higher
isk
AI
systems
,
nor
shall
they
represent
the
parties
engag
ed
in
those
activities
.
They
shall
not
enga
ge
in
any
activity
that
might
conf
lict
with
their
independence
of
judgement
or
integrity
in
relation
to
conf
ormity
assessment
activities
for
which
they
are
notified
.
This
shall
,
in
particular
,
apply
to
consultancy
services
.
their
activities
.
Notifi
ed
bodies
shall
document
and
implement
a
structure
and
procedures
to
safeguard
impartiality
and
to
promot
e
and
apply
the
principles
of
imp
artiality
throughout
their
organisation
,
personnel
and
assessment
activities
.
subcontractors
and
any
associated
body
or
personnel
of
exte
rnal
bodies
maintain
,
in
accordance
with
Article
78
,
the
confi
dentiality
of
the
information
which
comes
into
their
possession
during
the
perf
ormance
of
conf
ormity
assessment
activities
,
except
when
its
disclosure
is
required
by
law
.
The
staff
of
notified
bodies
shall
be
bound
to
obser
ve
profe
ssional
secrecy
with
regard
to
all
information
obtained
in
carrying
out
their
tasks
under
this
Regulation
,
excep
t
in
relation
to
the
notifying
authorities
of
the
Member
State
in
which
their
activities
are
carried
out
.
a
provid
er
,
the
sector
in
which
it
operat
es
,
its
structure
,
and
the
degree
of
comp
lexity
of
the
AI
system
concer
ned
.
is
assumed
by
the
Member
State
in
which
they
are
established
in
accordance
with
national
law
or
that
Member
State
is
itself
directly
responsible
for
the
conf
ormity
assessment
.
profe
ssional
integrity
and
the
requisite
comp
etence
in
the
specific
field
,
whether
those
tasks
are
carried
out
by
notif
ied
bodies
themselves
or
on
their
behalf
and
under
their
responsibility
.
external
parties
on
their
behalf
.
The
notif
ied
body
shall
have
permanent
availability
of
sufficient
administrative
,
technical
,
lega
l
and
scientific
personnel
who
possess
exper
ience
and
knowledg
e
relating
to
the
relevant
types
of
AI
systems
,
data
and
data
computing
,
and
relating
to
the
requirements
set
out
in
Section
2
.
directly
,
or
be
represente
d
in
,
European
standardisation
organi
sations
,
or
ensure
that
they
are
aware
and
up
to
date
in
respect
of
relevant
standards
.
Article
32
Presumption
of
confor
mity
with
requirements
relating
to
notif
ied
bodies
Where
a
conf
ormity
assessment
body
demonstrates
its
conf
ormity
with
the
criteria
laid
down
in
the
relevant
harmonised
standards
or
parts
thereof
,
the
references
of
which
have
been
published
in
the
of
the
European
Union
,
it
shall
be
presumed
to
comply
with
the
requirements
set
out
in
Article
31
in
so
far
as
the
applicable
harmonised
standards
cover
those
requirements
.
Subsidiar
ies
of
notif
ied
bodies
and
subcontracting
a
subsidiar
y
,
it
shall
ensure
that
the
subcontractor
or
the
subsidiar
y
meets
the
requirements
laid
down
in
Article
31
,
and
shall
inform
the
notifying
author
ity
according
ly
.
bodies
shall
mak
e
a
list
of
their
subsidiar
ies
publicly
available
.
the
work
carried
out
by
them
under
this
Regulation
shall
be
kept
at
the
disposal
of
the
notifying
author
ity
for
a
period
of
five
years
from
the
termination
date
of
the
subcontracting
.
Article
34
Operational
obligations
of
notif
ied
bodies
procedures
set
out
in
Article
43
.
of
the
size
of
the
provid
er
,
the
secto
r
in
which
it
operat
es
,
its
structure
and
the
degree
of
complexity
of
the
higher
isk
AI
syste
m
concer
ned
,
in
particular
in
view
of
minimising
administrative
burdens
and
compliance
costs
for
micro-
and
small
enterprises
within
the
meaning
of
Recommendation
/EC
.
The
notif
ied
body
shall
,
never
theless
,
respect
the
degree
of
rigour
and
the
level
of
protection
required
for
the
compliance
of
the
higher
isk
AI
system
with
the
requirements
of
this
Regulation
.
documentation
,
to
the
notifying
author
ity
referred
to
in
Article
28
to
allow
that
author
ity
to
conduct
its
assessment
,
designation
,
notification
and
monitoring
activities
,
and
to
facilitate
the
assessment
outlined
in
this
Section
.
Article
35
Identif
ication
numbers
and
lists
of
notif
ied
bodies
more
than
one
Union
act
.
identifica
tion
numbers
and
the
activities
for
which
they
have
been
notif
ied
.
The
Commission
shall
ensure
that
the
list
is
kept
up
to
date
.
Article
36
Changes
to
notif
ications
notif
ication
of
a
notif
ied
body
via
the
electronic
notification
tool
refer
red
to
in
Article
30(2
)
.
For
changes
to
the
notif
ication
other
than
extensi
ons
of
its
scope
,
the
procedures
laid
down
in
paragraphs
to
shall
apply
.
.
Where
a
notif
ied
body
decides
to
cease
its
conf
ormity
assessment
activities
,
it
shall
inform
the
notifying
author
ity
and
the
provid
ers
concer
ned
as
soon
as
possible
and
,
in
the
case
of
a
planned
cessation
,
at
least
one
year
before
ceasing
its
activities
.
The
certificates
of
the
notified
body
may
remain
valid
for
a
period
of
nine
months
after
cessation
of
the
notified
body
’s
activities
,
on
condition
that
another
notified
body
has
confi
rmed
in
writing
that
it
will
assume
responsibilities
for
the
higher
isk
AI
systems
covered
by
those
certificates
.
The
latte
r
notified
body
shall
complet
e
a
full
assessment
of
the
higher
isk
AI
systems
affect
ed
by
the
end
of
that
nine
-
month
-
per
iod
before
issuing
new
certificates
for
those
systems
.
Where
the
notif
ied
body
has
ceased
its
activity
,
the
notifying
author
ity
shall
withdra
w
the
designation
.
laid
down
in
Article
31
,
or
that
it
is
failing
to
fulfil
its
obligations
,
the
notifying
author
ity
shall
without
dela
y
investig
ate
the
matt
er
with
the
utmost
diligence
.
In
that
context
,
it
shall
inform
the
notif
ied
body
concer
ned
about
the
objections
raised
and
give
it
the
possibility
to
mak
e
its
views
known
.
If
the
notifying
author
ity
comes
to
the
conclusion
that
the
notified
body
no
longer
meets
the
requirements
laid
down
in
Article
31
or
that
it
is
failing
to
fulfil
its
obligations
,
it
shall
restr
ict
,
suspend
or
withdra
w
the
designation
as
appropriate
,
depending
on
the
seriousness
of
the
failure
to
meet
those
requirements
or
fulfil
those
obligations
.
It
shall
immediately
inform
the
Commission
and
the
other
Member
States
according
ly
.
the
provid
ers
concer
ned
within
10
days
.
appropriate
steps
to
ensure
that
the
files
of
the
notif
ied
body
concer
ned
are
kept
,
and
to
make
them
available
to
notifying
authorities
in
other
Member
States
and
to
mark
et
surveillance
authorities
at
their
request
.
the
change
s
to
the
designation
;
(
c)require
the
notif
ied
body
to
suspend
or
withdra
w
,
within
a
reasonable
period
of
time
determined
by
the
author
ity
,
any
certificates
which
were
unduly
issued
,
in
order
to
ensure
the
continuing
conf
ormity
of
higher
isk
AI
systems
on
the
market
;
(
d)inform
the
Commission
and
the
Member
States
about
certificates
the
suspension
or
withdra
wal
of
which
it
has
required
;
(
e)provid
e
the
national
competent
authorities
of
the
Member
State
in
which
the
provider
has
its
regist
ered
place
of
business
with
all
relevant
information
about
the
certificates
of
which
it
has
required
the
suspension
or
withdra
wal
;
that
author
ity
shall
take
the
appropriate
measures
,
where
necessary
,
to
avoid
a
pote
ntial
risk
to
health
,
safety
or
fundamental
rights
.
certificates
shall
remain
valid
in
one
of
the
following
circumstances
:
(
a)the
notifying
author
ity
has
conf
irmed
,
within
one
month
of
the
suspension
or
restr
iction
,
that
there
is
no
risk
to
health
,
safety
or
fundamental
rights
in
relation
to
certificates
affect
ed
by
the
suspension
or
restr
iction
,
and
the
notifying
author
ity
has
outlined
a
timeline
for
actions
to
remedy
the
suspension
or
restr
iction
;
or
(
b)the
notifying
author
ity
has
confir
med
that
no
certificates
relevant
to
the
suspension
will
be
issued
,
amended
or
re
-
issued
during
the
course
of
the
suspension
or
restr
iction
,
and
states
whether
the
notif
ied
body
has
the
capability
of
continuing
to
monitor
and
remain
responsible
for
existing
certificates
issued
for
the
period
of
the
suspension
or
restr
iction
;
in
the
event
that
the
notifying
author
ity
determ
ines
that
the
notif
ied
body
does
not
have
the
capability
to
support
existing
certificates
issued
,
the
provider
of
the
system
cover
ed
by
the
certificate
shall
conf
irm
in
writing
to
the
national
competent
authorities
of
the
Member
State
in
which
it
has
its
registered
place
of
business
,
within
three
months
of
the
suspension
or
restr
iction
,
that
another
qualif
ied
notif
ied
body
is
temporar
ily
assuming
the
functions
of
the
notif
ied
body
to
monitor
and
remain
responsible
for
the
certificates
during
the
period
of
suspension
or
restr
iction
.
With
the
excep
tion
of
certificates
unduly
issued
,
and
where
a
designation
has
been
withdra
wn
,
the
certificates
shall
remain
valid
for
a
period
of
nine
months
under
the
follo
wing
circumstances
:
(
a)the
national
competent
author
ity
of
the
Member
State
in
which
the
provider
of
the
higher
isk
AI
syste
m
cover
ed
by
the
certificate
has
its
registered
place
of
business
has
confi
rmed
that
there
is
no
risk
to
health
,
safety
or
fundamental
rights
associate
d
with
the
higher
isk
AI
systems
concer
ned
;
and
(
b)another
notified
body
has
confir
med
in
writing
that
it
will
assume
immediate
responsibility
for
those
AI
systems
and
complet
es
its
assessment
within
12
months
of
the
withdra
wal
of
the
designation
.
In
the
circumstances
referred
to
in
the
first
subparagraph
,
the
national
comp
etent
author
ity
of
the
Member
State
in
which
the
provid
er
of
the
syste
m
covered
by
the
certificate
has
its
place
of
business
may
exte
nd
the
provisional
validity
of
the
certificates
for
additional
periods
of
three
months
,
which
shall
not
exceed
12
months
in
total
.
The
national
competent
author
ity
or
the
notif
ied
body
assuming
the
functions
of
the
notified
body
affect
ed
by
the
chang
e
of
designation
shall
immediate
ly
inform
the
Commission
,
the
other
Member
States
and
the
other
notif
ied
bodies
thereof
.
Article
37
Challenge
to
the
competence
of
notif
ied
bodies
a
notified
body
or
the
continued
fulfilment
by
a
notif
ied
body
of
the
requirements
laid
down
in
Article
31
and
of
its
applicable
responsibilities
.
notif
ication
or
the
maintenance
of
the
comp
etence
of
the
notified
body
concer
ned
.
Article
is
treate
d
confidentially
in
accordance
with
Article
78
.
notif
ication
,
it
shall
inform
the
notifying
Member
State
according
ly
and
request
it
to
take
the
necessary
corrective
measures
,
including
the
suspension
or
withdra
wal
of
the
notif
ication
if
necessary
.
Where
the
Member
State
fails
to
take
the
necessary
corrective
measures
,
the
Commission
may
,
by
means
of
an
imp
lementing
act
,
suspend
,
restr
ict
or
withdraw
the
designation
.
That
implementing
act
shall
be
adop
ted
in
accordance
with
the
examination
procedure
refer
red
to
in
Article
98(2
)
.
Article
38
Coordination
of
notif
ied
bodies
between
notified
bodies
active
in
the
conf
ormity
assessment
procedures
pursuant
to
this
Regulation
are
put
in
place
and
properly
operat
ed
in
the
form
of
a
sectoral
group
of
notified
bodies
.
paragraph
1
,
directly
or
through
designated
representatives
.
Conf
ormity
assessment
bodies
of
third
countr
ies
Conf
ormity
assessment
bodies
established
under
the
law
of
a
third
countr
y
with
which
the
Union
has
concluded
an
agreement
may
be
authorised
to
carry
out
the
activities
of
notified
bodies
under
this
Regulation
,
provided
that
they
meet
the
requirements
laid
down
in
Article
31
or
they
ensure
an
equivalent
level
of
compliance
.
SECTION
5
Standar
ds
,
conf
ormity
assessment
,
certificates
,
registr
ation
Article
40
Har
monised
standards
and
standardisation
deliv
erables
thereof
the
references
of
which
have
been
published
in
the
of
the
European
Union
in
accordance
with
Regulation
(
EU
)
No
shall
be
presumed
to
be
in
conf
ormity
with
the
requirements
set
out
in
Section
2
of
this
Chapt
er
or
,
as
applicable
,
with
the
obligations
set
out
in
of
Chapt
er
V
,
Sections
2
and
3
,
of
this
Regulation
,
to
the
extent
that
those
standards
cover
those
requirements
or
obligations
.
standardisation
requests
coveri
ng
all
requirements
set
out
in
Section
2
of
this
Chap
ter
and
,
as
applicable
,
standardisation
requests
coverin
g
obligations
set
out
in
Chapt
er
V
,
Sections
2
and
3
,
of
this
Regulation
.
The
standardisation
request
shall
also
ask
for
deliverables
on
repor
ting
and
documentation
processes
to
impro
ve
AI
systems
’
resource
perf
ormance
,
such
as
reducing
the
higher
isk
AI
system
’s
consump
tion
of
energy
and
of
other
resources
during
its
lifecy
cle
,
and
on
the
energy
-
eff
icient
development
of
general
-
pur
pose
AI
models
.
When
prepar
ing
a
standardisation
request
,
the
Commission
shall
consult
the
Board
and
relevant
stakeh
olders
,
including
the
advisor
y
forum
.
When
issuing
a
standardisation
request
to
European
standardisation
organisations
,
the
Commission
shall
specify
that
standards
have
to
be
clear
,
consiste
nt
,
including
with
the
standards
developed
in
the
various
sectors
for
products
covered
by
the
existing
Union
harmonisation
legislation
listed
in
Annex
I
,
and
aiming
to
ensure
that
higher
isk
AI
systems
or
general
-
pur
pose
AI
models
placed
on
the
market
or
put
into
service
in
the
Union
meet
the
relevant
requirements
or
obligations
laid
down
in
this
Regulation
.
The
Commission
shall
request
the
European
standardisation
organisations
to
provide
evidence
of
their
best
efforts
to
fulfil
the
objectives
refer
red
to
in
the
first
and
the
second
subparagraph
of
this
paragraph
in
accordance
with
Article
24
of
Regulation
(
EU
)
No
.
through
increasing
legal
certainty
,
as
well
as
the
competitiveness
and
growth
of
the
Union
market
,
to
contribut
e
to
strengthening
global
cooperation
on
standardisation
and
taking
into
account
existing
intern
ational
standards
in
the
field
of
AI
that
are
consiste
nt
with
Union
values
,
fundamental
rights
and
interests
,
and
to
enhance
multi
-
stake
holder
gove
rnance
ensur
ing
a
balanced
representation
of
interests
and
the
effective
participation
of
all
relevant
stak
eholders
in
accordance
with
Articles
5
,
6
,
and
7
of
Regulation
(
EU
)
No
.
Article
41
Common
specifications
Section
2
of
this
Chapt
er
or
,
as
applicable
,
for
the
obligations
set
out
in
Sections
2
and
3
of
Chapt
er
V
where
the
following
conditions
have
been
fulfilled
:
(
a)the
Commission
has
requested
,
pursuant
to
Article
10(1
)
of
Regulation
(
EU
)
No
,
one
or
more
European
standardisation
organisations
to
draft
a
harmonised
standard
for
the
requirements
set
out
in
Section
2
of
this
Chapt
er
,
or
,
as
applicable
,
for
the
obligations
set
out
in
Sections
2
and
3
of
Chap
ter
V
,
and
:
(
i)the
request
has
not
been
accept
ed
by
any
of
the
European
standardisation
organisations
;
or
harmonised
standards
addressing
that
request
are
not
delivered
within
the
deadline
set
in
accordance
with
Article
10(1
)
of
Regulation
(
EU
)
No
;
or
(
iii
)
the
relevant
harmonised
standards
insuff
iciently
address
fundamental
rights
concer
ns
;
or
(
iv
)
the
harmonised
standards
do
not
com
ply
with
the
request
;
and
(
b)no
refere
nce
to
harmonised
standards
covering
the
requirements
refer
red
to
in
Section
2
of
this
Chapt
er
or
,
as
applicable
,
the
oblig
ations
referred
to
in
Sections
2
and
3
of
Chap
ter
V
has
been
published
in
the
of
the
Europe
an
Union
in
accordance
with
Regulation
(
EU
)
No
,
and
no
such
reference
is
expect
ed
to
be
published
within
a
reasonable
period
.
When
drafting
the
common
specifications
,
the
Commission
shall
consult
the
advisor
y
forum
refer
red
to
in
Article
67
.
The
implementing
acts
referred
to
in
the
first
subparagraph
of
this
paragraph
shall
be
adop
ted
in
accordance
with
the
examination
procedure
referred
to
in
Article
98(2
)
.
Regulation
(
EU
)
No
that
it
considers
the
conditions
laid
down
in
paragraph
1
of
this
Article
to
be
fulfilled
.
to
in
paragraph
1
,
or
parts
of
those
specifications
,
shall
be
presumed
to
be
in
conf
ormity
with
the
requirements
set
out
in
Section
2
of
this
Chapt
er
or
,
as
applicable
,
to
comp
ly
with
the
oblig
ations
refer
red
to
in
Sections
2
and
3
of
Chapt
er
V
,
to
the
extent
those
common
specifications
cover
those
requirements
or
those
obligations
.
Commission
for
the
publication
of
its
reference
in
the
of
the
European
Union
,
the
Commission
shall
assess
the
harmonised
standard
in
accordance
with
Regulation
(
EU
)
No
.
When
reference
to
a
harmonised
standard
is
published
in
the
of
the
European
Union
,
the
Commission
shall
repeal
the
imp
lementing
acts
refer
red
to
in
paragraph
1
,
or
parts
thereof
which
cover
the
same
requirements
set
out
in
Section
2
of
this
Chap
ter
or
,
as
applicable
,
the
same
obligations
set
out
in
Sections
2
and
3
of
Chap
ter
V.
specific
ations
referred
to
in
paragraph
1
,
they
shall
duly
justify
that
they
have
adopt
ed
technical
solutions
that
meet
the
requirements
refer
red
to
in
Section
2
of
this
Chapt
er
or
,
as
applicable
,
comply
with
the
obligations
set
out
in
Sections
2
and
Section
2
or
,
as
applicable
,
com
ply
with
obligations
set
out
in
Sections
2
and
3
of
Chapt
er
V
,
it
shall
inform
the
Commission
thereof
with
a
detailed
explanation
.
The
Commission
shall
assess
that
information
and
,
if
appropriate
,
amend
the
implementing
act
establishing
the
common
specification
concer
ned
.
Article
42
Presumption
of
confor
mity
with
certain
requirements
cont
extual
or
functional
setting
within
which
they
are
intended
to
be
used
shall
be
presumed
to
comply
with
the
relevant
requirements
laid
down
in
Article
10(4
)
.
a
cybersecurity
scheme
pursuant
to
Regulation
(
EU
)
and
the
references
of
which
have
been
published
in
the
of
the
European
Union
shall
be
presumed
to
comply
with
the
cybersecurity
requirements
set
out
in
Article
15
of
this
Regulation
in
so
far
as
the
cybersecurity
certificate
or
statem
ent
of
conf
ormity
or
parts
thereof
cover
those
requirements
.
Conformit
y
assessment
syste
m
with
the
requirements
set
out
in
Section
2
,
the
provid
er
has
applied
harmonised
standards
refer
red
to
in
Article
40
,
or
,
where
applicable
,
common
specifications
refer
red
to
in
Article
41
,
the
provider
shall
opt
for
one
of
the
following
conf
ormity
assessment
procedures
based
on
:
(
a)the
internal
control
refer
red
to
in
Annex
VI
;
or
(
b)the
assessment
of
the
quality
management
syste
m
and
the
assessment
of
the
technical
documentation
,
with
the
involvement
of
a
notified
body
,
referred
to
in
Annex
VII
.
In
demonstrating
the
compliance
of
a
higher
isk
AI
system
with
the
requirements
set
out
in
Section
2
,
the
provid
er
shall
follo
w
the
conf
ormity
assessment
procedure
set
out
in
Annex
VII
where
:
(
a)harmonised
standards
refer
red
to
in
Article
40
do
not
exist
,
and
common
specifications
referred
to
in
Article
41
are
not
available
;
(
b)the
provid
er
has
not
applied
,
or
has
applied
only
part
of
,
the
harmonised
standard
;
(
c)the
common
specifications
refer
red
to
in
point
(
a
)
exist
,
but
the
provider
has
not
applied
them
;
(
d)one
or
more
of
the
harmonised
standards
referred
to
in
point
(
a
)
has
been
published
with
a
restr
iction
,
and
only
on
the
part
of
the
standard
that
was
restr
icted
.
For
the
purposes
of
the
conf
ormity
assessment
procedure
refer
red
to
in
Annex
VII
,
the
provider
may
choose
any
of
the
notif
ied
bodies
.
How
ever
,
where
the
higher
isk
AI
syste
m
is
intended
to
be
put
into
service
by
law
enforcement
,
immigration
or
asylum
authorities
or
by
Union
institutions
,
bodies
,
offices
or
agencies
,
the
mark
et
surveillance
author
ity
refer
red
to
in
Article
74(8
)
or
,
as
applicable
,
shall
act
as
a
notified
body
.
procedure
based
on
inter
nal
control
as
referred
to
in
Annex
VI
,
which
does
not
provide
for
the
involvement
of
a
notif
ied
body
.
shall
follo
w
the
relevant
conf
ormity
assessment
procedure
as
required
under
those
legal
acts
.
The
requirements
set
out
in
Section
2
of
this
Chapt
er
shall
apply
to
those
higher
isk
AI
systems
and
shall
be
part
of
that
assessment
.
Points
4.3
.
,
4.4
.
,
4.5
.
and
the
fifth
paragraph
of
point
4.6
of
Annex
VII
shall
also
apply
.
For
the
purposes
of
that
assessment
,
notif
ied
bodies
which
have
been
notif
ied
under
those
legal
acts
shall
be
entitled
to
control
the
conf
ormity
of
the
higher
isk
AI
systems
with
the
requirements
set
out
in
Section
2
,
provided
that
the
compliance
of
those
notif
ied
bodies
with
requirements
laid
down
in
Article
31(4
)
,
,
and
has
been
assessed
in
the
context
of
the
notification
procedure
under
those
lega
l
acts
.
Where
a
legal
act
listed
in
Section
A
of
Annex
I
enables
the
product
manufa
cturer
to
opt
out
from
a
third
-
par
ty
conf
ormity
assessment
,
provided
that
that
manufacture
r
has
applied
all
harmonised
standards
coveri
ng
all
the
relevant
requirements
,
that
manufa
cturer
may
use
that
option
only
if
it
has
also
applied
harmonised
standards
or
,
where
applicable
,
common
specific
ations
referred
to
in
Article
41
,
covering
all
requirements
set
out
in
Section
2
of
this
Chapt
er
.
conf
ormity
assessment
procedure
in
the
event
of
a
substantial
modification
,
regardless
of
whether
the
modified
syste
m
is
intende
d
to
be
further
distr
ibut
ed
or
continues
to
be
used
by
the
current
deplo
yer
.
For
higher
isk
AI
systems
that
continue
to
learn
after
being
placed
on
the
market
or
put
into
service
,
chang
es
to
the
higher
isk
AI
syste
m
and
its
perfo
rmance
that
have
been
pre
-
dete
rmined
by
the
provid
er
at
the
moment
of
the
initial
conf
ormity
assessment
and
are
part
of
the
information
contained
in
the
technical
documentation
refer
red
to
in
point
2(f
)
of
Annex
IV
,
shall
not
constitute
a
substantial
modifi
cation
.
and
VII
by
updating
them
in
light
of
technical
progress
.
The
Commission
is
empo
wered
to
adop
t
deleg
ated
acts
in
accordance
with
Article
97
in
order
to
amend
paragraphs
1
and
2
of
this
Article
in
order
to
subject
higher
isk
AI
systems
referred
to
in
points
2
to
8
of
Annex
III
to
the
conf
ormity
assessment
procedure
referred
to
in
Annex
VII
or
parts
thereof
.
The
Commission
shall
adopt
such
delegat
ed
acts
taking
into
account
the
effectiveness
of
the
conf
ormity
assessment
procedure
based
on
internal
control
referred
to
in
Annex
VI
in
preventing
or
minimising
the
risks
to
health
and
safety
and
protection
of
fundamental
rights
posed
by
such
systems
,
as
well
as
the
availability
of
adequat
e
capacities
and
resources
among
notified
bodies
.
Article
44
Cer
tificates
easily
understood
by
the
relevant
authorities
in
the
Member
State
in
which
the
notified
body
is
established
.
Annex
I
,
and
four
years
for
AI
systems
covered
by
Annex
III
.
At
the
request
of
the
provider
,
the
validity
of
a
certificate
may
be
exte
nded
for
further
periods
,
each
not
exceeding
five
years
for
AI
systems
covered
by
Annex
I
,
and
four
years
for
AI
systems
cover
ed
by
Annex
III
,
based
on
a
re
-
assessment
in
accordance
with
the
applicable
conf
ormity
assessment
procedures
.
Any
supplement
to
a
certificate
shall
remain
valid
,
provided
that
the
certificate
which
it
supplements
is
valid
.
account
of
the
principle
of
propor
tionality
,
suspend
or
withdra
w
the
certificate
issued
or
impose
restrictions
on
it
,
unless
complia
nce
with
those
requirements
is
ensured
by
appropriate
corrective
action
take
n
by
the
provider
of
the
system
within
an
appropriate
deadline
set
by
the
notif
ied
body
.
The
notif
ied
body
shall
give
reasons
for
its
decision
.
An
appeal
procedure
against
decisions
of
the
notified
bodies
,
including
on
conf
ormity
certificates
issued
,
shall
be
available
.
Article
45
Infor
mation
obligations
of
notif
ied
bodies
managem
ent
syste
m
approva
ls
issued
in
accordance
with
the
requirements
of
Annex
VII
;
(
b)any
refusal
,
restr
iction
,
suspension
or
withdra
wal
of
a
Union
technical
documentation
assessment
certificate
or
a
quality
managem
ent
syste
m
approva
l
issued
in
accordance
with
the
requirements
of
Annex
VII
;
(
c)any
circumstances
affecting
the
scope
of
or
conditions
for
notif
ication
;
(
d)any
request
for
information
which
they
have
received
from
market
surveillance
authorities
regarding
conf
ormity
assessment
activities
;
(
e)on
request
,
conf
ormity
assessment
activities
perf
ormed
within
the
scope
of
their
notif
ication
and
any
other
activity
perfo
rmed
,
including
cross
-
border
activities
and
subcontracting
.
syste
m
approva
ls
which
it
has
issued
;
(
b)Union
technical
documentation
assessment
certificates
or
any
supplements
thereto
which
it
has
refused
,
withdra
wn
,
suspended
or
other
wise
restr
icted
,
and
,
upon
request
,
of
the
certificates
and/or
supplements
thereto
which
it
has
issued
.
.
Each
notif
ied
body
shall
provide
the
other
notified
bodies
carrying
out
similar
conf
ormity
assessment
activities
covering
the
same
types
of
AI
systems
with
relevant
information
on
issues
relating
to
negative
and
,
on
request
,
positive
conf
ormity
assessment
results
.
Article
46
Derogation
from
confor
mity
assessment
procedure
author
ise
the
placing
on
the
market
or
the
putting
into
service
of
specific
higher
isk
AI
systems
within
the
territory
of
the
Member
State
concer
ned
,
for
exceptiona
l
reasons
of
public
security
or
the
protection
of
life
and
health
of
persons
,
environmental
protection
or
the
protection
of
key
industr
ial
and
infrastr
uctural
assets
.
That
author
isation
shall
be
for
a
limited
period
while
the
necessary
conf
ormity
assessment
procedures
are
being
carried
out
,
taking
into
account
the
exceptional
reasons
justifying
the
deroga
tion
.
The
completion
of
those
procedures
shall
be
under
taken
without
undue
dela
y.
and
imminent
threat
to
the
life
or
physica
l
safety
of
natural
persons
,
law
-
enf
orcement
authorities
or
civil
protection
authorities
may
put
a
specific
higher
isk
AI
system
into
service
without
the
author
isation
referred
to
in
paragraph
1
,
provid
ed
that
such
author
isation
is
request
ed
during
or
after
the
use
without
undue
dela
y.
If
the
author
isation
referred
to
in
paragraph
1
is
refused
,
the
use
of
the
higher
isk
AI
system
shall
be
stopp
ed
with
immediate
effect
and
all
the
results
and
outputs
of
such
use
shall
be
immediately
discarded
.
the
higher
isk
AI
syste
m
comp
lies
with
the
requirements
of
Section
2
.
The
market
surveillance
author
ity
shall
inform
the
Commission
and
the
other
Member
States
of
any
author
isation
issued
pursuant
to
paragraphs
1
and
2
.
This
obligation
shall
not
cover
sensitive
operational
data
in
relation
to
the
activities
of
law
-
enf
orcement
authorities
.
by
either
a
Member
State
or
the
Commission
in
respect
of
an
author
isation
issued
by
a
market
surveillance
author
ity
of
a
Member
State
in
accordance
with
paragraph
1
,
that
author
isation
shall
be
deemed
justified
.
a
Member
State
against
an
author
isation
issued
by
a
mark
et
surveillance
author
ity
of
another
Member
State
,
or
where
the
Commission
considers
the
author
isation
to
be
contrar
y
to
Union
law
,
or
the
conclusion
of
the
Member
States
rega
rding
the
complia
nce
of
the
syste
m
as
referred
to
in
paragraph
3
to
be
unfounded
,
the
Commission
shall
,
without
dela
y
,
enter
into
consultations
with
the
relevant
Member
State
.
The
operators
concer
ned
shall
be
consulted
and
have
the
possibility
to
present
their
views
.
Having
regard
thereto
,
the
Commission
shall
decide
whether
the
author
isation
is
justified
.
The
Commission
shall
address
its
decision
to
the
Member
State
concer
ned
and
to
the
relevant
operators
.
author
ity
of
the
Member
State
concer
ned
.
Annex
I
,
only
the
derogat
ions
from
the
conf
ormity
assessment
established
in
that
Union
harmonisation
legislation
shall
apply
.
Article
47
EU
declaration
of
confor
mity
for
each
higher
isk
AI
system
,
and
keep
it
at
the
disposal
of
the
national
comp
etent
authorities
for
10
years
after
the
higher
isk
AI
syste
m
has
been
placed
on
the
mark
et
or
put
into
service
.
The
EU
declaration
of
conf
ormity
shall
identify
the
higher
isk
AI
system
for
which
it
has
been
drawn
up
.
A
copy
of
the
EU
declaration
of
conf
ormity
shall
be
submitt
ed
to
the
relevant
national
comp
etent
authorities
upon
request
.
The
EU
declaration
of
conf
ormity
shall
state
that
the
higher
isk
AI
system
concer
ned
meets
the
requirements
set
out
in
Section
2
.
The
EU
declaration
of
conf
ormity
shall
contain
the
information
set
out
in
Annex
V
,
and
shall
be
translated
into
a
language
that
can
be
easily
understoo
d
by
the
national
competent
authorities
of
the
Member
States
in
which
the
higher
isk
AI
system
is
placed
on
the
mark
et
or
made
available
.
declaration
of
conf
ormity
,
a
sing
le
EU
declaration
of
conf
ormity
shall
be
drawn
up
in
respect
of
all
Union
law
applicable
to
the
higher
isk
AI
system
.
The
declaration
shall
contain
all
the
information
required
to
identify
the
Union
harmonisation
legislation
to
which
the
declaration
relates
.
requirements
set
out
in
Section
2
.
The
provider
shall
keep
the
EU
declaration
of
conf
ormity
up
-
to
-
dat
e
as
appropriate
.
updating
the
cont
ent
of
the
EU
declaration
of
conf
ormity
set
out
in
that
Annex
,
in
order
to
introduce
elements
that
become
necessary
in
light
of
technical
progress
.
Article
48
CE
marking
interfac
e
from
which
that
system
is
accessed
or
via
an
easily
accessible
machi
ne
-
readable
code
or
other
electronic
means
.
not
warrant
ed
on
account
of
the
nature
of
the
higher
isk
AI
syste
m
,
it
shall
be
affixed
to
the
packag
ing
or
to
the
accom
panying
documentation
,
as
appropriate
.
the
conf
ormity
assessment
procedures
set
out
in
Article
43
.
The
identifica
tion
number
of
the
notif
ied
body
shall
be
affixed
by
the
body
itself
or
,
under
its
instr
uctions
,
by
the
provid
er
or
by
the
provid
er
’s
authorised
representative
.
The
identification
number
shall
also
be
indicated
in
any
promotional
material
which
mentions
that
the
higher
isk
AI
system
fulfils
the
requirements
for
CE
marking
.
CE
marking
shall
indicate
that
the
higher
isk
AI
system
also
fulfil
the
requirements
of
that
other
law
.
Article
49
Regis
tration
higher
isk
AI
systems
referred
to
in
point
2
of
Annex
III
,
the
provid
er
or
,
where
applicable
,
the
authorised
representative
shall
regist
er
themselves
and
their
syste
m
in
the
EU
database
referred
to
in
Article
71
.
higher
isk
according
to
Article
6(3
)
,
that
provider
or
,
where
applicable
,
the
authorised
representative
shall
regist
er
themselves
and
that
system
in
the
EU
database
refer
red
to
in
Article
71
.
systems
listed
in
point
2
of
Annex
III
,
deplo
yers
that
are
public
authorities
,
Union
institutions
,
bodies
,
offices
or
agencies
or
persons
acting
on
their
behalf
shall
regist
er
themselves
,
select
the
syste
m
and
register
its
use
in
the
EU
database
referred
to
in
Article
71
.
.
For
higher
isk
AI
systems
refer
red
to
in
points
1
,
6
and
7
of
Annex
III
,
in
the
areas
of
law
enforcement
,
migration
,
asylum
and
border
control
management
,
the
registration
referred
to
in
paragraphs
1
,
2
and
3
of
this
Article
shall
be
in
a
secure
non
-
public
section
of
the
EU
database
refer
red
to
in
Article
71
and
shall
include
only
the
follo
wing
information
,
as
applicable
,
referred
to
in
:
(
a)Section
A
,
points
1
to
10
,
of
Annex
VIII
,
with
the
excep
tion
of
points
6
,
8
and
9
;
(
b)Section
B
,
points
1
to
5
,
and
points
8
and
9
of
Annex
VIII
;
(
c)Section
C
,
points
1
to
3
,
of
Annex
VIII
;
(
d)points
1
,
2
,
3
and
5
,
of
Annex
IX
.
Only
the
Commission
and
national
authorities
referred
to
in
Article
74(8
)
shall
have
access
to
the
respective
restr
icted
sections
of
the
EU
database
listed
in
the
first
subparagraph
of
this
paragraph
.
CHAPTER
IV
TRANSP
ARENCY
obligations
FOR
PROVIDERS
AND
DEPLO
YERS
OF
CERT
AIN
AI
SYSTEMS
Article
50
Transparency
obligations
for
providers
and
deplo
yers
of
certain
AI
systems
such
a
way
that
the
natural
persons
concer
ned
are
informed
that
they
are
interacting
with
an
AI
system
,
unless
this
is
obvious
from
the
point
of
view
of
a
natural
person
who
is
reasonably
well
-
inf
ormed
,
obser
vant
and
circumspect
,
taking
into
account
the
circumstances
and
the
cont
ext
of
use
.
This
obligation
shall
not
apply
to
AI
systems
authorised
by
law
to
detect
,
prevent
,
invest
igate
or
prosecute
criminal
offences
,
subject
to
appropriate
safeguards
for
the
rights
and
freedoms
of
third
parties
,
unless
those
systems
are
available
for
the
public
to
repor
t
a
criminal
offence
.
cont
ent
,
shall
ensure
that
the
outputs
of
the
AI
system
are
marke
d
in
a
machine
-
readable
format
and
detectable
as
artificially
generat
ed
or
manipulate
d.
Providers
shall
ensure
their
technical
solutions
are
effective
,
interoperable
,
robust
and
reliable
as
far
as
this
is
technically
feasible
,
taking
into
account
the
specificities
and
limitations
of
various
types
of
cont
ent
,
the
costs
of
imp
lementation
and
the
generally
ackno
wledged
state
of
the
art
,
as
may
be
reflected
in
relevant
technical
standards
.
This
obligation
shall
not
apply
to
the
extent
the
AI
systems
perfo
rm
an
assistive
function
for
standard
editing
or
do
not
substantially
alter
the
input
data
provid
ed
by
the
deplo
yer
or
the
semantics
thereof
,
or
where
authorised
by
law
to
detect
,
prevent
,
investig
ate
or
prosecute
criminal
offences
.
exposed
thereto
of
the
operation
of
the
syste
m
,
and
shall
process
the
personal
data
in
accordance
with
Regulations
(
EU
)
and
(
EU
)
and
Directive
(
EU
)
,
as
applicable
.
This
obligation
shall
not
apply
to
AI
systems
used
for
biometric
categor
isation
and
emotion
recognition
,
which
are
permitted
by
law
to
detect
,
prevent
or
invest
igate
criminal
offences
,
subject
to
appropriate
safeguards
for
the
rights
and
freedoms
of
third
parties
,
and
in
accordance
with
Union
law
.
disclose
that
the
content
has
been
artificially
generated
or
manipulated
.
This
obligation
shall
not
apply
where
the
use
is
authorised
by
law
to
detect
,
prevent
,
invest
igate
or
prosecute
criminal
offence
.
Where
the
cont
ent
forms
part
of
an
evidently
artistic
,
creative
,
satir
ical
,
fictional
or
analogous
work
or
programme
,
the
transparency
obligations
set
out
in
this
paragraph
are
limit
ed
to
disclosure
of
the
existe
nce
of
such
generated
or
manipulate
d
cont
ent
in
an
appropriate
manner
that
does
not
ham
per
the
displa
y
or
enjo
yment
of
the
work
.
Deplo
yers
of
an
AI
system
that
generat
es
or
manipulate
s
text
which
is
published
with
the
purpose
of
informing
the
public
on
matt
ers
of
public
intere
st
shall
disclose
that
the
text
has
been
artificially
generated
or
manipulate
d.
This
obligation
shall
not
apply
where
the
use
is
authorised
by
law
to
detect
,
prevent
,
investig
ate
or
prosecute
criminal
offences
or
where
the
AI
-
g
enerated
cont
ent
has
undergone
a
process
of
human
review
or
edito
rial
control
and
where
a
natural
or
legal
person
holds
editor
ial
responsibility
for
the
publication
of
the
content
.
The
information
referred
to
in
paragraphs
1
to
4
shall
be
provided
to
the
natural
persons
concer
ned
in
a
clear
and
distinguishable
manner
at
the
latest
at
the
time
of
the
first
interaction
or
exposure
.
The
information
shall
conf
orm
to
the
applicable
accessibility
requirements
.
prejudice
to
other
transparency
oblig
ations
laid
down
in
Union
or
national
law
for
deplo
yers
of
AI
systems
.
imp
lementation
of
the
obligations
regard
ing
the
detection
and
labelling
of
artificially
generated
or
manipulat
ed
cont
ent
.
The
Commission
may
adop
t
imp
lementing
acts
to
approve
those
codes
of
practice
in
accordance
with
the
procedure
laid
down
in
Article
56
.
If
it
deems
the
code
is
not
adequat
e
,
the
Commission
may
adopt
an
imp
lementing
act
specifying
common
rules
for
the
implementation
of
those
obligations
in
accordance
with
the
examination
procedure
laid
down
in
Article
98(2
)
.
CHAPTER
V
GENERAL
-PURPOSE
AI
MODELS
SECTION
1
Classification
rules
Article
51
Classif
ication
of
general
-
pur
pose
AI
models
as
general
-
pur
pose
AI
models
with
systemic
risk
follo
wing
conditions
:
(
a)it
has
high
imp
act
capabilities
evaluated
on
the
basis
of
appropriate
technical
tools
and
methodologies
,
including
indicator
s
and
benchmarks
;
(
b)based
on
a
decision
of
the
Commission
,
ex
officio
or
followi
ng
a
qualif
ied
alert
from
the
scientific
panel
,
it
has
capabilities
or
an
imp
act
equivalent
to
those
set
out
in
point
(
a
)
having
rega
rd
to
the
criteria
set
out
in
Annex
XIII
.
when
the
cumulative
amount
of
computation
used
for
its
training
measured
in
floating
point
operations
is
great
er
than
paragraphs
1
and
2
of
this
Article
,
as
well
as
to
supplement
benchmarks
and
indicator
s
in
light
of
evolving
technological
developments
,
such
as
algor
ithmic
improvements
or
increased
hardware
efficiency
,
when
necessary
,
for
these
thresholds
to
reflect
the
state
of
the
art
.
Article
52
Procedure
shall
notify
the
Commission
without
dela
y
and
in
any
event
within
two
week
s
after
that
requirement
is
met
or
it
becomes
kno
wn
that
it
will
be
met
.
That
notif
ication
shall
include
the
information
necessary
to
demonstrate
that
the
relevant
requirement
has
been
met
.
If
the
Commission
becomes
aware
of
a
general
-
pur
pose
AI
model
presenting
syste
mic
risks
of
which
it
has
not
been
notified
,
it
may
decide
to
designate
it
as
a
model
with
syste
mic
risk
.
present
,
with
its
notif
ication
,
sufficiently
substantiate
d
arguments
to
demonstrate
that
,
exceptiona
lly
,
although
it
meets
that
requirement
,
the
general
-
pur
pose
AI
model
does
not
present
,
due
to
its
specific
characteristics
,
syste
mic
risks
and
theref
ore
should
not
be
classif
ied
as
a
general
-
pur
pose
AI
model
with
systemic
risk
.
.
Where
the
Commission
concludes
that
the
arguments
submitted
pursuant
to
paragraph
2
are
not
suffi
ciently
substantiate
d
and
the
relevant
provid
er
was
not
able
to
demonstrate
that
the
general
-
purpo
se
AI
model
does
not
present
,
due
to
its
specific
character
istics
,
syste
mic
risks
,
it
shall
reject
those
arguments
,
and
the
general
-
pur
pose
AI
model
shall
be
considered
to
be
a
general
-
pur
pose
AI
model
with
syste
mic
risk
.
a
qualif
ied
alert
from
the
scientific
panel
pursuant
to
Article
90(1
)
,
point
(
a
)
,
on
the
basis
of
criteria
set
out
in
Annex
XIII
.
The
Commission
is
emp
owered
to
adop
t
delegat
ed
acts
in
accordance
with
Article
97
in
order
to
amend
Annex
XIII
by
specifying
and
updating
the
criteria
set
out
in
that
Annex
.
risk
pursuant
to
paragraph
4
,
the
Commission
shall
take
the
request
into
account
and
may
decide
to
reassess
whether
the
general
-
pur
pose
AI
model
can
still
be
considered
to
present
systemic
risks
on
the
basis
of
the
criteria
set
out
in
Annex
XIII
.
Such
a
request
shall
contain
objective
,
detailed
and
new
reasons
that
have
arisen
since
the
designation
decision
.
Provi
ders
may
request
reassessment
at
the
earliest
six
months
after
the
designation
decision
.
Where
the
Commission
,
following
its
reassessment
,
decides
to
maintain
the
designation
as
a
general
-
pur
pose
AI
model
with
syste
mic
risk
,
providers
may
request
reassessment
at
the
earliest
six
months
after
that
decision
.
that
list
up
to
date
,
without
prejudice
to
the
need
to
obser
ve
and
prot
ect
intellectual
proper
ty
rights
and
confi
dential
business
information
or
trade
secrets
in
accordance
with
Union
and
national
law
.
SECTION
2
Oblig
ations
for
providers
of
gener
al
-
pur
pose
AI
models
Article
53
Obligations
for
providers
of
general
-
pur
pose
AI
models
the
results
of
its
evaluation
,
which
shall
contain
,
at
a
minimum
,
the
information
set
out
in
Annex
XI
for
the
purpose
of
provid
ing
it
,
upon
request
,
to
the
AI
Office
and
the
national
competent
authorities
;
(
b)draw
up
,
keep
up
-
to
-
da
te
and
mak
e
available
information
and
documentation
to
provid
ers
of
AI
systems
who
intend
to
integrat
e
the
general
-
pur
pose
AI
model
into
their
AI
systems
.
Without
prejudice
to
the
need
to
obser
ve
and
prot
ect
intellect
ual
proper
ty
rights
and
confidential
business
information
or
trade
secrets
in
accordance
with
Union
and
national
law
,
the
information
and
documentation
shall
:
(
i)enable
provider
s
of
AI
systems
to
have
a
good
understanding
of
the
capabilities
and
limitations
of
the
general
-
pur
pose
AI
model
and
to
comply
with
their
obligations
pursuant
to
this
Regulation
;
and
(
ii)contain
,
at
a
minimum
,
the
elements
set
out
in
Annex
XII
;
(
c)put
in
place
a
policy
to
comply
with
Union
law
on
copyright
and
related
rights
,
and
in
particular
to
identify
and
comply
with
,
including
through
state
-
of
-
t
he
-
ar
t
technologi
es
,
a
reser
vation
of
rights
expressed
pursuant
to
Article
4(3
)
of
Directive
(
EU
)
;
(
d)draw
up
and
make
publicly
availa
ble
a
sufficiently
detailed
summar
y
about
the
content
used
for
training
of
the
general
-
pur
pose
AI
model
,
according
to
a
templat
e
provided
by
the
AI
Office
.
The
obligations
set
out
in
paragraph
1
,
points
(
a
)
and
(
b
)
,
shall
not
apply
to
providers
of
AI
models
that
are
released
under
a
free
and
open
-
source
licence
that
allows
for
the
access
,
usage
,
modif
ication
,
and
distr
ibution
of
the
model
,
and
whose
paramet
ers
,
including
the
weights
,
the
information
on
the
model
archit
ecture
,
and
the
information
on
model
usage
,
are
made
publicly
available
.
This
exception
shall
not
apply
to
general
-
pur
pose
AI
models
with
systemic
risks
.
competent
authorities
in
the
exercise
of
their
comp
etences
and
powers
pursuant
to
this
Regulation
.
demonstrate
complia
nce
with
the
obligations
set
out
in
paragraph
1
of
this
Article
,
until
a
harmonised
standard
is
published
.
Compliance
with
European
harmonised
standards
grants
providers
the
presump
tion
of
conf
ormity
to
the
extent
that
those
standards
cover
those
obligations
.
Providers
of
general
-
pur
pose
AI
models
who
do
not
adhere
to
an
approved
code
of
practice
or
do
not
comply
with
a
European
harmonised
standard
shall
demonstrate
alternative
adequat
e
means
of
complia
nce
for
assessment
by
the
Commission
.
emp
owered
to
adop
t
deleg
ated
acts
in
accordance
with
Article
97
to
detail
measurement
and
calculation
methodologies
with
a
view
to
allowing
for
comp
arable
and
verifiable
documentation
.
in
light
of
evolving
technological
developments
.
accordance
with
the
conf
identiality
obligations
set
out
in
Article
78
.
Article
54
authorised
represent
ativ
es
of
providers
of
general
-
pur
pose
AI
models
written
mandat
e
,
appoint
an
authorised
representative
which
is
established
in
the
Union
.
provid
er
.
provid
e
a
copy
of
the
mandate
to
the
AI
Offi
ce
upon
request
,
in
one
of
the
official
languag
es
of
the
institutions
of
the
Union
.
For
the
purposes
of
this
Regulation
,
the
mandate
shall
empo
wer
the
authorised
representative
to
carry
out
the
follo
wing
tasks
:
(
a)verify
that
the
technical
documentation
specif
ied
in
Annex
XI
has
been
drawn
up
and
all
obligations
referred
to
in
Article
53
and
,
where
applicable
,
Article
55
have
been
fulfilled
by
the
provider
;
(
b)keep
a
copy
of
the
technical
documentation
specified
in
Annex
XI
at
the
disposal
of
the
AI
Offi
ce
and
national
competent
authorities
,
for
a
period
of
10
years
after
the
general
-
pur
pose
AI
model
has
been
placed
on
the
mark
et
,
and
the
contact
details
of
the
provid
er
that
appointed
the
authorised
representative
;
(
c)provid
e
the
AI
Office
,
upon
a
reasoned
request
,
with
all
the
information
and
documentation
,
including
that
referred
to
in
point
(
b
)
,
necessary
to
demonstrat
e
complia
nce
with
the
oblig
ations
in
this
Chapt
er
;
(
d)cooperate
with
the
AI
Office
and
competent
authorities
,
upon
a
reasoned
request
,
in
any
action
they
take
in
relation
to
the
general
-
pur
pose
AI
model
,
including
when
the
model
is
integrat
ed
into
AI
systems
placed
on
the
market
or
put
into
service
in
the
Union
.
by
the
AI
Offi
ce
or
the
competent
authorities
,
on
all
issues
related
to
ensur
ing
complia
nce
with
this
Regulation
.
.
The
authorised
representative
shall
terminate
the
mandat
e
if
it
considers
or
has
reason
to
consider
the
provider
to
be
acting
contrar
y
to
its
obligations
pursuant
to
this
Regulation
.
In
such
a
case
,
it
shall
also
immediate
ly
inform
the
AI
Office
about
the
termination
of
the
mandate
and
the
reasons
theref
or
.
a
free
and
open
-
source
licence
that
allows
for
the
access
,
usage
,
modifi
cation
,
and
distr
ibution
of
the
model
,
and
whose
parameter
s
,
including
the
weights
,
the
information
on
the
model
arch
itecture
,
and
the
information
on
model
usage
,
are
made
publicly
available
,
unless
the
general
-
pur
pose
AI
models
present
systemic
risks
.
SECTION
3
Oblig
ations
of
providers
of
gener
al
-
pur
pose
AI
models
with
syste
mic
risk
Article
55
Obligations
of
providers
of
general
-
pur
pose
AI
models
with
systemic
risk
shall
:
(
a)perfo
rm
model
evaluation
in
accordance
with
standardised
prot
ocols
and
tools
reflecting
the
state
of
the
art
,
including
conducting
and
documenting
adversar
ial
testing
of
the
model
with
a
view
to
identifying
and
mitiga
ting
syste
mic
risks
;
(
b)assess
and
mitig
ate
possible
systemic
risks
at
Union
level
,
including
their
sources
,
that
may
stem
from
the
development
,
the
placing
on
the
market
,
or
the
use
of
general
-
pur
pose
AI
models
with
syste
mic
risk
;
(
c)keep
track
of
,
document
,
and
repor
t
,
without
undue
dela
y
,
to
the
AI
Office
and
,
as
appropriate
,
to
national
competent
authorities
,
relevant
information
about
serious
incidents
and
possible
corrective
measures
to
address
them
;
(
d)ensure
an
adequate
level
of
cybersecurity
protect
ion
for
the
gene
ral
-
pur
pose
AI
model
with
systemic
risk
and
the
physica
l
infrastructure
of
the
model
.
Article
56
to
demonstrate
compliance
with
the
obligations
set
out
in
paragraph
1
of
this
Article
,
until
a
harmonised
standard
is
published
.
Compliance
with
European
harmonised
standards
grants
provid
ers
the
presump
tion
of
conf
ormity
to
the
extent
that
those
standards
cover
those
obligations
.
Providers
of
general
-
pur
pose
AI
models
with
syste
mic
risks
who
do
not
adhere
to
an
approved
code
of
practice
or
do
not
comply
with
a
European
harmonised
standard
shall
demonstrate
alternative
adequate
means
of
compliance
for
assessment
by
the
Commission
.
accordance
with
the
conf
identiality
obligations
set
out
in
Article
78
.
SECTION
4
Codes
of
practice
Article
56
Codes
of
practice
to
the
proper
application
of
this
Regulation
,
taking
into
account
international
approaches
.
Articles
53
and
55
,
including
the
follo
wing
issues
:
means
to
ensure
that
the
information
referred
to
in
Article
53(1
)
,
points
(
a
)
and
(
b
)
,
is
kept
up
to
date
in
light
of
market
and
technological
developments
;
(
b)the
adequat
e
level
of
detail
for
the
summar
y
about
the
cont
ent
used
for
training
;
(
c)the
identifica
tion
of
the
type
and
nature
of
the
systemic
risks
at
Union
level
,
including
their
sources
,
where
appropriate
;
(
d)the
measures
,
procedures
and
modalities
for
the
assessment
and
management
of
the
syste
mic
risks
at
Union
level
,
including
the
documentation
thereof
,
which
shall
be
propor
tionate
to
the
risks
,
take
into
consideration
their
sever
ity
and
probability
and
take
into
account
the
specific
challeng
es
of
tack
ling
those
risks
in
light
of
the
possible
ways
in
which
such
risks
may
emerge
and
materi
alise
along
the
AI
value
chain
.
authorities
,
to
participate
in
the
drawing
-
up
of
codes
of
practice
.
Civil
society
organisations
,
industr
y
,
academia
and
other
relevant
stak
eholders
,
such
as
downstream
provider
s
and
independent
exper
ts
,
may
support
the
process
.
contain
commitments
or
measures
,
including
key
perfo
rmance
indicator
s
as
appropriate
,
to
ensure
the
achievement
of
those
objectives
,
and
that
they
take
due
account
of
the
needs
and
interests
of
all
interest
ed
parties
,
including
affect
ed
persons
,
at
Union
level
.
imp
lementation
of
the
commitments
and
the
measures
taken
and
their
outcomes
,
including
as
measured
agains
t
the
key
perfo
rmance
indicator
s
as
appropriate
.
Key
perfo
rmance
indicator
s
and
repor
ting
commitments
shall
reflect
diffe
rences
in
size
and
capacity
between
various
participants
.
practice
by
the
participants
and
their
contribution
to
the
proper
application
of
this
Regulation
.
The
AI
Office
and
the
Board
shall
assess
whether
the
codes
of
practice
cover
the
obligations
provided
for
in
Articles
53
and
55
,
and
shall
regularly
monitor
and
evaluate
the
achievement
of
their
objectives
.
They
shall
publish
their
assessment
of
the
adequacy
of
the
codes
of
practice
.
The
Commission
may
,
by
way
of
an
implementing
act
,
approve
a
code
of
practice
and
give
it
a
general
validity
within
the
Union
.
That
implementing
act
shall
be
adop
ted
in
accordance
with
the
examination
procedure
referred
to
in
Article
98(2
)
.
of
general
-
pur
pose
AI
models
not
presenting
syste
mic
risks
this
adherence
may
be
limit
ed
to
the
obligations
provided
for
in
Article
53
,
unless
they
declare
explicitly
their
interest
to
join
the
full
code
.
particular
in
light
of
emerging
standards
.
The
AI
Office
shall
assist
in
the
assessment
of
available
standards
.
inviting
provider
s
pursuant
to
paragraph
7
.
If
,
by
2
August
2025
,
a
code
of
practice
can
not
be
final
ised
,
or
if
the
AI
Office
deems
it
is
not
adequat
e
following
its
assessment
under
paragraph
6
of
this
Article
,
the
Commission
may
provid
e
,
by
means
of
imp
lementing
acts
,
common
rules
for
the
implementation
of
the
obligations
provided
for
in
Articles
53
and
55
,
including
the
issues
set
out
in
paragraph
2
of
this
Article
.
Those
imp
lementing
acts
shall
be
adop
ted
in
accordance
with
the
examination
procedure
refer
red
to
in
Article
MEASURES
IN
support
OF
innovation
Article
57
AI
regulator
y
sandbo
xes
level
,
which
shall
be
operational
by
2
August
2026
.
That
sandbo
x
may
also
be
established
jointly
with
the
comp
etent
authorities
of
other
Member
States
.
The
Commission
may
provide
technical
support
,
advice
and
tools
for
the
establishment
and
operation
of
AI
regulatory
sandbo
xes
.
The
obligation
under
the
first
subparagraph
may
also
be
fulfilled
by
participating
in
an
existing
sandbo
x
in
so
far
as
that
participation
provides
an
equivalent
level
of
national
coverag
e
for
the
participating
Member
States
.
other
Member
States
may
also
be
established
.
offices
and
agencies
,
and
may
exercise
the
roles
and
the
tasks
of
national
competent
authorities
in
accordance
with
this
Chapt
er
.
resources
to
comply
with
this
Article
effectively
and
in
a
timely
manner
.
Where
appropriate
,
national
competent
authorities
shall
cooperate
with
other
relevant
authorities
,
and
may
allow
for
the
involvement
of
other
actors
within
the
AI
ecosyste
m.
This
Article
shall
not
affect
other
regulato
ry
sandbo
xes
established
under
Union
or
national
law
.
Member
States
shall
ensure
an
appropriate
level
of
cooperation
between
the
authorities
super
vising
those
other
sandbo
xes
and
the
national
competent
authorities
.
innovation
and
facilitates
the
development
,
training
,
testing
and
validation
of
inno
vative
AI
systems
for
a
limited
time
before
their
being
placed
on
the
market
or
put
into
service
pursuant
to
a
specific
sandbo
x
plan
agreed
between
the
provid
ers
or
prospective
providers
and
the
competent
author
ity
.
Such
sandbo
xes
may
include
testing
in
real
world
conditions
super
vised
therein
.
sandbo
x
with
a
view
to
identifying
risks
,
in
particular
to
fundamental
rights
,
health
and
safety
,
testing
,
mitigation
measures
,
and
their
effectiveness
in
relation
to
the
obligations
and
requirements
of
this
Regulation
and
,
where
relevant
,
other
Union
and
national
law
super
vised
within
the
sandbo
x.
with
guidance
on
regulator
y
expectations
and
how
to
fulfil
the
requirements
and
obligations
set
out
in
this
Regulation
.
Upon
request
of
the
provider
or
prospective
provid
er
of
the
AI
system
,
the
competent
author
ity
shall
provid
e
a
written
proof
of
the
activities
successfully
carried
out
in
the
sandbo
x.
The
competent
author
ity
shall
also
provide
an
exit
repor
t
detailing
the
activities
carried
out
in
the
sandbo
x
and
the
related
results
and
learning
outcomes
.
Provi
ders
may
use
such
documentation
to
demonstrate
their
compliance
with
this
Regulation
through
the
conf
ormity
assessment
process
or
relevant
market
surveillance
activities
.
In
this
regard
,
the
exit
repor
ts
and
the
written
proof
provid
ed
by
the
national
competent
author
ity
shall
be
taken
positively
into
account
by
market
surveillance
authorities
and
notif
ied
bodies
,
with
a
view
to
accelerating
conf
ormity
assessment
procedures
to
a
reasonable
extent
.
the
Commission
and
the
Board
shall
be
authorised
to
access
the
exit
repor
ts
and
shall
take
them
into
account
,
as
appropriate
,
when
exercising
their
tasks
under
this
Regulation
.
If
both
the
provid
er
or
prospective
provid
er
and
the
national
competent
author
ity
explicitly
agree
,
the
exit
repor
t
may
be
made
publicly
available
through
the
sing
le
information
platform
refer
red
to
in
this
Article
.
Union
and
national
law
;
the
shar
ing
of
best
practices
through
cooperation
with
the
authorities
involved
in
the
AI
regulator
y
sandbo
x
;
(
c)foste
ring
innovation
and
competitiveness
and
facilitating
the
development
of
an
AI
ecosyste
m
;
(
d)contributing
to
evidence
-
based
regulatory
learning
;
(
e)facilitating
and
accelerating
access
to
the
Union
market
for
AI
systems
,
in
particular
when
provided
by
SMEs
,
including
start
-
ups
.
personal
data
or
other
wise
fall
under
the
supervisory
remit
of
other
national
authorities
or
comp
etent
authorities
providing
or
supporting
access
to
data
,
the
national
data
protect
ion
authorities
and
those
other
national
or
competent
authorities
are
associate
d
with
the
operation
of
the
AI
regulator
y
sandbo
x
and
involved
in
the
super
vision
of
those
aspects
to
the
extent
of
their
respective
tasks
and
powers
.
super
vising
the
sandbo
xes
,
including
at
regional
or
local
level
.
Any
significant
risks
to
health
and
safety
and
fundamental
rights
identifi
ed
during
the
development
and
testing
of
such
AI
systems
shall
result
in
an
adequat
e
mitigation
.
National
competent
authorities
shall
have
the
power
to
temporar
ily
or
permanently
suspend
the
testing
process
,
or
the
participation
in
the
sandbo
x
if
no
effective
mitigation
is
possible
,
and
shall
inform
the
AI
Offi
ce
of
such
decision
.
National
competent
authorities
shall
exer
cise
their
supervisory
powe
rs
within
the
limits
of
the
relevant
law
,
using
their
discretionar
y
powers
when
imp
lementing
legal
provisions
in
respect
of
a
specific
AI
regulator
y
sandbo
x
project
,
with
the
objective
of
supporting
innovation
in
AI
in
the
Union
.
Union
and
national
liability
law
for
any
damage
inflicted
on
third
parties
as
a
result
of
the
exper
imentation
taking
place
in
the
sandbo
x.
How
ever
,
provid
ed
that
the
prospective
provider
s
obser
ve
the
specific
plan
and
the
terms
and
conditions
for
their
participation
and
follow
in
good
faith
the
guidance
given
by
the
national
competent
author
ity
,
no
administrative
fines
shall
be
imposed
by
the
authorities
for
infringements
of
this
Regulation
.
Where
other
com
petent
authorities
responsible
for
other
Union
and
national
law
were
actively
involved
in
the
super
vision
of
the
AI
system
in
the
sandbo
x
and
provid
ed
guidance
for
compliance
,
no
administrative
fines
shall
be
imp
osed
rega
rding
that
law
.
cross
-
border
cooperation
between
national
competent
authorities
.
ask
them
for
support
and
guidance
.
The
AI
Office
shall
make
publicly
available
a
list
of
planned
and
existing
sandbo
xes
and
keep
it
up
to
date
in
order
to
encourage
more
interaction
in
the
AI
regulator
y
sandbo
xes
and
cross
-
border
cooperation
.
the
establishment
of
the
AI
regulatory
sandbo
x
and
ever
y
year
thereaf
ter
until
its
term
ination
,
and
a
final
repor
t.
Those
repor
ts
shall
provid
e
information
on
the
progress
and
results
of
the
imp
lementation
of
those
sandbo
xes
,
including
best
practices
,
incidents
,
lessons
learnt
and
recommendations
on
their
setup
and
,
where
relevant
,
on
the
application
and
possible
revision
of
this
Regulation
,
including
its
deleg
ated
and
impl
ementing
acts
,
and
on
the
application
of
other
Union
law
super
vised
by
the
competent
authorities
within
the
sandbo
x.
The
national
competent
authorities
shall
make
those
annual
repor
ts
or
abstracts
thereof
available
to
the
public
,
online
.
The
Commission
shall
,
where
appropriate
,
take
the
annual
repor
ts
into
account
when
exer
cising
its
tasks
under
this
Regulation
.
regulator
y
sandbo
xes
to
allow
stak
eholders
to
interac
t
with
AI
regulato
ry
sandbo
xes
and
to
raise
enquir
ies
with
competent
authorities
,
and
to
seek
non
-
binding
guidance
on
the
conf
ormity
of
inno
vative
products
,
services
,
business
models
embedding
AI
technologies
,
in
accordance
with
Article
62(1
)
,
point
(
c
)
.
The
Commission
shall
proactively
coordinate
with
national
competent
authorities
,
where
relevant
.
Detail
ed
arrangements
for
,
and
functioning
of
,
AI
regulat
ory
sandbo
xes
detailed
arrangements
for
the
establishment
,
development
,
implementation
,
operation
and
super
vision
of
the
AI
regulator
y
sandbo
xes
.
The
imp
lementing
acts
shall
include
common
principles
on
the
follo
wing
issues
:
(
a)eligibility
and
selection
criteria
for
participation
in
the
AI
regulator
y
sandbo
x
;
(
b)procedures
for
the
application
,
participation
,
monitoring
,
exiting
from
and
termination
of
the
AI
regulator
y
sandbo
x
,
including
the
sandbo
x
plan
and
the
exit
repor
t
;
(
c)the
terms
and
conditions
applicable
to
the
participants
.
Those
implementing
acts
shall
be
adop
ted
in
accordance
with
the
examination
procedure
refer
red
to
in
Article
98(2
)
.
eligibility
and
selection
criteria
,
which
shall
be
transparent
and
fair
,
and
that
national
com
petent
authorities
inform
applicants
of
their
decision
within
three
months
of
the
application
;
(
b)that
AI
regulatory
sandbo
xes
allow
broad
and
equal
access
and
keep
up
with
demand
for
participation
;
providers
and
prospective
providers
may
also
submit
applications
in
partnerships
with
deplo
yers
and
other
relevant
third
parties
;
(
c)that
the
detailed
arrang
ements
for
,
and
conditions
concerning
AI
regulator
y
sandbo
xes
support
,
to
the
best
extent
possible
,
flexibility
for
national
comp
etent
authorities
to
establish
and
operate
their
AI
regulatory
sandbo
xes
;
(
d)that
access
to
the
AI
regulator
y
sandbo
xes
is
free
of
charge
for
SMEs
,
including
start
-
ups
,
without
prejudice
to
exceptional
costs
that
national
comp
etent
authorities
may
reco
ver
in
a
fair
and
propor
tionate
manner
;
(
e)that
they
facilitat
e
providers
and
prospective
providers
,
by
means
of
the
learning
outcomes
of
the
AI
regulator
y
sandbo
xes
,
in
comp
lying
with
conf
ormity
assessment
obligations
under
this
Regulation
and
the
voluntar
y
application
of
the
codes
of
conduct
referred
to
in
Article
95
;
(
f)that
AI
regulator
y
sandbo
xes
facilitate
the
involvement
of
other
relevant
actor
s
within
the
AI
ecosyste
m
,
such
as
notif
ied
bodies
and
standardisation
organisations
,
SMEs
,
including
start
-
ups
,
enterprises
,
innovat
ors
,
testing
and
exper
imenta
-
tion
facilities
,
research
and
exper
imentation
labs
and
European
Digital
innovation
Hubs
,
centres
of
excellence
,
individual
researchers
,
in
order
to
allow
and
facilitat
e
cooperation
with
the
public
and
private
secto
rs
;
(
g)that
procedures
,
processes
and
administrative
requirements
for
application
,
selection
,
participation
and
exiting
the
AI
regulator
y
sandbo
x
are
simple
,
easily
intelligible
,
and
clearly
communicated
in
order
to
facilitat
e
the
participation
of
SMEs
,
including
start
-
ups
,
with
limited
legal
and
administrative
capacities
and
are
streamlined
across
the
Union
,
in
order
to
avoid
fragmentation
and
that
participation
in
an
AI
regulatory
sandbo
x
established
by
a
Member
State
,
or
by
the
European
Data
Protection
Super
visor
is
mutually
and
uniformly
recognised
and
carries
the
same
legal
effects
across
the
Union
;
(
h)that
participation
in
the
AI
regulato
ry
sandbo
x
is
limited
to
a
period
that
is
appropriate
to
the
complexity
and
scale
of
the
project
and
that
may
be
exte
nded
by
the
national
comp
etent
author
ity
;
(
i)that
AI
regulator
y
sandbo
xes
facilitate
the
development
of
tools
and
infrastructure
for
testing
,
bench
marking
,
assessing
and
explaining
dimensions
of
AI
systems
relevant
for
regulator
y
learning
,
such
as
accuracy
,
robustness
and
cybersecurity
,
as
well
as
measures
to
mitiga
te
risks
to
fundamental
rights
and
society
at
large
.
relevant
,
to
pre
-
deplo
yment
services
such
as
guidance
on
the
implementation
of
this
Regulation
,
to
other
value
-
adding
services
such
as
help
with
standardisation
documents
and
certification
,
testing
and
exper
imentation
facilities
,
European
Digital
innovation
Hubs
and
centres
of
excellence
.
Where
national
comp
etent
authorities
consider
author
ising
testing
in
real
world
conditions
super
vised
within
the
framework
of
an
AI
regulator
y
sandbo
x
to
be
established
under
this
Article
,
they
shall
specifically
agree
the
terms
and
conditions
of
such
testing
and
,
in
particular
,
the
appropriate
safeguards
with
the
participants
,
with
a
view
to
protecting
fundamental
rights
,
health
and
safety
.
Where
appropriate
,
they
shall
cooperate
with
other
national
comp
etent
authorities
with
a
view
to
ensur
ing
consistent
practices
across
the
Union
.
Article
59
Further
processing
of
personal
data
for
dev
eloping
certain
AI
systems
in
the
public
interest
in
the
AI
regulator
y
sandbo
x
purpose
of
developing
,
training
and
testing
certain
AI
systems
in
the
sandbo
x
when
all
of
the
following
conditions
are
met
:
(
a)AI
systems
shall
be
developed
for
safeguarding
substantial
public
intere
st
by
a
public
author
ity
or
another
natural
or
lega
l
person
and
in
one
or
more
of
the
follo
wing
areas
:
(
i)public
safety
and
public
health
,
including
disease
detection
,
diagnosis
prevention
,
control
and
treatment
and
improvement
of
health
care
systems
;
(
ii)a
high
level
of
protect
ion
and
improvement
of
the
quality
of
the
environment
,
protection
of
biodiversity
,
protection
against
pollution
,
green
transition
measures
,
climat
e
change
mitiga
tion
and
adaptation
measures
;
(
iii
)
energy
sustainability
;
(
iv
)
safety
and
resilience
of
transport
systems
and
mobility
,
critical
infrastructure
and
networks
;
(
v)efficiency
and
quality
of
public
administration
and
public
services
;
(
b)the
data
processed
are
necessary
for
comp
lying
with
one
or
more
of
the
requirements
refer
red
to
in
Chap
ter
III
,
Section
2
where
those
requirements
can
not
effectively
be
fulfilled
by
processing
anonymised
,
synthetic
or
other
non
-
personal
data
;
(
c)there
are
effective
monitoring
mechanisms
to
identify
if
any
high
risks
to
the
rights
and
freedoms
of
the
data
subjects
,
as
referred
to
in
Article
35
of
Regulation
(
EU
)
and
in
Article
39
of
Regulation
(
EU
)
,
may
arise
during
the
sandbo
x
exper
imentation
,
as
well
as
response
mec
hanisms
to
promp
tly
mitiga
te
those
risks
and
,
where
necessary
,
stop
the
processing
;
(
d)any
personal
data
to
be
processed
in
the
cont
ext
of
the
sandbo
x
are
in
a
functionally
separate
,
isolated
and
prot
ected
data
processing
environment
under
the
control
of
the
prospective
provider
and
only
authorised
persons
have
access
to
those
data
;
(
e)provid
ers
can
further
share
the
originally
collect
ed
data
only
in
accordance
with
Union
data
protection
law
;
any
personal
data
create
d
in
the
sandbo
x
can
not
be
shared
outside
the
sandbo
x
;
(
f)any
processing
of
personal
data
in
the
context
of
the
sandbo
x
neither
leads
to
measures
or
decisions
affecting
the
data
subjects
nor
does
it
affect
the
application
of
their
rights
laid
down
in
Union
law
on
the
protection
of
personal
data
;
(
g)any
personal
data
processed
in
the
cont
ext
of
the
sandbo
x
are
protected
by
means
of
appropriate
technical
and
organisational
measures
and
deleted
once
the
participation
in
the
sandbo
x
has
terminated
or
the
personal
data
has
reac
he
d
the
end
of
its
retention
period
;
(
h)the
logs
of
the
processing
of
personal
data
in
the
cont
ext
of
the
sandbo
x
are
kept
for
the
duration
of
the
participation
in
the
sandbo
x
,
unless
provid
ed
other
wise
by
Union
or
national
law
;
(
i)a
comp
lete
and
detailed
descr
iption
of
the
process
and
rationale
behind
the
training
,
testing
and
validation
of
the
AI
syste
m
is
kept
together
with
the
testing
results
as
part
of
the
technical
documentation
refer
red
to
in
Annex
IV
;
(
j)a
shor
t
summar
y
of
the
AI
project
developed
in
the
sandbo
x
,
its
objectives
and
expecte
d
results
is
published
on
the
website
of
the
competent
authorities
;
this
obligati
on
shall
not
cover
sensitive
operational
data
in
relation
to
the
activities
of
law
enforcement
,
border
control
,
immigration
or
asylum
authorities
.
criminal
penalties
,
including
safeguarding
against
and
preventing
threats
to
public
security
,
under
the
control
and
responsibility
of
law
enforcement
authorities
,
the
processing
of
personal
data
in
AI
regulato
ry
sandbo
xes
shall
be
based
on
a
specific
Union
or
national
law
and
subject
to
the
same
cumulative
conditions
as
referred
to
in
paragraph
1
.
purposes
than
those
explicitly
mentioned
in
that
law
,
as
well
as
to
Union
or
national
law
laying
down
the
basis
for
the
processing
of
personal
data
which
is
necessary
for
the
purpose
of
developing
,
testing
or
training
of
inno
vative
AI
systems
or
any
other
legal
basis
,
in
compliance
with
Union
law
on
the
protection
of
personal
data
.
Article
60
Testing
of
higher
isk
AI
systems
in
real
world
conditions
outside
AI
regulator
y
sandbo
xes
provid
ers
or
prospective
provider
s
of
higher
isk
AI
systems
listed
in
Annex
III
,
in
accordance
with
this
Article
and
the
real
-
world
testing
plan
referred
to
in
this
Article
,
without
prejudice
to
the
prohibitions
under
Article
5
.
The
Commission
shall
,
by
means
of
implementing
acts
,
specify
the
detailed
elements
of
the
real
-
wo
rld
testing
plan
.
Those
imp
lementing
acts
shall
be
adopt
ed
in
accordance
with
the
examination
procedure
refer
red
to
in
Article
98(2
)
.
This
paragraph
shall
be
without
prejudice
to
Union
or
national
law
on
the
testing
in
real
world
conditions
of
higher
isk
AI
systems
related
to
products
covered
by
Union
harmonisation
legislation
listed
in
Annex
I.
conditions
at
any
time
before
the
placing
on
the
mark
et
or
the
putting
into
service
of
the
AI
syste
m
on
their
own
or
in
partnership
with
one
or
more
deplo
yers
or
prospective
deplo
yers
.
review
that
is
required
by
Union
or
national
law
.
conditions
are
met
:
(
a)the
provid
er
or
prospective
provider
has
drawn
up
a
real
-
world
testing
plan
and
submitted
it
to
the
market
surveillance
author
ity
in
the
Member
State
where
the
testing
in
real
world
conditions
is
to
be
conducte
d
;
(
b)the
market
surveillance
author
ity
in
the
Member
State
where
the
testing
in
real
world
conditions
is
to
be
conducte
d
has
approved
the
testing
in
real
world
conditions
and
the
real
-
world
testing
plan
;
where
the
market
surveillance
author
ity
has
not
provided
an
answer
within
30
days
,
the
testing
in
real
world
conditions
and
the
real
-
world
testing
plan
shall
be
understood
to
have
been
approved
;
where
national
law
does
not
provid
e
for
a
tacit
approva
l
,
the
testing
in
real
world
conditions
shall
remain
subject
to
an
author
isation
;
(
c)the
provid
er
or
prospective
provider
,
with
the
excep
tion
of
provider
s
or
prospective
provider
s
of
higher
isk
AI
systems
referred
to
in
points
1
,
6
and
7
of
Annex
III
in
the
areas
of
law
enforcement
,
migration
,
asylum
and
border
control
managem
ent
,
and
higher
isk
AI
systems
referred
to
in
point
2
of
Annex
III
has
registered
the
testing
in
real
world
conditions
in
accordance
with
Article
71(4
)
with
a
Union
-
wide
unique
sing
le
identification
number
and
with
the
information
specified
in
Annex
IX
;
the
provider
or
prospective
provider
of
higher
isk
AI
systems
referred
to
in
points
1
,
the
testing
in
real
-
world
conditions
in
the
secure
non
-
public
section
of
the
EU
database
according
to
Article
49(4
)
,
point
(
d
)
,
with
a
Union
-
wide
unique
sing
le
identifica
tion
number
and
with
the
information
specified
therein
;
the
provid
er
or
prospective
provid
er
of
higher
isk
AI
systems
referred
to
in
point
2
of
Annex
III
has
regist
ered
the
testing
in
real
-
wo
rld
conditions
in
accordance
with
Article
49(5
)
;
provider
or
prospective
provider
conducting
the
testing
in
real
world
conditions
is
established
in
the
Union
or
has
appointed
a
legal
representative
who
is
established
in
the
Union
;
(
e)data
collected
and
processed
for
the
purpose
of
the
testing
in
real
world
conditions
shall
be
transf
erred
to
third
countr
ies
only
provided
that
appropriate
and
applicable
safeguards
under
Union
law
are
imp
lemented
;
(
f)the
testing
in
real
world
conditions
does
not
last
longer
than
necessary
to
achieve
its
objectives
and
in
any
case
not
longer
than
six
months
,
which
may
be
exte
nded
for
an
additional
period
of
six
months
,
subject
to
prior
notif
ication
by
the
provider
or
prospective
provider
to
the
market
surveillance
author
ity
,
accom
panied
by
an
explanation
of
the
need
for
such
an
extension
;
(
g)the
subjects
of
the
testing
in
real
world
conditions
who
are
persons
belonging
to
vulnerable
groups
due
to
their
age
or
disability
,
are
appropriately
prot
ected
;
(
h)where
a
provider
or
prospective
provider
organises
the
testing
in
real
world
conditions
in
cooperation
with
one
or
more
deplo
yers
or
prospective
deplo
yers
,
the
latter
have
been
informed
of
all
aspects
of
the
testing
that
are
relevant
to
their
decision
to
participate
,
and
given
the
relevant
instr
uctions
for
use
of
the
AI
system
refer
red
to
in
Article
13
;
the
provid
er
or
prospective
provider
and
the
deplo
yer
or
prospective
deplo
yer
shall
conclude
an
agreement
specifying
their
roles
and
responsibilities
with
a
view
to
ensur
ing
compliance
with
the
provisions
for
testing
in
real
world
conditions
under
this
Regulation
and
under
other
applicable
Union
and
national
law
;
(
i)the
subjects
of
the
testing
in
real
world
conditions
have
given
informed
consent
in
accordance
with
Article
61
,
or
in
the
case
of
law
enforcement
,
where
the
seeking
of
informed
consent
would
prevent
the
AI
system
from
being
tested
,
the
testing
itself
and
the
outcome
of
the
testing
in
the
real
world
conditions
shall
not
have
any
negative
effect
on
the
subjects
,
and
their
personal
data
shall
be
deleted
after
the
test
is
perfo
rmed
;
(
j)the
testing
in
real
world
conditions
is
effectively
overseen
by
the
provider
or
prospective
provider
,
as
well
as
by
deplo
yers
or
prospective
deplo
yers
through
persons
who
are
suitably
qualified
in
the
relevant
field
and
have
the
necessary
capacity
,
training
and
author
ity
to
perf
orm
their
tasks
;
(
k)the
predictions
,
recommendations
or
decisions
of
the
AI
syste
m
can
be
effectively
reversed
and
disreg
arded
.
without
any
resulting
detr
iment
and
without
having
to
provide
any
justifi
cation
,
withdra
w
from
the
testing
at
any
time
by
revok
ing
their
informed
consent
and
may
request
the
immediate
and
permanent
deletion
of
their
personal
data
.
The
withdra
wal
of
the
informed
consent
shall
not
affect
the
activities
already
carried
out
.
requir
ing
providers
and
prospective
provid
ers
to
provide
information
,
of
carrying
out
unannounced
remot
e
or
on
-
site
inspections
,
and
of
perfor
ming
checks
on
the
conduct
of
the
testing
in
real
world
conditions
and
the
related
higher
isk
AI
systems
.
market
surveillance
authorities
shall
use
those
powers
to
ensure
the
safe
development
of
testing
in
real
world
conditions
.
market
surveillance
author
ity
in
accordance
with
Article
73
.
The
provid
er
or
prospective
provid
er
shall
adopt
immediate
mitiga
tion
measures
or
,
failing
that
,
shall
suspend
the
testing
in
real
world
conditions
until
such
mitigation
take
s
place
,
or
other
wise
terminate
it
.
The
provider
or
prospective
provider
shall
establish
a
procedure
for
the
promp
t
recall
of
the
AI
syste
m
upon
such
termination
of
the
testing
in
real
world
conditions
.
the
testing
in
real
world
conditions
is
to
be
conducte
d
of
the
suspension
or
term
ination
of
the
testing
in
real
world
conditions
and
of
the
final
outcomes
.
caused
in
the
course
of
their
testing
in
real
world
conditions
.
Infor
med
consent
to
participate
in
testing
in
real
world
conditions
outside
AI
regulat
ory
sandbo
xes
from
the
subjects
of
testing
prior
to
their
participation
in
such
testing
and
after
their
having
been
duly
informed
with
concise
,
clear
,
relevant
,
and
understandable
information
regarding
:
(
a)the
nature
and
objectives
of
the
testing
in
real
world
conditions
and
the
possible
inconve
nience
that
may
be
linke
d
to
their
participation
;
(
b)the
conditions
under
which
the
testing
in
real
world
conditions
is
to
be
conducted
,
including
the
expecte
d
duration
of
the
subject
or
subjects
’
participation
;
(
c)their
rights
,
and
the
guarantees
regard
ing
their
participation
,
in
particular
their
right
to
refuse
to
participate
in
,
and
the
right
to
withdraw
from
,
testing
in
real
world
conditions
at
any
time
without
any
resulting
detr
iment
and
without
having
to
provid
e
any
justificati
on
;
(
d)the
arrang
ements
for
requesting
the
reversal
or
the
disregarding
of
the
predictions
,
recommendations
or
decisions
of
the
AI
syste
m
;
(
e)the
Union
-
wide
unique
sing
le
identification
number
of
the
testing
in
real
world
conditions
in
accordance
with
Article
obtained
.
representative
.
Article
62
Measures
for
providers
and
deplo
yers
,
in
particular
SMEs
,
including
start
-
ups
regulator
y
sandbo
xes
,
to
the
extent
that
they
fulfil
the
eligibility
conditions
and
selection
criteria
;
the
priority
access
shall
not
preclude
other
SMEs
,
including
start
-
ups
,
other
than
those
refer
red
to
in
this
paragraph
from
access
to
the
AI
regulator
y
sandbo
x
,
provided
that
they
also
fulfil
the
eligibility
conditions
and
selection
criteria
;
(
b)organise
specific
awareness
raising
and
training
activities
on
the
application
of
this
Regulation
tailored
to
the
needs
of
SMEs
including
start
-
ups
,
deplo
yers
and
,
as
appropriate
,
local
public
authorities
;
(
c)utilise
existing
dedicat
ed
channels
and
where
appropriate
,
establish
new
ones
for
communication
with
SMEs
including
start
-
ups
,
deplo
yers
,
other
innovat
ors
and
,
as
appropriate
,
local
public
authorities
to
provide
advice
and
respond
to
quer
ies
about
the
implementation
of
this
Regulation
,
including
as
rega
rds
participation
in
AI
regulator
y
sandbo
xes
;
(
d)facilitate
the
participation
of
SMEs
and
other
relevant
stak
eholders
in
the
standardisation
development
process
.
fees
for
conf
ormity
assessment
under
Article
43
,
reducing
those
fees
propor
tionately
to
their
size
,
market
size
and
other
relevant
indicator
s.
all
operators
across
the
Union
;
appropriate
communication
campaigns
to
raise
awareness
about
the
obligati
ons
arising
from
this
Regulation
;
(
d)evaluate
and
promot
e
the
convergence
of
best
practices
in
public
procurement
procedures
in
relation
to
AI
systems
.
Article
63
Derogations
for
specific
operators
quality
management
system
required
by
Article
17
of
this
Regulation
in
a
simplified
manner
,
provid
ed
that
they
do
not
have
partner
enterprises
or
linked
enter
prises
within
the
meaning
of
that
Recommendation
.
For
that
purpose
,
the
Commission
shall
develop
guidelines
on
the
elements
of
the
quality
management
system
which
may
be
comp
lied
with
in
a
simplified
manner
consider
ing
the
needs
of
microenterp
rises
,
without
affecting
the
level
of
protection
or
the
need
for
complia
nce
with
the
requirements
in
respect
of
higher
isk
AI
systems
.
requirements
or
oblig
ations
laid
down
in
this
Regulation
,
including
those
established
in
Articles
9
,
10
,
11
,
12
,
13
,
14
,
15
,
CHAPTER
VII
GOVERNANCE
SECTION
1
Gove
rnance
at
Union
level
Article
64
AI
Office
Article
65
Establishment
and
structure
of
the
European
Artificial
Intelligence
Board
participate
as
obser
ver
.
The
AI
Office
shall
also
attend
the
Board
’s
meetings
,
without
taking
part
in
the
votes
.
Other
national
and
Union
authorities
,
bodies
or
exper
ts
may
be
invit
ed
to
the
meetings
by
the
Board
on
a
case
by
case
basis
,
where
the
issues
discussed
are
of
relevance
for
them
.
Board
’s
tasks
refer
red
to
in
Article
66
;
(
b)are
designate
d
as
a
sing
le
contact
point
vis
-
à
-
vis
the
Board
and
,
where
appropriate
,
taking
into
account
Member
States
’
needs
,
as
a
sing
le
contact
point
for
stakeholders
;
(
c)are
emp
ower
ed
to
facilitate
consistency
and
coordination
between
national
competent
authorities
in
their
Member
State
as
rega
rds
the
imp
lementation
of
this
Regulation
,
including
through
the
collection
of
relevant
data
and
information
for
the
purpose
of
fulfilling
their
tasks
on
the
Board
.
majorit
y.
The
rules
of
procedure
shall
,
in
particular
,
lay
down
procedures
for
the
selection
process
,
the
duration
of
the
mandate
of
,
and
specifications
of
the
tasks
of
,
the
Chair
,
detailed
arrang
ements
for
voting
,
and
the
organi
sation
of
the
Board
’s
activities
and
those
of
its
sub
-
groups
.
surveillance
authorities
and
notifying
authorities
about
issues
related
to
market
surveillance
and
notified
bodies
respectively
.
The
standing
sub
-
group
for
market
surveillance
should
act
as
the
administrative
cooperation
group
(
ADCO
)
for
this
Regulation
within
the
meaning
of
Article
30
of
Regulation
(
EU
)
.
The
Board
may
establish
other
standing
or
temporar
y
sub
-
groups
as
appropriate
for
the
purpose
of
examining
specific
issues
.
Where
appropriate
,
representatives
of
the
advisor
y
forum
refer
red
to
in
Article
67
may
be
invited
to
such
sub
-
groups
or
to
specific
meetings
of
those
subgroups
as
obser
vers
.
secretar
iat
for
the
Board
,
convene
the
meetings
upon
request
of
the
Chair
,
and
prepare
the
agenda
in
accordance
with
the
tasks
of
the
Board
pursuant
to
this
Regulation
and
its
rules
of
procedure
.
Article
66
Tasks
of
the
Board
The
Board
shall
advise
and
assist
the
Commission
and
the
Member
States
in
order
to
facilitat
e
the
consistent
and
effective
application
of
this
Regulation
.
To
that
end
,
the
Board
may
in
particular
:
(
a
)
contribut
e
to
the
coordination
among
national
comp
etent
authorities
responsible
for
the
application
of
this
Regulation
and
,
in
cooperation
with
and
subject
to
the
agreement
of
the
market
surveillance
authorities
concer
ned
,
support
joint
activities
of
market
surveillance
authorities
referred
to
in
Article
74(11
)
;
(
b
)
collect
and
share
technical
and
regulato
ry
exper
tise
and
best
practices
among
Member
States
;
(
c
)
provid
e
advice
on
the
imp
lementation
of
this
Regulation
,
in
particular
as
regards
the
enforcement
of
rules
on
general
-
pur
pose
AI
models
;
(
d
)
contribut
e
to
the
harmonisation
of
administrative
practices
in
the
Member
States
,
including
in
relation
to
the
derogat
ion
from
the
conf
ormity
assessment
procedures
refer
red
to
in
Article
46
,
the
functioning
of
AI
regulator
y
sandbo
xes
,
and
testing
in
real
world
conditions
referred
to
in
Articles
57
,
59
and
60
;
(
e
)
at
the
request
of
the
Commission
or
on
its
own
initiative
,
issue
recommendations
and
written
opinions
on
any
relevant
matt
ers
related
to
the
implementation
of
this
Regulation
and
to
its
consiste
nt
and
effective
application
,
including
:
(
i)on
the
development
and
application
of
codes
of
conduct
and
codes
of
practice
pursuant
to
this
Regulation
,
as
well
as
of
the
Commission
’s
guidelines
;
(
ii)the
evaluation
and
review
of
this
Regulation
pursuant
to
Article
112
,
including
as
rega
rds
the
serious
incident
repor
ts
referred
to
in
Article
73
,
and
the
functioning
of
the
EU
database
referred
to
in
Article
71
,
the
preparation
of
the
deleg
ated
or
implementing
acts
,
and
as
regard
s
possible
alignments
of
this
Regulation
with
the
Union
harmonisation
legislation
listed
in
Annex
I
;
(
iii
)
on
technical
specifications
or
existing
standards
regarding
the
requirements
set
out
in
Chapt
er
III
,
Section
2
;
on
the
use
of
harmonised
standards
or
common
specifications
refer
red
to
in
Articles
40
and
41
;
(
v)trends
,
such
as
European
global
competitiveness
in
AI
,
the
uptak
e
of
AI
in
the
Union
,
and
the
development
of
digital
skills
;
(
vi
)
trends
on
the
evolving
typology
of
AI
value
chains
,
in
particular
on
the
resulting
implications
in
terms
of
accountability
;
(
vii
)
on
the
potential
need
for
amendment
to
Annex
III
in
accordance
with
Article
7
,
and
on
the
potential
need
for
possible
revision
of
Article
5
pursuant
to
Article
112
,
taking
into
account
relevant
available
evidence
and
the
latest
developments
in
technology
;
(
f
)
support
the
Commission
in
promoting
AI
literac
y
,
public
awareness
and
understanding
of
the
benefi
ts
,
risks
,
safeguards
and
rights
and
obligations
in
relation
to
the
use
of
AI
systems
;
(
g
)
facilitate
the
development
of
common
criteria
and
a
shared
understanding
among
market
operators
and
competent
authorities
of
the
relevant
concep
ts
provid
ed
for
in
this
Regulation
,
including
by
contributing
to
the
development
of
bench
mark
s
;
(
h
)
cooperate
,
as
appropriate
,
with
other
Union
institutions
,
bodies
,
offices
and
agencies
,
as
well
as
relevant
Union
exper
t
groups
and
networks
,
in
particular
in
the
fields
of
product
safety
,
cybersecurity
,
comp
etition
,
digital
and
media
services
,
financ
ial
services
,
consumer
protection
,
data
and
fundamental
rights
protection
;
(
i
)
contribut
e
to
effective
cooperation
with
the
competent
authorities
of
third
countr
ies
and
with
inter
national
organisations
;
(
j
)
assist
national
comp
etent
authorities
and
the
Commission
in
developing
the
organi
sational
and
technical
exper
tise
required
for
the
imp
lementation
of
this
Regulation
,
including
by
contributing
to
the
assessment
of
training
needs
for
staff
of
Member
States
involved
in
implementing
this
Regulation
;
(
k
)
assist
the
AI
Offi
ce
in
supporting
national
comp
etent
authorities
in
the
establishment
and
development
of
AI
regulator
y
sandbo
xes
,
and
facilitate
cooperation
and
information
-
shar
ing
among
AI
regulator
y
sandbo
xes
;
(
l
)
contribut
e
to
,
and
provide
relevant
advice
on
,
the
development
of
guidance
documents
;
(
m
)
advise
the
Commission
in
relation
to
inter
national
matt
ers
on
AI
;
(
n
)
provid
e
opinions
to
the
Commission
on
the
qualif
ied
alerts
regard
ing
general
-
pur
pose
AI
models
;
(
o
)
receive
opinions
by
the
Member
States
on
qualified
alerts
regard
ing
general
-
pur
pose
AI
models
,
and
on
national
exper
iences
and
practices
on
the
monitoring
and
enforcement
of
AI
systems
,
in
particular
systems
integrating
the
general
-
pur
pose
AI
models
.
Article
67
Advisor
y
forum
to
contribut
e
to
their
tasks
under
this
Regulation
.
start
-
ups
,
SMEs
,
civil
society
and
academia
.
The
membership
of
the
advisor
y
forum
shall
be
balanced
with
rega
rd
to
commercial
and
non
-
commercial
interests
and
,
within
the
catego
ry
of
commercial
interests
,
with
regard
to
SMEs
and
other
undertakings
.
paragraph
2
,
from
amongst
stakeholders
with
recognised
exper
tise
in
the
field
of
AI
.
.
The
term
of
offic
e
of
the
members
of
the
advisor
y
forum
shall
be
two
years
,
which
may
be
extended
by
up
to
no
more
than
four
years
.
Committee
for
Electrotec
hnical
Standardization
(
CENELEC
)
,
and
the
European
Telecommunications
Standards
Institute
(
ETSI
)
shall
be
permanent
members
of
the
advisor
y
forum
.
accordance
with
criteria
set
out
in
paragraph
2
.
The
term
of
office
of
the
co
-
c
hairs
shall
be
two
years
,
renewable
once
.
stak
eholders
to
its
meetings
.
the
Commission
.
specific
questions
related
to
the
objectives
of
this
Regulation
.
Article
68
Scientif
ic
panel
of
independent
exper
ts
of
independent
exper
ts
(
the
‘
scientific
panel
’
)
intended
to
support
the
enforcement
activities
under
this
Regulation
.
That
imp
lementing
act
shall
be
adop
ted
in
accordance
with
the
examination
procedure
referred
to
in
Article
98(2
)
.
technical
exper
tise
in
the
field
of
AI
necessary
for
the
tasks
set
out
in
paragraph
3
,
and
shall
be
able
to
demonstrate
meeting
all
of
the
following
conditions
:
(
a)having
particular
exper
tise
and
compet
ence
and
scientific
or
technical
exper
tise
in
the
field
of
AI
;
(
b)independence
from
any
provid
er
of
AI
systems
or
general
-
pur
pose
AI
models
;
(
c)an
ability
to
carry
out
activities
dilige
ntly
,
accurately
and
objectively
.
The
Commission
,
in
consultation
with
the
Board
,
shall
determine
the
number
of
exper
ts
on
the
panel
in
accordance
with
the
required
needs
and
shall
ensure
fair
gender
and
geographical
representation
.
in
particular
by
:
(
i)alerting
the
AI
Office
of
possible
systemic
risks
at
Union
level
of
general
-
pur
pose
AI
models
,
in
accordance
with
Article
90
;
(
ii)contributing
to
the
development
of
tools
and
methodologies
for
evaluating
capabilities
of
general
-
pur
pose
AI
models
and
systems
,
including
through
bench
mark
s
;
(
iii
)
providing
advice
on
the
classif
ication
of
general
-
pur
pose
AI
models
with
systemic
risk
;
(
iv
)
providing
advice
on
the
classif
ication
of
various
general
-
pur
pose
AI
models
and
systems
;
to
the
development
of
tools
and
templates
;
(
b)supporting
the
work
of
market
surveillance
authorities
,
at
their
request
;
(
c)supporting
cross
-
border
market
surveillance
activities
as
referred
to
in
Article
74(11
)
,
without
prejudice
to
the
powers
of
market
surveillance
authorities
;
(
d)supporting
the
AI
Offi
ce
in
carrying
out
its
duties
in
the
cont
ext
of
the
Union
safeguard
procedure
pursuant
to
Article
81
.
confi
dentiality
of
information
and
data
obtained
in
carrying
out
their
tasks
and
activities
.
They
shall
neither
seek
nor
take
instr
uctions
from
anyone
when
exercising
their
tasks
under
paragraph
3
.
Each
exper
t
shall
draw
up
a
declaration
of
intere
sts
,
which
shall
be
made
publicly
available
.
The
AI
Office
shall
establish
systems
and
procedures
to
actively
manage
and
prevent
pote
ntial
conf
licts
of
interest
.
arrangements
for
the
scientific
panel
and
its
members
to
issue
alerts
,
and
to
request
the
assistance
of
the
AI
Office
for
the
perfo
rmance
of
the
tasks
of
the
scientifi
c
panel
.
Article
69
Access
to
the
pool
of
exper
ts
by
the
Member
States
Regulation
.
the
level
of
fees
as
well
as
the
scale
and
structure
of
reco
verable
costs
shall
be
set
out
in
the
imp
lementing
act
refer
red
to
in
Article
68(1
)
,
taking
into
account
the
objectives
of
the
adequat
e
imp
lementation
of
this
Regulation
,
cost
-
effectiveness
and
the
necessity
of
ensur
ing
effective
access
to
exper
ts
for
all
Member
States
.
combination
of
support
activities
carried
out
by
Union
AI
testing
support
pursuant
to
Article
84
and
exper
ts
pursuant
to
this
Article
is
efficiently
organised
and
provides
the
best
possible
added
value
.
SECTION
2
National
compe
tent
authorities
Article
70
Designation
of
national
competent
authorities
and
single
points
of
cont
act
at
least
one
market
surveillance
author
ity
for
the
purposes
of
this
Regulation
.
Those
national
comp
etent
authorities
shall
exercise
their
powers
independently
,
impar
tially
and
without
bias
so
as
to
safeguard
the
objectivity
of
their
activities
and
tasks
,
and
to
ensure
the
application
and
implementation
of
this
Regulation
.
The
members
of
those
authorities
shall
refrain
from
any
action
incom
patible
with
their
duties
.
Provided
that
those
principles
are
obser
ved
,
such
activities
and
tasks
may
be
perfo
rmed
by
one
or
more
designate
d
authorities
,
in
accordance
with
the
organisational
needs
of
the
Member
State
.
surveillance
authorities
and
the
tasks
of
those
authorities
,
as
well
as
any
subsequent
change
s
thereto
.
Member
States
shall
make
publicly
availa
ble
information
on
how
competent
authorities
and
sing
le
points
of
contact
can
be
contact
ed
,
through
electronic
communication
means
by
2
August
2025
.
Member
States
shall
designate
a
market
surveillance
author
ity
to
act
as
the
sing
le
point
of
contact
for
this
Regulation
,
and
shall
notify
the
Commission
of
the
identity
of
the
sing
le
point
of
contact
.
The
Commission
shall
make
a
list
of
the
sing
le
points
of
contact
publicly
available
.
.
Member
States
shall
ensure
that
their
national
competent
authorities
are
provided
with
adequate
technical
,
financial
and
human
resources
,
and
with
infrastructure
to
fulfil
their
tasks
effectively
under
this
Regulation
.
In
particular
,
the
national
competent
authorities
shall
have
a
suffi
cient
number
of
personnel
permanently
available
whose
comp
etences
and
exper
tise
shall
include
an
in
-
depth
understanding
of
AI
technologies
,
data
and
data
computing
,
personal
data
protection
,
cybersecurity
,
fundamental
rights
,
health
and
safety
risks
and
kno
wledge
of
existing
standards
and
legal
requirements
.
Member
States
shall
assess
and
,
if
necessary
,
update
compet
ence
and
resource
requirements
referred
to
in
this
paragraph
on
an
annual
basis
.
obligations
set
out
in
Article
78
.
of
the
financ
ial
and
human
resources
of
the
national
comp
etent
authorities
,
with
an
assessment
of
their
adequacy
.
The
Commission
shall
transmit
that
information
to
the
Board
for
discussion
and
possible
recommendations
.
particular
to
SMEs
including
start
-
ups
,
taking
into
account
the
guidance
and
advice
of
the
Board
and
the
Commission
,
as
appropriate
.
Whenever
national
comp
etent
authorities
intend
to
provide
guidance
and
advice
with
rega
rd
to
an
AI
system
in
areas
covered
by
other
Union
law
,
the
national
comp
etent
authorities
under
that
Union
law
shall
be
consult
ed
,
as
appropriate
.
Protection
Super
visor
shall
act
as
the
competent
author
ity
for
their
super
vision
.
CHAPTER
VIII
EU
DATABASE
FOR
higherISK
AI
SYSTEMS
Article
71
EU
database
for
higher
isk
AI
systems
listed
in
Annex
III
information
refer
red
to
in
paragraphs
2
and
3
of
this
Article
concerning
higher
isk
AI
systems
referred
to
in
Article
6(2
)
which
are
registered
in
accordance
with
Articles
49
and
60
and
AI
systems
that
are
not
considered
as
higher
isk
pursuant
to
Article
6(3
)
and
which
are
registered
in
accordance
with
Article
6(4
)
and
Article
49
.
When
setting
the
functional
specific
ations
of
such
database
,
the
Commission
shall
consult
the
relevant
exper
ts
,
and
when
updating
the
functional
specific
ations
of
such
database
,
the
Commission
shall
consult
the
Board
.
applicable
,
by
the
authorised
representative
.
behalf
of
,
a
public
author
ity
,
agency
or
body
,
in
accordance
with
Article
49(3
)
and
.
the
EU
database
register
ed
in
accordance
with
Article
49
shall
be
accessible
and
publicly
available
in
a
user
-friendly
manner
.
The
information
should
be
easily
navig
able
and
machi
ne
-
readable
.
The
information
regist
ered
in
accordance
with
Article
60
shall
be
accessible
only
to
market
surveillance
authorities
and
the
Commission
,
unless
the
prospective
provider
or
provid
er
has
given
consent
for
also
making
the
information
accessible
the
public
.
accordance
with
this
Regulation
.
That
information
shall
include
the
names
and
contact
details
of
natural
persons
who
are
responsible
for
registeri
ng
the
syste
m
and
have
the
lega
l
author
ity
to
represent
the
provid
er
or
the
deplo
yer
,
as
applicable
.
The
Commission
shall
be
the
controller
of
the
EU
database
.
It
shall
make
available
to
providers
,
prospective
provider
s
and
deplo
yers
adequat
e
technical
and
administrative
support
.
The
EU
database
shall
comply
with
the
applicable
accessibility
requirements
.
CHAPTER
IX
POST
-MARKET
MONITORING
,
INFORMA
TION
SHARING
AND
MARKET
SUR
VEILL
ANCE
SECTION
1
Post
-
market
monitor
ing
Article
72
Post
-
mark
et
monitorin
g
by
providers
and
pos
t
-
mark
et
monitor
ing
plan
for
higher
isk
AI
systems
nature
of
the
AI
technologies
and
the
risks
of
the
higher
isk
AI
system
.
which
may
be
provided
by
deplo
yers
or
which
may
be
collected
through
other
sources
on
the
perform
ance
of
higher
isk
AI
systems
throughout
their
lifetime
,
and
which
allow
the
provider
to
evaluate
the
continuous
complia
nce
of
AI
systems
with
the
requirements
set
out
in
Chap
ter
III
,
Section
2
.
Where
relevant
,
post
-
mark
et
monitoring
shall
include
an
analysis
of
the
interac
tion
with
other
AI
systems
.
This
obligati
on
shall
not
cover
sensitive
operational
data
of
deplo
yers
which
are
law
-
enf
orcement
authorities
.
plan
shall
be
part
of
the
technical
documentation
referred
to
in
Annex
IV
.
The
Commission
shall
adopt
an
imp
lementing
act
laying
down
detailed
provisions
establishing
a
templat
e
for
the
post
-
mark
et
monitoring
plan
and
the
list
of
elements
to
be
included
in
the
plan
by
2
Febr
uary
2026
.
That
imp
lementing
act
shall
be
adop
ted
in
accordance
with
the
examination
procedure
referred
to
in
Article
98(2
)
.
a
post
-
market
monitoring
system
and
plan
are
already
established
under
that
legislation
,
in
order
to
ensure
consiste
ncy
,
avoid
duplications
and
minimise
additional
burdens
,
provid
ers
shall
have
a
choice
of
integrat
ing
,
as
appropriate
,
the
necessary
elements
descr
ibed
in
paragraphs
1
,
2
and
3
using
the
templat
e
refer
red
in
paragraph
3
into
systems
and
plans
already
existing
under
that
legislation
,
provid
ed
that
it
achieves
an
equivalent
level
of
protect
ion
.
The
first
subparagraph
of
this
paragraph
shall
also
apply
to
higher
isk
AI
systems
referred
to
in
point
5
of
Annex
III
placed
on
the
market
or
put
into
service
by
financ
ial
institutions
that
are
subject
to
requirements
under
Union
financ
ial
services
law
regarding
their
inter
nal
gove
rnance
,
arrang
ements
or
processes
.
SECTION
2
Shar
ing
of
information
on
serious
incidents
Article
73
Repor
ting
of
serious
incidents
surveillance
authorities
of
the
Member
States
where
that
incident
occur
red
.
.
The
repor
t
referred
to
in
paragraph
1
shall
be
made
immediate
ly
after
the
provider
has
established
a
causal
link
between
the
AI
system
and
the
serious
incident
or
the
reasonable
likelihood
of
such
a
link
,
and
,
in
any
event
,
not
later
than
The
period
for
the
repor
ting
refer
red
to
in
the
first
subparagraph
shall
take
account
of
the
sever
ity
of
the
serious
incident
.
defined
in
Article
3
,
point(b
)
,
the
repor
t
referred
to
in
paragraph
1
of
this
Article
shall
be
provid
ed
immediately
,
and
not
later
than
two
days
after
the
provider
or
,
where
applicable
,
the
deplo
yer
becomes
aware
of
that
incident
.
provid
er
or
the
deplo
yer
has
established
,
or
as
soon
as
it
suspects
,
a
causal
relationship
between
the
higher
isk
AI
syste
m
and
the
serious
incident
,
but
not
later
than
10
days
after
the
date
on
which
the
provider
or
,
where
applicable
,
the
deplo
yer
becomes
aware
of
the
serious
incident
.
repor
t
that
is
incom
plete
,
follo
we
d
by
a
comp
lete
repor
t.
necessary
investigations
in
relation
to
the
serious
incident
and
the
AI
syste
m
concer
ned
.
This
shall
include
a
risk
assessment
of
the
incident
,
and
corrective
action
.
The
provider
shall
cooperate
with
the
competent
authorities
,
and
where
relevant
with
the
notif
ied
body
concer
ned
,
during
the
invest
igations
refer
red
to
in
the
first
subparagraph
,
and
shall
not
perfo
rm
any
investig
ation
which
involves
altering
the
AI
syste
m
concer
ned
in
a
way
which
may
affect
any
subsequent
evaluation
of
the
causes
of
the
incident
,
prior
to
informing
the
comp
etent
authorities
of
such
action
.
surveillance
author
ity
shall
inform
the
national
public
authorities
or
bodies
referred
to
in
Article
77(1
)
.
The
Commission
shall
develop
dedicated
guidance
to
facilitate
compliance
with
the
obligations
set
out
in
paragraph
1
of
this
Article
.
That
guidance
shall
be
issued
by
2
August
2025
,
and
shall
be
assessed
regularly
.
,
within
seven
days
from
the
date
it
received
the
notif
ication
refer
red
to
in
paragraph
1
of
this
Article
,
and
shall
follo
w
the
notification
procedures
as
provided
in
that
Regulation
.
subject
to
Union
legislative
instr
uments
laying
down
repor
ting
obligati
ons
equivalent
to
those
set
out
in
this
Regulation
,
the
notif
ication
of
serious
incidents
shall
be
limit
ed
to
those
referred
to
in
Article
3
,
point(c
)
.
point(c
)
of
this
Regulation
,
and
shall
be
made
to
the
national
competent
author
ity
chosen
for
that
purpose
by
the
Member
States
where
the
incident
occur
red
.
they
have
take
n
action
on
it
,
in
accordance
with
Article
20
of
Regulation
(
EU
)
.
SECTION
3
Enforcement
Article
74
market
surveillance
and
control
of
AI
systems
in
the
Union
mark
et
enforcement
of
this
Regulation
:
reference
to
an
economic
operator
under
Regulation
(
EU
)
shall
be
understood
as
including
all
operators
identifie
d
in
Article
2(1
)
of
this
Regulation
;
(
b)any
reference
to
a
product
under
Regulation
(
EU
)
shall
be
understood
as
including
all
AI
systems
falling
within
the
scope
of
this
Regulation
.
authorities
shall
repor
t
annually
to
the
Commission
and
relevant
national
competition
authorities
any
information
identifie
d
in
the
course
of
market
surveillance
activities
that
may
be
of
potential
interest
for
the
application
of
Union
law
on
competition
rules
.
They
shall
also
annually
repor
t
to
the
Commission
about
the
use
of
prohibite
d
practices
that
occur
red
during
that
year
and
about
the
measures
take
n.
Annex
I
,
the
market
surveillance
author
ity
for
the
purposes
of
this
Regulation
shall
be
the
author
ity
responsible
for
market
surveillance
activities
designated
under
those
legal
acts
.
By
derogat
ion
from
the
first
subparagraph
,
and
in
appropriate
circumstances
,
Member
States
may
designate
another
relevant
author
ity
to
act
as
a
market
surveillance
author
ity
,
provid
ed
they
ensure
coordination
with
the
relevant
sectoral
market
surveillance
authorities
responsible
for
the
enforcement
of
the
Union
harmonisation
legislation
listed
in
Annex
I.
covered
by
the
Union
harmonisation
legislation
listed
in
section
A
of
Annex
I
,
where
such
lega
l
acts
already
provide
for
procedures
ensur
ing
an
equivalent
level
of
protection
and
having
the
same
objective
.
In
such
cases
,
the
relevant
sectoral
procedures
shall
apply
instead
.
for
the
purpose
of
ensur
ing
the
effective
enforcement
of
this
Regulation
,
market
surveillance
authorities
may
exercise
the
powe
rs
referred
to
in
Article
14(4
)
,
points
(
d
)
and
(
j
)
,
of
that
Regulation
remotely
,
as
appropriate
.
financ
ial
services
law
,
the
market
surveillance
author
ity
for
the
purposes
of
this
Regulation
shall
be
the
relevant
national
author
ity
responsible
for
the
financ
ial
super
vision
of
those
institutions
under
that
legislation
in
so
far
as
the
placing
on
the
market
,
putting
into
service
,
or
the
use
of
the
AI
system
is
in
direct
connection
with
the
provision
of
those
financial
services
.
another
relevant
author
ity
may
be
identified
by
the
Member
State
as
market
surveillance
author
ity
for
the
purposes
of
this
Regulation
.
National
market
surveillance
authorities
super
vising
regulated
credit
institutions
regulate
d
under
Directive
/EU
,
which
are
participating
in
the
Sing
le
supervisory
Mec
hanism
established
by
Regulation
(
EU
)
No
,
should
repor
t
,
without
dela
y
,
to
the
European
Central
Bank
any
information
identified
in
the
course
of
their
market
surveillance
activities
that
may
be
of
potent
ial
interest
for
the
prudential
supervisory
tasks
of
the
European
Central
Bank
specified
in
that
Regulation
.
enforcement
purposes
,
border
management
and
justice
and
democracy
,
and
for
higher
isk
AI
systems
listed
in
points
6
,
7
and
8
of
Annex
III
to
this
Regulation
,
Member
States
shall
designate
as
market
surveillance
authorities
for
the
purposes
of
this
Regulation
either
the
comp
etent
data
protection
supervisory
authorities
under
Regulation
(
EU
)
or
Directive
(
EU
)
,
or
any
other
author
ity
designate
d
pursuant
to
the
same
conditions
laid
down
in
Articles
41
to
44
of
Directive
(
EU
)
.
Market
surveillance
activities
shall
in
no
way
affect
the
independence
of
judicial
authorities
,
or
other
wise
interfere
with
their
activities
when
acting
in
their
judicial
capacity
.
Protection
Super
visor
shall
act
as
their
market
surveillance
author
ity
,
excep
t
in
relation
to
the
Cour
t
of
Justice
of
the
European
Union
acting
in
its
judicial
capacity
.
and
other
relevant
national
authorities
or
bodies
which
super
vise
the
application
of
Union
harmonisation
legislation
listed
in
Annex
I
,
or
in
other
Union
law
,
that
might
be
relevant
for
the
higher
isk
AI
systems
refer
red
to
in
Annex
III
.
.
Market
surveillance
authorities
and
the
Commission
shall
be
able
to
propose
joint
activities
,
including
joint
invest
igations
,
to
be
conducted
by
either
market
surveillance
authorities
or
mark
et
surveillance
authorities
jointly
with
the
Commission
,
that
have
the
aim
of
promoting
compliance
,
identifying
non
-
compliance
,
raising
awareness
or
providing
guidance
in
relation
to
this
Regulation
with
respect
to
specific
cate
gories
of
higher
isk
AI
systems
that
are
found
to
present
a
serious
risk
across
two
or
more
Member
States
in
accordance
with
Article
9
of
Regulation
(
EU
)
.
The
AI
Office
shall
provid
e
coordination
support
for
joint
investig
ations
.
what
is
necessary
to
fulfil
their
tasks
,
the
market
surveillance
authorities
shall
be
granted
full
access
by
provid
ers
to
the
documentation
as
well
as
the
training
,
validation
and
testing
data
sets
used
for
the
development
of
higher
isk
AI
systems
,
including
,
where
appropriate
and
subject
to
security
safeguards
,
through
application
programming
interfaces
(
API
)
or
other
relevant
technical
means
and
tools
enabling
remot
e
access
.
request
and
only
when
both
of
the
following
conditions
are
fulfilled
:
(
a)access
to
source
code
is
necessary
to
assess
the
conf
ormity
of
a
higher
isk
AI
syste
m
with
the
requirements
set
out
in
Chapt
er
III
,
Section
2
;
and
(
b)testing
or
auditing
procedures
and
verificati
ons
based
on
the
data
and
documentation
provided
by
the
provider
have
been
exhaust
ed
or
proved
insuff
icient
.
the
confidentiality
obligations
set
out
in
Article
78
.
Article
75
Mutual
assis
tance
,
market
surveillance
and
control
of
general
-
pur
pose
AI
systems
same
provider
,
the
AI
Office
shall
have
powers
to
monitor
and
super
vise
compliance
of
that
AI
syste
m
with
obligations
under
this
Regulation
.
To
carry
out
its
monitoring
and
super
vision
tasks
,
the
AI
Offi
ce
shall
have
all
the
powers
of
a
market
surveillance
author
ity
provided
for
in
this
Section
and
Regulation
(
EU
)
.
can
be
used
directly
by
deplo
yers
for
at
least
one
purpose
that
is
classif
ied
as
higher
isk
pursuant
to
this
Regulation
to
be
non
-
comp
liant
with
the
requirements
laid
down
in
this
Regulation
,
they
shall
cooperate
with
the
AI
Office
to
carry
out
complia
nce
evaluations
,
and
shall
inform
the
Board
and
other
market
surveillance
authorities
according
ly
.
inability
to
access
certain
information
related
to
the
general
-
pur
pose
AI
model
despite
having
made
all
appropriate
efforts
to
obtain
that
information
,
it
may
submit
a
reasoned
request
to
the
AI
Office
,
by
which
access
to
that
information
shall
be
enforced
.
In
that
case
,
the
AI
Office
shall
supply
to
the
applicant
author
ity
without
dela
y
,
and
in
any
event
within
30
days
,
any
information
that
the
AI
Office
considers
to
be
relevant
in
order
to
establish
whether
a
higher
isk
AI
system
is
non
-
comp
liant
.
market
surveillance
authorities
shall
safegua
rd
the
confidentiality
of
the
information
that
they
obtain
in
accordance
with
Article
78
of
this
Regulation
.
The
procedure
provided
for
in
Chapt
er
VI
of
Regulation
(
EU
)
shall
apply
mutatis
mutandis
.
Article
76
Super
vision
of
testing
in
real
world
conditions
by
market
surveillance
authorities
accordance
with
this
Regulation
.
Where
testing
in
real
world
conditions
is
conducte
d
for
AI
systems
that
are
super
vised
within
an
AI
regulator
y
sandbo
x
under
Article
58
,
the
market
surveillance
authorities
shall
verify
the
compliance
with
Article
60
as
part
of
their
supervisory
role
for
the
AI
regulator
y
sandbo
x.
Those
authorities
may
,
as
appropriate
,
allow
the
testing
in
real
world
conditions
to
be
conducte
d
by
the
provid
er
or
prospective
provid
er
,
in
deroga
tion
from
the
conditions
set
out
in
Article
of
a
serious
incident
or
has
other
grounds
for
consider
ing
that
the
conditions
set
out
in
Articles
60
and
61
are
not
met
,
it
may
take
either
of
the
following
decisions
on
its
territory
,
as
appropriate
:
(
a)to
suspend
or
terminate
the
testing
in
real
world
conditions
;
(
b)to
require
the
provid
er
or
prospective
provider
and
the
deplo
yer
or
prospective
deplo
yer
to
modify
any
aspect
of
the
testing
in
real
world
conditions
.
objection
within
the
meaning
of
Article
60(4
)
,
point
(
b
)
,
the
decision
or
the
objection
shall
indicate
the
grounds
theref
or
and
how
the
provid
er
or
prospective
provider
can
challeng
e
the
decision
or
objection
.
communicate
the
grounds
theref
or
to
the
market
surveillance
authorities
of
other
Member
States
in
which
the
AI
system
has
been
tested
in
accordance
with
the
testing
plan
.
Article
77
Powers
of
authorities
protecting
fundamental
rights
protect
ing
fundamental
rights
,
including
the
right
to
non
-
discr
imination
,
in
relation
to
the
use
of
higher
isk
AI
systems
referred
to
in
Annex
III
shall
have
the
power
to
request
and
access
any
documentation
create
d
or
maintained
under
this
Regulation
in
accessible
language
and
format
when
access
to
that
documentation
is
necessary
for
effectively
fulfilling
their
mandates
within
the
limits
of
their
jurisdiction
.
The
relevant
public
author
ity
or
body
shall
inform
the
market
surveillance
author
ity
of
the
Member
State
concer
ned
of
any
such
request
.
make
a
list
of
them
publicly
available
.
Member
States
shall
notify
the
list
to
the
Commission
and
to
the
other
Member
States
,
and
shall
keep
the
list
up
to
date
.
obligations
under
Union
law
prot
ecting
fundamental
rights
has
occur
red
,
the
public
author
ity
or
body
refer
red
to
in
paragraph
1
may
make
a
reasoned
request
to
the
market
surveillance
author
ity
,
to
organise
testing
of
the
higher
isk
AI
syste
m
through
technical
means
.
The
market
surveillance
author
ity
shall
organise
the
testing
with
the
close
involvement
of
the
requesting
public
author
ity
or
body
within
a
reasonable
time
follo
wing
the
request
.
this
Article
pursuant
to
this
Article
shall
be
treated
in
accordance
with
the
confidentiality
obligations
set
out
in
Article
78
.
Article
78
Conf
identiality
in
the
application
of
this
Regulation
shall
,
in
accordance
with
Union
or
national
law
,
respect
the
confi
dentiality
of
information
and
data
obtained
in
carrying
out
their
tasks
and
activities
in
such
a
manner
as
to
prot
ect
,
in
particular
:
(
a)the
intellectual
proper
ty
rights
and
confidential
business
information
or
trade
secrets
of
a
natural
or
lega
l
person
,
including
source
code
,
excep
t
in
the
cases
referred
to
in
Article
5
of
Directive
(
EU
)
of
the
European
Parliame
nt
and
of
the
Council
;
(
b)the
effective
imp
lementation
of
this
Regulation
,
in
particular
for
the
purposes
of
inspections
,
investig
ations
or
audits
;
(
c)public
and
national
security
interests
;
(
d)the
conduct
of
criminal
or
administrative
proceedings
;
(
e)information
classif
ied
pursuant
to
Union
or
national
law
.
strictly
necessary
for
the
assessment
of
the
risk
posed
by
AI
systems
and
for
the
exercise
of
their
powers
in
accordance
with
this
Regulation
and
with
Regulation
(
EU
)
.
They
shall
put
in
place
adequate
and
effective
cybersecurity
measures
to
protect
the
security
and
conf
identiality
of
the
information
and
data
obtained
,
and
shall
delete
the
data
collected
as
soon
as
it
is
no
longe
r
needed
for
the
purpose
for
which
it
was
obtained
,
in
accordance
with
applicable
Union
or
national
law
.
competent
authorities
or
between
national
competent
authorities
and
the
Commission
shall
not
be
disclosed
without
prior
consultation
of
the
originating
national
competent
author
ity
and
the
deplo
yer
when
higher
isk
AI
systems
referred
to
in
point
1
,
6
or
7
of
Annex
III
are
used
by
law
enforcement
,
border
control
,
immigration
or
asylum
authorities
and
when
such
disclosure
would
jeopardise
public
and
national
security
interests
.
This
exchange
of
information
shall
not
cover
sensitive
operational
data
in
relation
to
the
activities
of
law
enforcement
,
border
control
,
immigration
or
asylum
authorities
.
When
the
law
enforcement
,
immigration
or
asylum
authorities
are
provid
ers
of
higher
isk
AI
systems
referred
to
in
point
1
,
authorities
.
Those
authorities
shall
ensure
that
the
mark
et
surveillance
authorities
referred
to
in
Article
74(8
)
and
,
as
applicable
,
can
,
upon
request
,
immediate
ly
access
the
documentation
or
obtain
a
copy
thereof
.
Only
staff
of
the
market
surveillance
author
ity
holding
the
appropriate
level
of
security
clearance
shall
be
allowed
to
access
that
documentation
or
any
copy
thereof
.
authorities
,
as
well
as
those
of
notif
ied
bodies
,
with
regard
to
the
exchang
e
of
information
and
the
dissemination
of
warnings
,
including
in
the
cont
ext
of
cross
-
border
cooperation
,
nor
shall
they
affect
the
oblig
ations
of
the
parties
concer
ned
to
provid
e
information
under
criminal
law
of
the
Member
States
.
inter
national
and
trade
agreements
,
confi
dential
information
with
regulator
y
authorities
of
third
countr
ies
with
which
they
have
concluded
bilat
eral
or
multilate
ral
conf
identiality
arrang
ements
guaranteeing
an
adequate
level
of
confidentiality
.
Article
79
Procedure
at
national
level
for
dealing
with
AI
systems
presenting
a
risk
Regulation
(
EU
)
,
in
so
far
as
they
present
risks
to
the
health
or
safety
,
or
to
fundamental
rights
,
of
persons
.
a
risk
as
refer
red
to
in
paragraph
1
of
this
Article
,
it
shall
carry
out
an
evaluation
of
the
AI
syste
m
concer
ned
in
respect
of
its
compliance
with
all
the
requirements
and
obligations
laid
down
in
this
Regulation
.
Particular
attention
shall
be
given
to
AI
systems
presenting
a
risk
to
vulnerable
groups
.
Where
risks
to
fundamental
rights
are
identified
,
the
market
surveillance
author
ity
shall
also
inform
and
fully
cooperate
with
the
relevant
national
public
authorities
or
bodies
referred
to
in
Article
national
public
authorities
or
bodies
referred
to
in
Article
77(1
)
.
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
8
June
2016
on
the
protection
of
undisclosed
know
-
ho
w
and
business
information
(
trade
secrets
)
against
their
unlaw
ful
acquisition
,
use
and
disclosure
(
6.2016
,
p.
1).Where
,
in
the
course
of
that
evaluation
,
the
market
surveillance
author
ity
or
,
where
applicable
the
market
surveillance
author
ity
in
cooperation
with
the
national
public
author
ity
referred
to
in
Article
77(1
)
,
finds
that
the
AI
system
does
not
comply
with
the
requirements
and
obligations
laid
down
in
this
Regulation
,
it
shall
without
undue
dela
y
require
the
relevant
operato
r
to
take
all
appropriate
corrective
actions
to
bring
the
AI
syste
m
into
compliance
,
to
withdra
w
the
AI
system
from
the
mark
et
,
or
to
recall
it
within
a
period
the
market
surveillance
author
ity
may
prescr
ibe
,
and
in
any
event
within
the
shor
ter
of
15
working
days
,
or
as
provided
for
in
the
relevant
Union
harmonisation
legislation
.
The
market
surveillance
author
ity
shall
inform
the
relevant
notif
ied
body
according
ly
.
Article
18
of
Regulation
(
EU
)
shall
apply
to
the
measures
referred
to
in
the
second
subparagraph
of
this
paragraph
.
shall
inform
the
Commission
and
the
other
Member
States
without
undue
dela
y
of
the
results
of
the
evaluation
and
of
the
actions
which
it
has
required
the
operato
r
to
take
.
it
has
made
available
on
the
Union
market
.
paragraph
2
,
the
market
surveillance
author
ity
shall
take
all
appropriate
provisional
measures
to
prohibit
or
restr
ict
the
AI
syste
m
’s
being
made
available
on
its
national
market
or
put
into
service
,
to
withdra
w
the
product
or
the
standalone
AI
syste
m
from
that
market
or
to
recall
it
.
That
author
ity
shall
without
undue
dela
y
notify
the
Commission
and
the
other
Member
States
of
those
measures
.
for
the
identification
of
the
non
-
comp
liant
AI
syste
m
,
the
origin
of
the
AI
syste
m
and
the
supply
chain
,
the
nature
of
the
non
-
compliance
alleged
and
the
risk
involved
,
the
nature
and
duration
of
the
national
measures
taken
and
the
arguments
put
forward
by
the
relevant
operat
or
.
In
particular
,
the
market
surveillance
authorities
shall
indicate
whether
the
non
-
compliance
is
due
to
one
or
more
of
the
follo
wing
:
(
a)non
-
compliance
with
the
prohibition
of
the
AI
practices
referred
to
in
Article
5
;
(
b)a
failure
of
a
higher
isk
AI
system
to
meet
requirements
set
out
in
Chapt
er
III
,
Section
2
;
(
c)shor
tcomings
in
the
harmonised
standards
or
common
specifications
refer
red
to
in
Articles
40
and
41
confer
ring
a
presump
tion
of
conf
ormity
;
(
d)non
-
compliance
with
Article
50
.
procedure
shall
,
without
undue
dela
y
,
inform
the
Commission
and
the
other
Member
States
of
any
measures
adopt
ed
and
of
any
additional
information
at
their
disposal
relating
to
the
non
-
compliance
of
the
AI
system
concer
ned
,
and
,
in
the
event
of
disagreement
with
the
notif
ied
national
measure
,
of
their
objections
.
been
raised
by
either
a
market
surveillance
author
ity
of
a
Member
State
or
by
the
Commission
in
respect
of
a
provis
ional
measure
taken
by
a
market
surveillance
author
ity
of
another
Member
State
,
that
measure
shall
be
deemed
justifi
ed
.
This
shall
be
without
prejudice
to
the
procedural
rights
of
the
concer
ned
operator
in
accordance
with
Article
18
of
Regulation
(
EU
)
.
The
three
-
month
period
referred
to
in
this
paragraph
shall
be
reduced
to
30
days
in
the
event
of
non
-
compliance
with
the
prohibition
of
the
AI
practices
referred
to
in
Article
5
of
this
Regulation
.
product
or
the
AI
syste
m
concer
ned
,
such
as
withdra
wal
of
the
product
or
the
AI
syste
m
from
their
market
,
without
undue
dela
y.
Article
80
Procedure
for
dealing
with
AI
systems
classif
ied
by
the
provider
as
non
-
higher
isk
in
application
of
Annex
III
non
-
higher
isk
pursuant
to
Article
6(3
)
is
indeed
higher
isk
,
the
mark
et
surveillance
author
ity
shall
carry
out
an
evaluation
of
the
AI
system
concer
ned
in
respect
of
its
classific
ation
as
a
higher
isk
AI
syste
m
based
on
the
conditions
set
out
in
Article
.
Where
,
in
the
course
of
that
evaluation
,
the
market
surveillance
author
ity
finds
that
the
AI
syste
m
concer
ned
is
higher
isk
,
it
shall
without
undue
dela
y
require
the
relevant
provider
to
take
all
necessary
actions
to
bring
the
AI
syste
m
into
complia
nce
with
the
requirements
and
obligations
laid
down
in
this
Regulation
,
as
well
as
take
appropriate
corrective
action
within
a
period
the
market
surveillance
author
ity
may
prescr
ibe
.
national
territory
,
it
shall
inform
the
Commission
and
the
other
Member
States
without
undue
dela
y
of
the
results
of
the
evaluation
and
of
the
actions
which
it
has
required
the
provider
to
take
.
requirements
and
obligations
laid
down
in
this
Regulation
.
Where
the
provider
of
an
AI
system
concer
ned
does
not
bring
the
AI
syste
m
into
compliance
with
those
requirements
and
obligati
ons
within
the
period
referred
to
in
paragraph
2
of
this
Article
,
the
provid
er
shall
be
subject
to
fines
in
accordance
with
Article
99
.
it
has
made
available
on
the
Union
market
.
in
paragraph
2
of
this
Article
,
Article
79(5
)
to
shall
apply
.
establishes
that
the
AI
system
was
misclassified
by
the
provider
as
non
-
higher
isk
in
order
to
circum
vent
the
application
of
requirements
in
Chapt
er
III
,
Section
2
,
the
provid
er
shall
be
subject
to
fines
in
accordance
with
Article
99
.
information
stored
in
the
EU
database
referred
to
in
Article
71
of
this
Regulation
.
Article
81
Union
safeguard
procedure
non
-
compliance
with
the
prohibition
of
the
AI
practices
refer
red
to
in
Article
5
,
objections
are
raised
by
the
market
surveillance
author
ity
of
a
Member
State
to
a
measure
taken
by
another
market
surveillance
author
ity
,
or
where
the
Commission
considers
the
measure
to
be
contrar
y
to
Union
law
,
the
Commission
shall
without
undue
dela
y
enter
into
consultation
with
the
market
surveillance
author
ity
of
the
relevant
Member
State
and
the
operat
or
or
operators
,
and
shall
evaluate
the
national
measure
.
On
the
basis
of
the
results
of
that
evaluation
,
the
Commission
shall
,
within
six
months
,
or
within
60
days
in
the
case
of
non
-
compliance
with
the
prohibition
of
the
AI
practices
referred
to
in
Article
5
,
starting
from
the
notification
referred
to
in
Article
79(5
)
,
decide
whether
the
national
measure
is
justified
and
shall
notify
its
decision
to
the
market
surveillance
author
ity
of
the
Member
State
concer
ned
.
The
Commission
shall
also
inform
all
other
market
surveillance
authorities
of
its
decision
.
shall
ensure
that
they
take
appropriate
restr
ictive
measures
in
respect
of
the
AI
syste
m
concer
ned
,
such
as
requir
ing
the
withdra
wal
of
the
AI
system
from
their
mark
et
without
undue
dela
y
,
and
shall
inform
the
Commission
according
ly
.
Where
the
Commission
considers
the
national
measure
to
be
unjustified
,
the
Member
State
concer
ned
shall
withdra
w
the
measure
and
shall
inform
the
Commission
according
ly
.
shor
tcomings
in
the
harmonised
standards
or
common
specific
ations
refer
red
to
in
Articles
40
and
41
of
this
Regulation
,
the
Commission
shall
apply
the
procedure
provided
for
in
Article
11
of
Regulation
(
EU
)
No
.
Article
82
Compliant
AI
systems
which
present
a
risk
referred
to
in
Article
77(1
)
,
the
mark
et
surveillance
author
ity
of
a
Member
State
finds
that
although
a
higher
isk
AI
system
complie
s
with
this
Regulation
,
it
never
theless
presents
a
risk
to
the
health
or
safety
of
persons
,
to
fundamental
rights
,
or
to
other
aspects
of
public
interest
protection
,
it
shall
require
the
relevant
operato
r
to
take
all
appropriate
measures
to
ensure
that
the
AI
system
concer
ned
,
when
placed
on
the
market
or
put
into
service
,
no
longer
presents
that
risk
without
undue
dela
y
,
within
a
period
it
may
prescr
ibe
.
The
provider
or
other
relevant
operato
r
shall
ensure
that
corrective
action
is
taken
in
respect
of
all
the
AI
systems
concer
ned
that
it
has
made
available
on
the
Union
market
within
the
timeline
prescr
ibed
by
the
market
surveillance
author
ity
of
the
Member
State
refer
red
to
in
paragraph
1
.
paragraph
1
.
That
information
shall
include
all
available
details
,
in
particular
the
data
necessary
for
the
identifica
tion
of
the
AI
syste
m
concer
ned
,
the
origin
and
the
supply
chain
of
the
AI
system
,
the
nature
of
the
risk
involved
and
the
nature
and
duration
of
the
national
measures
take
n.
relevant
operators
,
and
shall
evaluate
the
national
measures
taken
.
On
the
basis
of
the
results
of
that
evaluation
,
the
Commission
shall
decide
whether
the
measure
is
justified
and
,
where
necessary
,
propose
other
appropriate
measures
.
operators
.
It
shall
also
inform
the
other
Member
States
.
Article
83
Formal
non
-
compliance
relevant
provid
er
to
put
an
end
to
the
non
-
compliance
concer
ned
,
within
a
period
it
may
prescr
ibe
:
(
a)the
CE
marking
has
been
affixed
in
violation
of
Article
48
;
(
b)the
CE
marking
has
not
been
affixed
;
(
c)the
EU
declaration
of
conf
ormity
referred
to
in
Article
47
has
not
been
drawn
up
;
(
d)the
EU
declaration
of
conf
ormity
referred
to
in
Article
47
has
not
been
drawn
up
correctly
;
(
e)the
registration
in
the
EU
database
referred
to
in
Article
71
has
not
been
carried
out
;
(
f)where
applicable
,
no
authorised
representative
has
been
appointed
;
(
g)technical
documentation
is
not
available
.
concer
ned
shall
take
appropriate
and
propor
tionate
measures
to
restr
ict
or
prohibit
the
higher
isk
AI
system
being
made
available
on
the
market
or
to
ensure
that
it
is
recalled
or
withdrawn
from
the
mark
et
without
dela
y.
Article
84
Union
AI
testing
support
structures
Article
21(6
)
of
Regulation
(
EU
)
in
the
area
of
AI
.
independent
technical
or
scientific
advice
at
the
request
of
the
Board
,
the
Commission
,
or
of
market
surveillance
authorities
.
Remedies
Article
85
Right
to
lodge
a
complaint
with
a
market
surveillance
author
ity
Without
prejudice
to
other
administrative
or
judicial
remedies
,
any
natural
or
legal
person
having
grounds
to
consider
that
there
has
been
an
infringement
of
the
provisions
of
this
Regulation
may
submit
complaints
to
the
relevant
market
surveillance
author
ity
.
In
accordance
with
Regulation
(
EU
)
,
such
complaints
shall
be
take
n
into
account
for
the
purpose
of
conducting
market
surveillance
activities
,
and
shall
be
handled
in
line
with
the
dedicat
ed
procedures
established
theref
or
by
the
market
surveillance
authorities
.
Article
86
Right
to
explanation
of
individual
decision
-
making
syste
m
listed
in
Annex
III
,
with
the
exception
of
systems
listed
under
point
2
thereof
,
and
which
produces
lega
l
effects
or
similarly
signifi
cantly
affects
that
person
in
a
way
that
they
consider
to
have
an
adverse
impact
on
their
health
,
safety
or
fundamental
rights
shall
have
the
right
to
obtain
from
the
deplo
yer
clear
and
meaningful
explanations
of
the
role
of
the
AI
syste
m
in
the
decision
-
making
procedure
and
the
main
elements
of
the
decision
take
n.
under
that
paragraph
follow
from
Union
or
national
law
in
compliance
with
Union
law
.
Union
law
.
Article
87
Repor
ting
of
infr
ingements
and
protection
of
repor
ting
persons
Directive
(
EU
)
shall
apply
to
the
repor
ting
of
infringem
ents
of
this
Regulation
and
the
protection
of
persons
repor
ting
such
infringem
ents
.
SECTION
5
Super
vision
,
investig
ation
,
enforcement
and
monitor
ing
in
respect
of
providers
of
gener
al
-
pur
pose
AI
models
Article
88
Enforcement
of
the
obligations
of
providers
of
general
-
pur
pose
AI
models
guarantees
under
Article
94
.
The
Commission
shall
entr
ust
the
imp
lementation
of
these
tasks
to
the
AI
Office
,
without
prejudice
to
the
powers
of
organisation
of
the
Commission
and
the
division
of
compet
ences
between
Member
States
and
the
Union
based
on
the
Treaties
.
powe
rs
laid
down
in
this
Section
,
where
that
is
necessary
and
propor
tionate
to
assist
with
the
fulfilment
of
their
tasks
under
this
Regulation
.
Monitor
ing
actions
actions
to
monitor
the
effective
implementation
and
compliance
with
this
Regulation
by
provid
ers
of
general
-
pur
pose
AI
models
,
including
their
adherence
to
approve
d
codes
of
practice
.
A
comp
laint
shall
be
duly
reasoned
and
indicate
at
least
:
(
a)the
point
of
contact
of
the
provid
er
of
the
general
-
pur
pose
AI
model
concer
ned
;
(
b)a
descr
iption
of
the
relevant
facts
,
the
provisions
of
this
Regulation
concer
ned
,
and
the
reason
why
the
downstream
provid
er
considers
that
the
provid
er
of
the
general
-
purpo
se
AI
model
concer
ned
infringed
this
Regulation
;
(
c)any
other
information
that
the
downstream
provider
that
sent
the
request
considers
relevant
,
including
,
where
appropriate
,
information
gathered
on
its
own
initiative
.
Article
90
Aler
ts
of
systemic
risks
by
the
scientif
ic
panel
the
powers
laid
down
in
this
Section
for
the
purpose
of
assessing
the
matt
er
.
The
AI
Office
shall
inform
the
Board
of
any
measure
according
to
Articles
91
to
94
.
gathered
on
its
own
initiative
.
Article
91
Power
to
request
document
ation
and
information
drawn
up
by
the
provider
in
accordance
with
Articles
53
and
55
,
or
any
additional
information
that
is
necessary
for
the
purpose
of
assessing
compliance
of
the
provider
with
this
Regulation
.
general
-
pur
pose
AI
model
.
a
provider
of
a
general
-
pur
pose
AI
model
,
where
the
access
to
information
is
necessary
and
propor
tionate
for
the
fulfilment
of
the
tasks
of
the
scientific
panel
under
Article
68(2
)
.
.
The
request
for
information
shall
state
the
lega
l
basis
and
the
purpose
of
the
request
,
specify
what
information
is
required
,
set
a
period
within
which
the
information
is
to
be
provid
ed
,
and
indicate
the
fines
provided
for
in
Article
101
for
supplying
incor
rect
,
incom
plete
or
misleading
information
.
In
the
case
of
lega
l
persons
,
comp
anies
or
firms
,
or
where
the
provider
has
no
lega
l
personality
,
the
persons
authorised
to
represent
them
by
law
or
by
their
statutes
,
shall
supply
the
information
requested
on
behalf
of
the
provider
of
the
general
-
pur
pose
AI
model
concer
ned
.
Lawy
ers
duly
authorised
to
act
may
supply
information
on
behalf
of
their
clients
.
The
clients
shall
never
theless
remain
fully
responsible
if
the
information
supplied
is
incomplet
e
,
incor
rect
or
misleading
.
Article
92
Power
to
conduct
evaluations
to
Article
91
is
insufficient
;
or
(
b)to
investig
ate
syste
mic
risks
at
Union
level
of
general
-
pur
pose
AI
models
with
syste
mic
risk
,
in
particular
following
a
qualified
alert
from
the
scientific
panel
in
accordance
with
Article
90(1
)
,
point
(
a
)
.
scientific
panel
established
pursuant
to
Article
68
.
Independent
exper
ts
appointed
for
this
task
shall
meet
the
criteria
outlined
in
Article
68(2
)
.
through
APIs
or
further
appropriate
technical
means
and
tools
,
including
source
code
.
which
the
access
is
to
be
provided
,
and
the
fines
provided
for
in
Article
101
for
failure
to
provide
access
.
In
the
case
of
lega
l
persons
,
comp
anies
or
firms
,
or
where
the
provider
has
no
lega
l
personality
,
the
persons
authorised
to
represent
them
by
law
or
by
their
statute
s
,
shall
provide
the
access
request
ed
on
behalf
of
the
provider
of
the
general
-
pur
pose
AI
model
concer
ned
.
evaluations
,
including
the
detailed
arrang
ements
for
involving
independent
exper
ts
,
and
the
procedure
for
the
selection
thereof
.
Those
implementing
acts
shall
be
adop
ted
in
accordance
with
the
examination
procedure
referred
to
in
Article
with
the
provider
of
the
general
-
pur
pose
AI
model
to
gather
more
information
on
the
inter
nal
testing
of
the
model
,
intern
al
safeguards
for
preventing
systemic
risks
,
and
other
internal
procedures
and
measures
the
provid
er
has
taken
to
mitigat
e
such
risks
.
Article
93
Power
to
request
measures
lement
mitigation
measures
,
where
the
evaluation
carried
out
in
accordance
with
Article
92
has
given
rise
to
serious
and
substantiate
d
concer
n
of
a
syste
mic
risk
at
Union
level
;
(
c)restr
ict
the
making
available
on
the
market
,
withdra
w
or
recall
the
model
.
general
-
pur
pose
AI
model
.
syste
mic
risk
offers
commitments
to
implement
mitigation
measures
to
address
a
syste
mic
risk
at
Union
level
,
the
Commission
may
,
by
decision
,
make
those
commitments
binding
and
declare
that
there
are
no
further
grounds
for
action
.
Article
94
Procedural
rights
of
economic
operators
of
the
general
-
pur
pose
AI
model
Article
18
of
Regulation
(
EU
)
shall
apply
mutatis
mutandis
to
the
providers
of
the
general
-
pur
pose
AI
model
,
without
prejudice
to
more
specific
procedural
rights
provided
for
in
this
Regulation
.
CHAPTER
X
CODES
OF
CONDUCT
AND
GUIDELINES
Article
95
Codes
of
conduct
for
volunt
ary
application
of
specific
requirements
related
gover
nance
mechanisms
,
intended
to
foster
the
voluntar
y
application
to
AI
systems
,
other
than
higher
isk
AI
systems
,
of
some
or
all
of
the
requirements
set
out
in
Chap
ter
III
,
Section
2
taking
into
account
the
available
technical
solutions
and
industr
y
best
practices
allowi
ng
for
the
application
of
such
requirements
.
application
,
including
by
deplo
yers
,
of
specific
requirements
to
all
AI
systems
,
on
the
basis
of
clear
objectives
and
key
perfo
rmance
indicato
rs
to
measure
the
achievement
of
those
objectives
,
including
elements
such
as
,
but
not
limited
to
:
(
a)applicable
elements
provided
for
in
Union
ethical
guidelines
for
trustwo
rthy
AI
;
(
b)assessing
and
minimising
the
imp
act
of
AI
systems
on
environmental
sustainability
,
including
as
regards
energy
-
eff
icient
programming
and
techniques
for
the
efficient
design
,
training
and
use
of
AI
;
(
c)promoting
AI
literac
y
,
in
particular
that
of
persons
dealing
with
the
development
,
operation
and
use
of
AI
;
(
d)facilitating
an
inclusive
and
diverse
design
of
AI
systems
,
including
through
the
establishment
of
inclusive
and
diverse
development
teams
and
the
promotion
of
stak
eholders
’
participation
in
that
process
;
(
e)assessing
and
preventing
the
nega
tive
imp
act
of
AI
systems
on
vulnerable
persons
or
groups
of
vulnerable
persons
,
including
as
regards
accessibility
for
persons
with
a
disability
,
as
well
as
on
gender
equality
.
representing
them
or
by
both
,
including
with
the
involvement
of
any
intere
sted
stakeholders
and
their
representative
organisations
,
including
civil
society
organisations
and
academia
.
Codes
of
conduct
may
cover
one
or
more
AI
systems
taking
into
account
the
similar
ity
of
the
intende
d
purpose
of
the
relevant
systems
.
start
-
ups
,
when
encouraging
and
facilitating
the
drawing
up
of
codes
of
conduct
.
Guidelines
from
the
Commission
on
the
implementa
tion
of
this
Regulation
as
well
as
with
other
relevant
Union
law
,
including
as
regards
consistency
in
their
enforcement
;
(
f)the
application
of
the
definition
of
an
AI
system
as
set
out
in
Article
3
,
point
.
When
issuing
such
guidelines
,
the
Commission
shall
pay
particular
attention
to
the
needs
of
SMEs
including
start
-
ups
,
of
local
public
authorities
and
of
the
sectors
most
likely
to
be
affected
by
this
Regulation
.
The
guidelines
referred
to
in
the
first
subparagraph
of
this
paragraph
shall
take
due
account
of
the
generally
acknowledg
ed
state
of
the
art
on
AI
,
as
well
as
of
relevant
harmonised
standards
and
common
specifications
that
are
refer
red
to
in
Articles
40
and
41
,
or
of
those
harmonised
standards
or
technical
specific
ations
that
are
set
out
pursuant
to
Union
harmonisation
law
.
previously
adop
ted
when
deemed
necessary
.
CHAPTER
XI
DELEGA
TION
OF
POWER
AND
COMM
ITTEE
PROCEDURE
Article
97
Exe
rcise
of
the
delegation
and
,
Article
47(5
)
,
Article
51(3
)
,
Article
52(4
)
and
Article
53(5
)
and
shall
be
conferr
ed
on
the
Commission
for
a
period
of
five
years
from
1
August
2024
.
The
Commission
shall
draw
up
a
repor
t
in
respect
of
the
delegation
of
power
not
later
than
nine
months
before
the
end
of
the
five
-
year
period
.
The
deleg
ation
of
power
shall
be
tacitly
extende
d
for
periods
of
an
identical
duration
,
unless
the
European
Parliame
nt
or
the
Council
opposes
such
extension
not
later
than
three
months
before
the
end
of
each
period
.
Article
47(5
)
,
Article
51(3
)
,
Article
52(4
)
and
Article
53(5
)
and
may
be
revok
ed
at
any
time
by
the
European
Parliament
or
by
the
Council
.
A
decision
of
revoc
ation
shall
put
an
end
to
the
deleg
ation
of
power
specif
ied
in
that
decision
.
It
shall
take
effect
the
day
following
that
of
its
publication
in
the
of
the
European
Union
or
at
a
later
date
specified
therein
.
It
shall
not
affect
the
validity
of
any
delegat
ed
acts
already
in
force
.
with
the
principles
laid
down
in
the
Interinstitutional
Agreement
of
13
Apr
il
2016
on
Bett
er
Law
-
Making
.
As
soon
as
it
adop
ts
a
deleg
ated
act
,
the
Commission
shall
notify
it
simultaneously
to
the
European
Parliament
and
to
the
Council
.
Article
47(5
)
,
Article
51(3
)
,
Article
52(4
)
or
Article
53(5
)
or
shall
enter
into
force
only
if
no
objection
has
been
expressed
by
either
the
European
Parliame
nt
or
the
Council
within
a
period
of
three
months
of
notification
of
that
act
to
the
European
Parliament
and
the
Council
or
if
,
before
the
expir
y
of
that
period
,
the
European
Parliament
and
the
Council
have
both
informed
the
Commission
that
they
will
not
object
.
That
period
shall
be
extended
by
three
months
at
the
initiative
of
the
European
Parliame
nt
or
of
the
Council
.
Article
98
Committee
procedure
Regulation
(
EU
)
No
.
CHAPTER
XII
PENAL
TIES
Article
99
Penalties
penalties
and
other
enforcement
measures
,
which
may
also
include
warnings
and
non
-
monetar
y
measures
,
applicable
to
infringements
of
this
Regulation
by
operators
,
and
shall
take
all
measures
necessary
to
ensure
that
they
are
properly
and
effectively
implement
ed
,
thereby
taking
into
account
the
guidelines
issued
by
the
Commission
pursuant
to
Article
96
.
The
penalties
provided
for
shall
be
effective
,
propor
tionate
and
dissuasive
.
They
shall
take
into
account
the
interests
of
SMEs
,
including
start
-
ups
,
and
their
economic
viability
.
of
the
rules
on
penalties
and
of
other
enforcement
measures
referred
to
in
paragraph
1
,
and
shall
notify
it
,
without
dela
y
,
of
any
subsequent
amendment
to
them
.
fines
of
up
to
EUR
35
000
000
or
,
if
the
offender
is
an
under
taking
,
up
to
7
%
of
its
total
worldwide
annual
turnover
for
the
preceding
financial
year
,
whichever
is
higher
.
down
in
Articles
5
,
shall
be
subject
to
administrative
fines
of
up
to
EUR
15
000
000
or
,
if
the
offender
is
an
under
taking
,
up
to
3
%
of
its
total
worldwide
annual
turnover
for
the
preceding
financ
ial
year
,
whichever
is
higher
:
(
a)obligations
of
provid
ers
pursuant
to
Article
16
;
(
b)obligations
of
authorised
representatives
pursuant
to
Article
22
;
(
c)obligations
of
importers
pursuant
to
Article
23
;
(
d)obligations
of
distr
ibutors
pursuant
to
Article
24
;
(
e)obligations
of
deplo
yers
pursuant
to
Article
26
;
(
f)requirements
and
obligations
of
notified
bodies
pursuant
to
Article
31
,
Article
33(1
)
,
and
or
Article
34
;
(
g)transparency
obligations
for
provid
ers
and
deplo
yers
pursuant
to
Article
50
.
.
The
supply
of
incor
rect
,
incom
plete
or
misleading
information
to
notif
ied
bodies
or
national
competent
authorities
in
reply
to
a
request
shall
be
subject
to
administrative
fines
of
up
to
EUR
7
500
000
or
,
if
the
offender
is
an
under
taking
,
up
to
referred
to
in
paragraphs
3
,
4
and
5
,
whichever
thereof
is
lower
.
in
each
individual
case
,
all
relevant
circumstances
of
the
specific
situation
shall
be
take
n
into
account
and
,
as
appropriate
,
regard
shall
be
given
to
the
following
:
(
a)the
nature
,
gravity
and
duration
of
the
infringem
ent
and
of
its
consequences
,
taking
into
account
the
purpose
of
the
AI
syste
m
,
as
well
as
,
where
appropriate
,
the
number
of
affect
ed
persons
and
the
level
of
damagesuffer
ed
by
them
;
(
b)whether
administrative
fines
have
already
been
applied
by
other
market
surveillance
authorities
to
the
same
operator
for
the
same
infringem
ent
;
(
c)whether
administrative
fines
have
already
been
applied
by
other
authorities
to
the
same
operato
r
for
infringements
of
other
Union
or
national
law
,
when
such
infringements
result
from
the
same
activity
or
omission
constituting
a
relevant
infringement
of
this
Regulation
;
(
d)the
size
,
the
annual
turnover
and
market
share
of
the
operat
or
committing
the
infringement
;
(
e)any
other
aggravating
or
mitiga
ting
factor
applicable
to
the
circumstances
of
the
case
,
such
as
financ
ial
benefits
gained
,
or
losses
avoided
,
directly
or
indirectly
,
from
the
infringem
ent
;
(
f)the
degree
of
cooperation
with
the
national
competent
authorities
,
in
order
to
remedy
the
infringem
ent
and
mitigat
e
the
possible
adverse
effects
of
the
infringement
;
(
g)the
degree
of
responsibility
of
the
operator
taking
into
account
the
technical
and
organisational
measures
imp
lemented
by
it
;
(
h)the
manner
in
which
the
infringement
became
kno
wn
to
the
national
comp
etent
authorities
,
in
particular
whether
,
and
if
so
to
what
extent
,
the
operato
r
notif
ied
the
infringem
ent
;
(
i)the
intentional
or
negligent
character
of
the
infringement
;
(
j)any
action
taken
by
the
operator
to
mitigat
e
the
harm
suffer
ed
by
the
affected
persons
.
and
bodies
established
in
that
Member
State
.
a
manner
that
the
fines
are
imp
osed
by
competent
national
cour
ts
or
by
other
bodies
,
as
applicable
in
those
Member
States
.
The
application
of
such
rules
in
those
Member
States
shall
have
an
equivalent
effect
.
Union
and
national
law
,
including
effective
judicial
remedies
and
due
process
.
during
that
year
,
in
accordance
with
this
Article
,
and
about
any
related
litigation
or
judicial
proceedings
.
Article
100
Adminis
trativ
e
fines
on
Union
instit
utions
,
bodies
,
offices
and
agencies
agencies
falling
within
the
scope
of
this
Regulation
.
When
deciding
whether
to
impose
an
administrative
fine
and
when
deciding
on
the
amount
of
the
administrative
fine
in
each
individual
case
,
all
relevant
circumstances
of
the
specific
situation
shall
be
take
n
into
account
and
due
rega
rd
shall
be
given
to
the
followi
ng
:
nature
,
gravity
and
duration
of
the
infringem
ent
and
of
its
consequences
,
taking
into
account
the
purpose
of
the
AI
syste
m
concer
ned
,
as
well
as
,
where
appropriate
,
the
number
of
affected
persons
and
the
level
of
damagesuffered
by
them
;
(
b)the
degree
of
responsibility
of
the
Union
institution
,
body
,
office
or
agency
,
taking
into
account
technical
and
organisational
measures
imp
lemented
by
them
;
(
c)any
action
take
n
by
the
Union
institution
,
body
,
office
or
agency
to
mitigat
e
the
damagesuffered
by
affected
persons
;
(
d)the
degree
of
cooperation
with
the
European
Data
protection
Super
visor
in
order
to
remedy
the
infringement
and
mitiga
te
the
possible
adverse
effects
of
the
infringement
,
including
compliance
with
any
of
the
measures
previously
ordered
by
the
European
Data
protection
Super
visor
against
the
Union
institution
,
body
,
office
or
agency
concer
ned
with
regard
to
the
same
subject
matt
er
;
(
e)any
similar
previous
infringements
by
the
Union
institution
,
body
,
office
or
agency
;
(
f)the
manner
in
which
the
infringem
ent
became
kno
wn
to
the
European
Data
protection
Super
visor
,
in
particular
whether
,
and
if
so
to
what
extent
,
the
Union
institution
,
body
,
office
or
agency
notif
ied
the
infringem
ent
;
(
g)the
annual
budg
et
of
the
Union
institution
,
body
,
office
or
agency
.
fines
of
up
to
EUR
1
500
000
.
laid
down
in
Article
5
,
shall
be
subject
to
administrative
fines
of
up
to
EUR
750
000
.
institution
,
body
,
office
or
agency
which
is
the
subject
of
the
proceedings
conducted
by
the
European
Data
Protection
Super
visor
the
oppor
tunity
of
being
heard
on
the
matt
er
regard
ing
the
possible
infringem
ent
.
The
European
Data
Protection
Super
visor
shall
base
his
or
her
decisions
only
on
elements
and
circumstances
on
which
the
parties
concer
ned
have
been
able
to
comment
.
Comp
lainants
,
if
any
,
shall
be
associated
closely
with
the
proceedings
.
have
access
to
the
European
Data
Protection
Super
visor
’s
file
,
subject
to
the
legitimat
e
interest
of
individuals
or
undertakings
in
the
protect
ion
of
their
personal
data
or
business
secrets
.
shall
not
affect
the
effective
operation
of
the
Union
institution
,
body
,
office
or
agency
fined
.
it
has
imp
osed
pursuant
to
this
Article
and
of
any
litigation
or
judicial
proceedings
it
has
initiated
.
Article
101
Fines
for
providers
of
general
-
pur
pose
AI
models
worldwide
turnove
r
in
the
preceding
financial
year
or
EUR
15
000
000
,
whichever
is
higher
.
,
when
the
Commission
finds
that
the
provider
intent
ionally
or
negligently
:
(
a)infringed
the
relevant
provisions
of
this
Regulation
;
(
b)failed
to
comply
with
a
request
for
a
document
or
for
information
pursuant
to
Article
91
,
or
supplied
incor
rect
,
incom
plete
or
misleading
information
;
(
c)failed
to
comp
ly
with
a
measure
request
ed
under
Article
93
;
(
d)failed
to
make
available
to
the
Commission
access
to
the
general
-
pur
pose
AI
model
or
general
-
pur
pose
AI
model
with
syste
mic
risk
with
a
view
to
conducting
an
evaluation
pursuant
to
Article
92
.
In
fixing
the
amount
of
the
fine
or
periodic
penalty
payment
,
regard
shall
be
had
to
the
nature
,
gravity
and
duration
of
the
infringement
,
taking
due
account
of
the
principles
of
propor
tionality
and
appropriateness
.
The
Commission
shall
also
into
account
commitments
made
in
accordance
with
Article
93(3
)
or
made
in
relevant
codes
of
practice
in
accordance
with
Article
56
.
the
provid
er
of
the
gene
ral
-
pur
pose
AI
model
and
give
it
an
oppor
tunity
to
be
heard
.
fixing
a
fine
under
this
Article
.
It
may
cancel
,
reduce
or
increase
the
fine
imposed
.
proceedings
in
view
of
the
possible
adoption
of
decisions
pursuant
to
paragraph
1
of
this
Article
.
Those
imp
lementing
acts
shall
be
adop
ted
in
accordance
with
the
examination
procedure
refer
red
to
in
Article
98(2
)
.
CHAPTER
XIII
FINAL
PROVISIONS
Article
102
Amendment
to
Regulation
(
EC
)
No
In
Article
4(3
)
of
Regulation
(
EC
)
No
,
the
following
subparagraph
is
added
:
‘
When
adopti
ng
detailed
measures
related
to
technical
specifications
and
procedures
for
approva
l
and
use
of
security
equipment
concerning
Artificial
Intellig
ence
systems
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliame
nt
and
of
the
Council
(
*
)
,
the
requirements
set
out
in
Chap
ter
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Article
103
Amendment
to
Regulation
(
EU
)
No
In
Article
17(5
)
of
Regulation
(
EU
)
No
,
the
follo
wing
subparagraph
is
added
:
‘
When
adopting
delegat
ed
acts
pursuant
to
the
first
subparagraph
concerning
artificial
intellig
ence
systems
which
are
safety
compo
nents
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
(
*
)
,
the
requirements
set
out
in
Chap
ter
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Amendment
to
Regulation
(
EU
)
No
In
Article
22(5
)
of
Regulation
(
EU
)
No
,
the
follo
wing
subparagraph
is
added
:
‘
When
adopting
deleg
ated
acts
pursuant
to
the
first
subparagraph
concerning
Artificial
Intell
igence
systems
which
are
safety
compo
nents
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
(
*
)
,
the
requirements
set
out
in
Chap
ter
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Article
105
Amendment
to
Directiv
e
/EU
In
Article
8
of
Directive
/EU
,
the
following
paragraph
is
added
:
‘
5
.
For
Artificial
Intelligence
systems
which
are
safety
comp
onents
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
(
*
)
,
when
carrying
out
its
activities
pursuant
to
paragraph
1
and
when
adop
ting
technical
specifications
and
testing
standards
in
accordance
with
paragraphs
2
and
3
,
the
Commission
shall
take
into
account
the
requirements
set
out
in
Chap
ter
III
,
Section
2
,
of
that
Regulation
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Article
106
Amendment
to
Directiv
e
(
EU
)
In
Article
5
of
Directive
(
EU
)
,
the
follo
wing
paragraph
is
added
:
‘
12
.
When
adop
ting
delegat
ed
acts
pursuant
to
paragraph
1
and
implementing
acts
pursuant
to
paragraph
11
concerning
Artificial
Intellig
ence
systems
which
are
safety
components
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
(
*
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
take
n
into
account
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Amendment
to
Regulation
(
EU
)
In
Article
5
of
Regulation
(
EU
)
the
followi
ng
paragraph
is
added
:
‘
4
.
When
adop
ting
deleg
ated
acts
pursuant
to
paragraph
3
concerning
Artificial
Intellig
ence
systems
which
are
safety
compo
nents
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
(
*
)
,
the
requirements
set
out
in
Chap
ter
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Article
108
Amendments
to
Regulation
(
EU
)
Regulation
(
EU
)
is
amended
as
follo
ws
:
in
Article
17
,
the
following
paragraph
is
added
:
‘
3
.
Without
prejudice
to
paragraph
2
,
when
adop
ting
implementing
acts
pursuant
to
paragraph
1
concerning
Artificial
Intellig
ence
systems
which
are
safety
comp
onents
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliame
nt
and
of
the
Council
(
*
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intellig
ence
Act
)
.
’
;
in
Article
19
,
the
following
paragraph
is
added
:
‘
4
.
When
adop
ting
deleg
ated
acts
pursuant
to
paragraphs
1
and
2
concerning
Artificial
Intel
ligence
systems
which
are
safety
comp
onents
within
the
meaning
of
Regulation
(
EU
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
’
;
in
Article
43
,
the
following
paragraph
is
added
:
‘
4
.
When
adop
ting
implementing
acts
pursuant
to
paragraph
1
concerning
Artificial
Intelligence
systems
which
are
safety
compo
nents
within
the
meaning
of
Regulation
(
EU
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
’
;
in
Article
47
,
the
following
paragraph
is
added
:
‘
3
.
When
adop
ting
deleg
ated
acts
pursuant
to
paragraphs
1
and
2
concerning
Artificial
Intel
ligence
systems
which
are
safety
comp
onents
within
the
meaning
of
Regulation
(
EU
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
’
;
in
Article
57
,
the
following
subparagraph
is
added
:
‘
When
adop
ting
those
imp
lementing
acts
concerning
Artificial
Intelligence
systems
which
are
safety
comp
onents
within
the
meaning
of
Regulation
(
EU
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
take
n
into
account
.
’
;
in
Article
58
,
the
following
paragraph
is
added
:
‘
3
.
When
adop
ting
deleg
ated
acts
pursuant
to
paragraphs
1
and
2
concerning
Artificial
Intel
ligence
systems
which
are
safety
comp
onents
within
the
meaning
of
Regulation
(
EU
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
taken
into
account
.
’
.
Article
109
Amendment
to
Regulation
(
EU
)
In
Article
11
of
Regulation
(
EU
)
,
the
following
paragraph
is
added
:
‘
3
.
When
adop
ting
the
imp
lementing
acts
pursuant
to
paragraph
2
,
concerning
artificial
intellig
ence
systems
which
are
safety
comp
onents
within
the
meaning
of
Regulation
(
EU
)
of
the
European
Parliame
nt
and
of
the
Council
(
*
)
,
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
of
that
Regulation
shall
be
take
n
into
account
.
(
*
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intellig
ence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Article
110
Amendment
to
Directiv
e
(
EU
)
In
Annex
I
to
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
,
the
following
point
is
added
:
‘
(
68
)
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
13
June
2024
laying
down
harmonised
rules
on
artificial
intelligence
and
amending
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
and
Directives
/EU
,
(
EU
)
and
(
EU
)
(
Artificial
Intelligence
Act
)
.
’
.
Article
111
AI
systems
already
placed
on
the
mark
et
or
put
into
service
and
general
-
pur
pose
AI
models
already
placed
on
the
marked
compo
nents
of
the
large
-
scale
IT
systems
established
by
the
legal
acts
listed
in
Annex
X
that
have
been
placed
on
the
market
or
put
into
service
before
2
Augu
st
2027
shall
be
brought
into
compliance
with
this
Regulation
by
31
December
2030
.
The
requirements
laid
down
in
this
Regulation
shall
be
taken
into
account
in
the
evaluation
of
each
large
-
scale
IT
system
established
by
the
legal
acts
listed
in
Annex
X
to
be
under
take
n
as
provid
ed
for
in
those
lega
l
acts
and
where
those
lega
l
acts
are
replaced
or
amended
.
to
operators
of
higher
isk
AI
systems
,
other
than
the
systems
referred
to
in
paragraph
1
of
this
Article
,
that
have
been
placed
on
the
mark
et
or
put
into
service
before
2
August
2026
,
only
if
,
as
from
that
date
,
those
systems
are
subject
to
significant
changes
in
their
designs
.
In
any
case
,
the
providers
and
deplo
yers
of
higher
isk
AI
systems
intended
to
be
used
by
public
authorities
shall
take
the
necessary
steps
to
comply
with
the
requirements
and
obligations
of
this
Regulation
by
2
August
necessary
steps
in
order
to
comp
ly
with
the
obligations
laid
down
in
this
Regulation
by
2
Augu
st
2027
.
(
58
)
Directive
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
25
November
2020
on
representative
actions
for
the
protection
of
the
collective
interests
of
consumers
and
repealing
Directive
/EC
(
12.2020
,
p.
1).Article
112
Evaluation
and
review
practices
laid
down
in
Article
5
,
once
a
year
following
the
entr
y
into
force
of
this
Regulation
,
and
until
the
end
of
the
period
of
the
deleg
ation
of
power
laid
down
in
Article
97
.
The
Commission
shall
submit
the
findi
ngs
of
that
assessment
to
the
European
Parliament
and
the
Council
.
Parliame
nt
and
to
the
Council
on
the
following
:
(
a)the
need
for
amendments
extending
existing
area
headings
or
adding
new
area
headings
in
Annex
III
;
(
b)amendments
to
the
list
of
AI
systems
requir
ing
additional
transparency
measures
in
Article
50
;
(
c)amendments
enhancing
the
effectiveness
of
the
super
vision
and
gove
rnance
system
.
of
this
Regulation
to
the
European
Parliament
and
to
the
Council
.
The
repor
t
shall
include
an
assessment
with
regard
to
the
structure
of
enforcement
and
the
possible
need
for
a
Union
agency
to
resolve
any
identified
shor
tcomings
.
On
the
basis
of
the
find
ings
,
that
repor
t
shall
,
where
appropriate
,
be
accom
panied
by
a
proposal
for
amendment
of
this
Regulation
.
The
repor
ts
shall
be
made
public
.
perfo
rm
the
tasks
assigned
to
them
under
this
Regulation
;
(
b)the
state
of
penalties
,
in
particular
administrative
fines
as
refer
red
to
in
Article
99(1
)
,
applied
by
Member
States
for
infringements
of
this
Regulation
;
(
c)adop
ted
harmonised
standards
and
common
specifications
developed
to
support
this
Regulation
;
(
d)the
number
of
undertakings
that
enter
the
market
after
the
entr
y
into
application
of
this
Regulation
,
and
how
many
of
them
are
SMEs
.
given
sufficient
powers
and
comp
etences
to
fulfil
its
tasks
,
and
whether
it
would
be
relevant
and
needed
for
the
proper
imp
lementation
and
enforcement
of
this
Regulation
to
upgrade
the
AI
Offi
ce
and
its
enforcement
compet
ences
and
to
increase
its
resources
.
The
Commission
shall
submit
a
repor
t
on
its
evaluation
to
the
European
Parliame
nt
and
to
the
Council
.
on
the
development
of
standardisation
deliverables
on
the
energy
-
eff
icient
development
of
general
-
pur
pose
AI
models
,
and
asses
the
need
for
further
measures
or
actions
,
including
binding
measures
or
actions
.
The
repor
t
shall
be
submitted
to
the
European
Parliament
and
to
the
Council
,
and
it
shall
be
made
public
.
voluntar
y
codes
of
conduct
to
foste
r
the
application
of
the
requirements
set
out
in
Chapt
er
III
,
Section
2
for
AI
systems
other
than
higher
isk
AI
systems
and
possibly
other
additional
requirements
for
AI
systems
other
than
higher
isk
AI
systems
,
including
as
regards
environmental
sustainability
.
the
Commission
with
information
upon
its
request
and
without
undue
dela
y.
the
positions
and
find
ings
of
the
Board
,
of
the
European
Parliame
nt
,
of
the
Council
,
and
of
other
relevant
bodies
or
sources
.
The
Commission
shall
,
if
necessary
,
submit
appropriate
proposals
to
amend
this
Regulation
,
in
particular
taking
into
account
developments
in
technology
,
the
effect
of
AI
systems
on
health
and
safety
,
and
on
fundamental
rights
,
and
in
light
of
the
state
of
progress
in
the
information
society
.
develop
an
objective
and
participative
methodology
for
the
evaluation
of
risk
levels
based
on
the
criteria
outlined
in
the
relevant
Articles
and
the
inclusion
of
new
systems
in
:
(
a)the
list
set
out
in
Annex
III
,
including
the
extension
of
existing
area
headings
or
the
addition
of
new
area
headings
in
that
Annex
;
(
b)the
list
of
prohibited
practices
set
out
in
Article
5
;
and
(
c)the
list
of
AI
systems
requir
ing
additional
transparency
measures
pursuant
to
Article
50
.
concer
ns
sectoral
Union
harmonisation
legislation
listed
in
Section
B
of
Annex
I
shall
take
into
account
the
regulator
y
specific
ities
of
each
secto
r
,
and
the
existing
gover
nance
,
conf
ormity
assessment
and
enforcement
mechanisms
and
authorities
established
therein
.
repor
t
on
it
to
the
European
Parliament
,
the
Council
and
the
European
Economic
and
Social
Committee
,
taking
into
account
the
first
years
of
application
of
this
Regulation
.
On
the
basis
of
the
findings
,
that
repor
t
shall
,
where
appropriate
,
be
accom
panied
by
a
proposal
for
amendment
of
this
Regulation
with
regard
to
the
structure
of
enforcement
and
the
need
for
a
Union
agency
to
resolve
any
identifie
d
shor
tcomings
.
Article
113
Entr
y
into
force
and
application
This
Regulation
shall
enter
into
force
on
the
twentieth
day
following
that
of
its
publication
in
the
of
the
Europe
an
Union
.
It
shall
apply
from
2
August
2026
.
How
ever
:
(
a)Chapt
ers
I
and
II
shall
apply
from
2
Febr
uary
2025
;
(
b)Chapt
er
III
Section
4
,
Chap
ter
V
,
Chapt
er
VII
and
Chap
ter
XII
and
Article
78
shall
apply
from
2
Augu
st
2025
,
with
the
exception
of
Article
101
;
(
c)Article
6(1
)
and
the
corresponding
obligations
in
this
Regulation
shall
apply
from
2
August
2027
.
This
Regulation
shall
be
binding
in
its
entirety
and
directly
applicable
in
all
Member
States
.
Done
at
Brussels
,
13
June
2024
.
For
the
European
Parliament
The
President
R.
METSOL
AFor
the
Council
The
President
M.
MICHEL
List
of
Union
harmonisation
legislation
Section
A.
List
of
Union
harmonisation
legislation
based
on
the
New
Legislative
Framewo
rk
Directive
/EC
(
6.2006
,
p.
24
)
;
and
personal
watercraf
t
and
repealing
Directive
/EC
(
12.2013
,
p.
90
)
;
the
laws
of
the
Member
States
relating
to
lifts
and
safety
comp
onents
for
lifts
(
3.2014
,
p.
251
)
;
the
laws
of
the
Member
States
relating
to
equipment
and
protect
i
ve
systems
intende
d
for
use
in
pote
ntially
explosive
atmospheres
(
3.2014
,
p.
309
)
;
laws
of
the
Member
States
relating
to
the
making
available
on
the
market
of
radio
equipment
and
repealing
Directive
/EC
(
5.2014
,
p.
62
)
;
laws
of
the
Member
States
relating
to
the
making
available
on
the
market
of
pressure
equipment
;
installations
and
repealing
Directive
/EC
(
3.2016
,
p.
1
)
;
equipment
and
repealing
Council
Directive
/EEC
(
3.2016
,
p.
51
)
;
gaseous
fuels
and
repealing
Directive
/EC
(
3.2016
,
p.
99
)
;
amending
Directive
/EC
,
Regulation
(
EC
)
No
and
Regulation
(
EC
)
No
and
repealing
Council
Directives
/EEC
and
/EEC
(
5.2017
,
p.
1
)
;
medical
devices
and
repealing
Directive
/EC
and
Commission
Decision
/EU
(
5.2017
,
p.
176
)
.
Section
B.
List
of
other
Union
harmonisation
legislation
in
the
field
of
civil
aviation
security
and
repealing
Regulation
(
EC
)
No
(
4.2008
,
p.
72
)
;
and
market
surveillance
of
two-
or
three
-
wheel
vehicles
and
quadr
icycles
(
3.2013
,
p.
52
)
;
and
market
surveillance
of
agricultural
and
forestr
y
vehicles
(
3.2013
,
p.
1
)
;
Directive
/EU
of
the
European
Parliament
and
of
the
Council
of
23
July
2014
on
mar
ine
equipment
and
repealing
Council
Directive
/EC
(
8.2014
,
p.
146
)
;
the
rail
syste
m
within
the
European
Union
(
5.2016
,
p.
44
)
;
market
surveillance
of
motor
vehicles
and
their
trailers
,
and
of
systems
,
compo
nents
and
separate
technical
units
intende
d
for
such
vehicles
,
amending
Regulations
(
EC
)
No
and
(
EC
)
No
and
repealing
Directive
/EC
(
6.2018
,
p.
1
)
;
requirements
for
motor
vehicles
and
their
trailers
,
and
systems
,
compo
nents
and
separate
technical
units
intended
for
such
vehicles
,
as
rega
rds
their
general
safety
and
the
protection
of
vehicle
occupants
and
vulnerable
road
users
,
amending
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
repealing
Regulations
(
EC
)
No
,
(
EC
)
No
and
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
and
Commission
Regulations
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
No
and
(
EU
)
(
12.2019
,
p.
1
)
;
field
of
civil
aviat
ion
and
establishing
a
European
Union
Aviation
Safety
Agency
,
and
amending
Regulations
(
EC
)
No
,
(
EC
)
No
,
(
EU
)
No
,
(
EU
)
No
and
Directives
/EU
and
/EU
of
the
European
Parliament
and
of
the
Council
,
and
repealing
Regulations
(
EC
)
No
and
(
EC
)
No
of
the
European
Parliament
and
of
the
Council
and
Council
Regulation
(
EEC
)
No
,
in
so
far
as
the
design
,
production
and
placing
on
the
market
of
aircraf
ts
referred
to
in
Article
2(1
)
,
points
(
a
)
and
(
b
)
thereof
,
where
it
concer
ns
unmanned
aircraf
t
and
their
engines
,
propellers
,
parts
and
equipment
to
control
them
remotely
,
are
concer
ned
.
List
of
criminal
offences
referred
to
in
Article
5(1
)
,
first
subparag
raph
,
point
(
h)(iii
)
Criminal
offences
referred
to
in
Article
5(1
)
,
first
subparagraph
,
point
(
h)(iii
):
—
terrorism
,
—
trafficking
in
human
beings
,
—
sexual
exploitation
of
children
,
and
child
pornography
,
—
illicit
traffi
cking
in
narcotic
drugs
or
psychotropic
substances
,
—
illicit
traffi
cking
in
weapons
,
munitions
or
explosives
,
—
murder
,
grievous
bodily
injur
y
,
—
illicit
trade
in
human
organs
or
tissue
,
—
illicit
traffi
cking
in
nuclear
or
radioactive
materi
als
,
—
kidnapping
,
illegal
restraint
or
hostage
-taking
,
—
crimes
within
the
jurisdiction
of
the
International
Criminal
Cour
t
,
—
unla
wful
seizure
of
aircraf
t
or
ships
,
—
rape
,
—
environmental
crime
,
—
organised
or
armed
robber
y
,
—
sabotag
e
,
—
participation
in
a
criminal
organisation
involved
in
one
or
more
of
the
offences
listed
above
.
III
higher
isk
AI
systems
referred
to
in
Article
6(2
)
higher
isk
AI
systems
pursuant
to
Article
6(2
)
are
the
AI
systems
listed
in
any
of
the
follo
wing
areas
:
This
shall
not
include
AI
systems
intende
d
to
be
used
for
biometric
verification
the
sole
purpose
of
which
is
to
confir
m
that
a
specific
natural
person
is
the
person
he
or
she
claims
to
be
;
(
b)AI
systems
intended
to
be
used
for
biometric
categorisation
,
according
to
sensitive
or
protected
attributes
or
character
istics
based
on
the
inference
of
those
attribut
es
or
characteristics
;
(
c)AI
systems
intended
to
be
used
for
emotion
recognition
.
critical
digital
infrastructure
,
road
traff
ic
,
or
in
the
supply
of
wate
r
,
gas
,
heating
or
electr
icity
.
vocational
training
institutions
at
all
levels
;
(
b)AI
systems
intended
to
be
used
to
evaluate
learning
outcomes
,
including
when
those
outcomes
are
used
to
steer
the
learning
process
of
natural
persons
in
educational
and
vocational
training
institutions
at
all
levels
;
(
c)AI
systems
intended
to
be
used
for
the
purpose
of
assessing
the
appropriate
level
of
education
that
an
individual
will
receive
or
will
be
able
to
access
,
in
the
context
of
or
within
educational
and
vocational
training
institutions
at
all
levels
;
(
d)AI
systems
intended
to
be
used
for
monitoring
and
detecting
prohibite
d
behavi
our
of
students
during
tests
in
the
cont
ext
of
or
within
educational
and
vocational
training
institutions
at
all
levels
.
job
adver
tisements
,
to
analyse
and
filter
job
applications
,
and
to
evaluate
candidates
;
(
b)AI
systems
intended
to
be
used
to
mak
e
decisions
affecting
terms
of
work
-relate
d
relationships
,
the
promotion
or
termination
of
work
-
related
contractual
relationships
,
to
allocat
e
tasks
based
on
individual
behavi
our
or
personal
traits
or
character
istics
or
to
monitor
and
evaluate
the
perf
ormance
and
behavi
our
of
persons
in
such
relationships
.
of
natural
persons
for
essential
public
assistance
benefits
and
services
,
including
healthcare
services
,
as
well
as
to
grant
,
reduce
,
revok
e
,
or
reclaim
such
benefi
ts
and
services
;
(
b)AI
systems
intended
to
be
used
to
evaluate
the
creditwo
rthiness
of
natural
persons
or
establish
their
credit
score
,
with
the
exception
of
AI
systems
used
for
the
purpose
of
detect
ing
financ
ial
fraud
;
(
c)AI
systems
intended
to
be
used
for
risk
assessment
and
pricing
in
relation
to
natural
persons
in
the
case
of
life
and
health
insurance
;
(
d)AI
systems
intended
to
evaluate
and
classify
emerge
ncy
calls
by
natural
persons
or
to
be
used
to
dispatch
,
or
to
establish
priority
in
the
dispatching
of
,
emergency
first
response
services
,
including
by
police
,
firef
ighters
and
medical
aid
,
as
well
as
of
emerg
ency
healthcare
patient
triage
systems
.
offices
or
agencies
in
support
of
law
enforcement
authorities
or
on
their
behalf
to
assess
the
risk
of
a
natural
person
becoming
the
victim
of
criminal
offences
;
(
b)AI
systems
intended
to
be
used
by
or
on
behalf
of
law
enforcement
authorities
or
by
Union
institutions
,
bodies
,
offices
or
agencies
in
support
of
law
enforcement
authorities
as
polygraphs
or
similar
tools
;
(
c)AI
systems
intended
to
be
used
by
or
on
behalf
of
law
enforcement
authorities
,
or
by
Union
institutions
,
bodies
,
offices
or
agencies
,
in
support
of
law
enforcement
authorities
to
evaluate
the
reliability
of
evidence
in
the
course
of
the
invest
igation
or
prosecution
of
criminal
offences
;
(
d)AI
systems
intende
d
to
be
used
by
law
enforcement
authorities
or
on
their
behalf
or
by
Union
institutions
,
bodies
,
offices
or
agencies
in
support
of
law
enforcement
authorities
for
assessing
the
risk
of
a
natural
person
offending
or
re
-
offending
not
solely
on
the
basis
of
the
profil
ing
of
natural
persons
as
referred
to
in
Article
3(4
)
of
Directive
(
EU
)
,
or
to
assess
personality
traits
and
character
istics
or
past
criminal
behavi
our
of
natural
persons
or
groups
;
(
e)AI
systems
intended
to
be
used
by
or
on
behalf
of
law
enforcement
authorities
or
by
Union
institutions
,
bodies
,
offices
or
agencies
in
support
of
law
enforcement
authorities
for
the
profil
ing
of
natural
persons
as
refer
red
to
in
Article
3(4
)
of
Directive
(
EU
)
in
the
course
of
the
detection
,
invest
igation
or
prosecution
of
criminal
offenc
es
.
national
law
:
(
a)AI
systems
intended
to
be
used
by
or
on
behalf
of
competent
public
authorities
or
by
Union
institutions
,
bodies
,
offices
or
agencies
as
polygraphs
or
similar
tools
;
(
b)AI
systems
intended
to
be
used
by
or
on
behalf
of
competent
public
authorities
or
by
Union
institutions
,
bodies
,
offices
or
agencies
to
assess
a
risk
,
including
a
security
risk
,
a
risk
of
irregular
migration
,
or
a
health
risk
,
posed
by
a
natural
person
who
intends
to
enter
or
who
has
entered
into
the
territory
of
a
Member
State
;
(
c)AI
systems
intended
to
be
used
by
or
on
behalf
of
competent
public
authorities
or
by
Union
institutions
,
bodies
,
offices
or
agencies
to
assist
competent
public
authorities
for
the
examination
of
applications
for
asylum
,
visa
or
residence
permits
and
for
associate
d
complaints
with
rega
rd
to
the
eligibility
of
the
natural
persons
applying
for
a
status
,
including
relate
d
assessments
of
the
reliability
of
evidence
;
(
d)AI
systems
intended
to
be
used
by
or
on
behalf
of
comp
etent
public
authorities
,
or
by
Union
institutions
,
bodies
,
offices
or
agencies
,
in
the
cont
ext
of
migration
,
asylum
or
border
control
management
,
for
the
purpose
of
detect
ing
,
recognising
or
identifying
natural
persons
,
with
the
exception
of
the
verificati
on
of
trave
l
documents
.
researchi
ng
and
inter
preting
facts
and
the
law
and
in
applying
the
law
to
a
concrete
set
of
facts
,
or
to
be
used
in
a
similar
way
in
alternative
dispute
resolution
;
systems
intended
to
be
used
for
influencing
the
outcome
of
an
election
or
refere
ndum
or
the
voting
behavio
ur
of
natural
persons
in
the
exercise
of
their
vote
in
elections
or
referenda
.
This
does
not
include
AI
systems
to
the
output
of
which
natural
persons
are
not
directly
exposed
,
such
as
tools
used
to
organise
,
opti
mise
or
structure
political
camp
aigns
from
an
administrative
or
logistical
point
of
view
.
Technical
document
ation
referred
to
in
Article
11(1
)
The
technical
documentation
referred
to
in
Article
11(1
)
shall
contain
at
least
the
following
information
,
as
applicable
to
the
relevant
AI
syste
m
:
versions
;
(
b)how
the
AI
system
interacts
with
,
or
can
be
used
to
interac
t
with
,
hardware
or
software
,
including
with
other
AI
systems
,
that
are
not
part
of
the
AI
syste
m
itself
,
where
applicable
;
(
c)the
versions
of
relevant
software
or
firmware
,
and
any
requirements
related
to
version
update
s
;
(
d)the
descr
iption
of
all
the
forms
in
which
the
AI
system
is
placed
on
the
market
or
put
into
service
,
such
as
software
pack
ages
embedded
into
hardware
,
downloads
,
or
APIs
;
(
e)the
descr
iption
of
the
hardware
on
which
the
AI
system
is
intended
to
run
;
(
f)where
the
AI
system
is
a
comp
onent
of
products
,
photographs
or
illustrations
showing
exte
rnal
features
,
the
marking
and
internal
layout
of
those
products
;
(
g)a
basic
descr
iption
of
the
user
-interface
provided
to
the
deplo
yer
;
(
h)instr
uctions
for
use
for
the
deplo
yer
,
and
a
basic
descr
iption
of
the
user
-inte
rface
provided
to
the
deplo
yer
,
where
applicable
;
pre
-
trained
systems
or
tools
provided
by
third
parties
and
how
those
were
used
,
integrat
ed
or
modifi
ed
by
the
provid
er
;
(
b)the
design
specifications
of
the
syste
m
,
namely
the
general
logic
of
the
AI
system
and
of
the
algor
ithms
;
the
key
design
choices
including
the
rationale
and
assump
tions
made
,
including
with
regard
to
persons
or
groups
of
persons
in
respect
of
who
,
the
system
is
intended
to
be
used
;
the
main
classific
ation
choices
;
what
the
syste
m
is
designed
to
optimise
for
,
and
the
relevance
of
the
different
parameters
;
the
descr
iption
of
the
expected
output
and
output
quality
of
the
system
;
the
decisions
about
any
possible
trade
-
off
made
regard
ing
the
technical
solutions
adopt
ed
to
comply
with
the
requirements
set
out
in
Chapt
er
III
,
Section
2
;
(
c)the
descr
iption
of
the
system
arch
itecture
explaining
how
software
comp
onents
build
on
or
feed
into
each
other
and
integrat
e
into
the
overall
processing
;
the
comp
utational
resources
used
to
develop
,
train
,
test
and
validat
e
the
AI
system
;
(
d)where
relevant
,
the
data
requirements
in
terms
of
datasheets
descr
ibing
the
training
methodologies
and
techniques
and
the
training
data
sets
used
,
including
a
general
descr
iption
of
these
data
sets
,
information
about
their
prove
nance
,
scope
and
main
character
istics
;
how
the
data
was
obtained
and
selected
;
labelling
procedures
(
e.g.
for
super
vised
learning
)
,
data
cleaning
methodologies
(
e.g.
outliers
detection
)
;
(
e)assessment
of
the
human
oversight
measures
needed
in
accordance
with
Article
14
,
including
an
assessment
of
the
technical
measures
needed
to
facilitate
the
interpretation
of
the
outputs
of
AI
systems
by
the
deplo
yers
,
in
accordance
with
Article
13(3
)
,
point
(
d
)
;
(
f)where
applicable
,
a
detailed
descr
iption
of
pre
-
deter
mined
chang
es
to
the
AI
system
and
its
perfo
rmance
,
together
with
all
the
relevant
information
relate
d
to
the
technical
solutions
adop
ted
to
ensure
continuous
complia
nce
of
the
AI
system
with
the
relevant
requirements
set
out
in
Chapt
er
III
,
Section
2
;
(
g)the
validation
and
testing
procedures
used
,
including
information
about
the
validation
and
testing
data
used
and
their
main
characteristics
;
metr
ics
used
to
measure
accuracy
,
robustness
and
compliance
with
other
relevant
requirements
set
out
in
Chap
ter
III
,
Section
2
,
as
well
as
pote
ntially
discr
iminat
ory
imp
acts
;
test
logs
and
all
test
repor
ts
dated
and
signed
by
the
responsible
persons
,
including
with
regard
to
pre
-
determ
ined
changes
as
referred
to
under
point
(
f
)
;
measures
put
in
place
;
its
capabilities
and
limitations
in
perf
ormance
,
including
the
degrees
of
accuracy
for
specific
persons
or
groups
of
persons
on
which
the
system
is
intended
to
be
used
and
the
overall
expected
level
of
accuracy
in
relation
to
its
intende
d
purpose
;
the
foreseeable
unintended
outcomes
and
sources
of
risks
to
health
and
safety
,
fundamental
rights
and
discr
imination
in
view
of
the
intended
purpose
of
the
AI
system
;
the
human
oversight
measures
needed
in
accordance
with
Article
14
,
including
the
technical
measures
put
in
place
to
facilitate
the
interpretation
of
the
outputs
of
AI
systems
by
the
deplo
yers
;
specifications
on
input
data
,
as
appropriate
;
of
the
European
Union
;
where
no
such
harmonised
standards
have
been
applied
,
a
detailed
descr
iption
of
the
solutions
adop
ted
to
meet
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
including
a
list
of
other
relevant
standards
and
technical
specifications
applied
;
accordance
with
Article
72
,
including
the
post
-
mark
et
monitoring
plan
refer
red
to
in
Article
72(3
)
.
EU
declaration
of
confor
mity
The
EU
declaration
of
conf
ormity
refer
red
to
in
Article
47
,
shall
contain
all
of
the
followi
ng
information
:
the
AI
syste
m
;
the
provid
er
;
Union
law
that
provides
for
the
issuing
of
the
EU
declaration
of
conf
ormity
referred
to
in
Article
47
;
Regulations
(
EU
)
and
(
EU
)
and
Directive
(
EU
)
;
conf
ormity
is
declared
;
assessment
procedure
perf
ormed
,
and
identification
of
the
certificate
issued
;
indication
for
,
or
on
behalf
of
whom
,
that
person
signed
,
a
signature
.
VI
Conform
ity
assessment
procedure
based
on
inter
nal
control
points
2
,
3
and
4
.
Article
17
.
of
the
AI
system
with
the
relevant
essential
requirements
set
out
in
Chapt
er
III
,
Section
2
.
as
refer
red
to
in
Article
72
is
consiste
nt
with
the
technical
documentation
.
Conf
ormity
based
on
an
assessment
of
the
quality
management
system
and
an
assessment
of
the
technical
documentation
Conf
ormity
based
on
an
assessment
of
the
quality
management
syste
m
and
an
assessment
of
the
technical
documentation
is
the
conf
ormity
assessment
procedure
based
on
points
2
to
5
.
The
approved
quality
managem
ent
syste
m
for
the
design
,
development
and
testing
of
AI
systems
pursuant
to
Article
17
shall
be
examined
in
accordance
with
point
3
and
shall
be
subject
to
surveillance
as
specified
in
point
5
.
The
technical
documentation
of
the
AI
system
shall
be
examined
in
accordance
with
point
4
.
name
and
address
;
(
b)the
list
of
AI
systems
covered
under
the
same
quality
managem
ent
syste
m
;
(
c)the
technical
documentation
for
each
AI
system
covered
under
the
same
quality
managem
ent
syste
m
;
(
d)the
documentation
concerning
the
quality
managem
ent
syste
m
which
shall
cover
all
the
aspects
listed
under
Article
17
;
(
e)a
descr
iption
of
the
procedures
in
place
to
ensure
that
the
quality
management
system
remains
adequate
and
effective
;
(
f)a
written
declaration
that
the
same
application
has
not
been
lodg
ed
with
any
other
notified
body
.
requirements
referred
to
in
Article
17
.
The
decision
shall
be
notif
ied
to
the
provider
or
its
authorised
representative
.
The
notification
shall
contain
the
conclusions
of
the
assessment
of
the
quality
management
syste
m
and
the
reasoned
assessment
decision
.
that
it
remains
adequate
and
efficient
.
be
brought
to
the
attention
of
the
notified
body
by
the
provider
.
The
proposed
changes
shall
be
examined
by
the
notified
body
,
which
shall
decide
whether
the
modif
ied
quality
managem
ent
system
continues
to
satisfy
the
requirements
referred
to
in
point
3.2
or
whether
a
reassessment
is
necessary
.
The
notif
ied
body
shall
notify
the
provider
of
its
decision
.
The
notif
ication
shall
contain
the
conclusions
of
the
examination
of
the
changes
and
the
reasoned
assessment
decision
.
lodg
ed
by
the
provider
for
the
assessment
of
the
technical
documentation
relating
to
the
AI
system
which
the
provid
er
intends
to
place
on
the
market
or
put
into
service
and
which
is
covered
by
the
quality
managem
ent
system
referred
to
under
point
3
.
The
technical
documentation
shall
be
examined
by
the
notified
body
.
Where
relevant
,
and
limited
to
what
is
necessary
to
fulfil
its
tasks
,
the
notif
ied
body
shall
be
granted
full
access
to
the
training
,
validation
,
and
testing
data
sets
used
,
including
,
where
appropriate
and
subject
to
security
safegua
rds
,
through
API
or
other
relevant
technical
means
and
tools
enabling
remote
access
.
or
carry
out
further
tests
so
as
to
enable
a
proper
assessment
of
the
conf
ormity
of
the
AI
syste
m
with
the
requirements
set
out
in
Chapt
er
III
,
Section
2
.
Where
the
notif
ied
body
is
not
satisf
ied
with
the
tests
carried
out
by
the
provid
er
,
the
notif
ied
body
shall
itself
directly
carry
out
adequat
e
tests
,
as
appropriate
.
Section
2
,
after
all
other
reasonable
means
to
verify
conf
ormity
have
been
exhaust
ed
and
have
proven
to
be
insuffic
ient
,
and
upon
a
reasoned
request
,
the
notif
ied
body
shall
also
be
grante
d
access
to
the
training
and
trained
models
of
the
AI
system
,
including
its
relevant
parameters
.
Such
access
shall
be
subject
to
existing
Union
law
on
the
protect
ion
of
intellectual
proper
ty
and
trade
secrets
.
shall
contain
the
conclusions
of
the
assessment
of
the
technical
documentation
and
the
reasoned
assessment
decision
.
Where
the
AI
system
is
in
conf
ormity
with
the
requirements
set
out
in
Chapt
er
III
,
Section
2
,
the
notif
ied
body
shall
issue
a
Union
technical
documentation
assessment
certificate
.
The
certificate
shall
indicate
the
name
and
address
of
the
provider
,
the
conclusions
of
the
examination
,
the
conditions
(
if
any
)
for
its
validity
and
the
data
necessary
for
the
identifica
tion
of
the
AI
syste
m.
The
certificate
and
its
annexe
s
shall
contain
all
relevant
information
to
allow
the
conf
ormity
of
the
AI
system
to
be
evaluated
,
and
to
allow
for
control
of
the
AI
system
while
in
use
,
where
applicable
.
Where
the
AI
syste
m
is
not
in
conf
ormity
with
the
requirements
set
out
in
Chap
ter
III
,
Section
2
,
the
notif
ied
body
shall
refuse
to
issue
a
Union
technical
documentation
assessment
certificate
and
shall
inform
the
applicant
according
ly
,
giving
detailed
reasons
for
its
refusal
.
Where
the
AI
syste
m
does
not
meet
the
requirement
relating
to
the
data
used
to
train
it
,
re
-
training
of
the
AI
system
will
be
needed
prior
to
the
application
for
a
new
conf
ormity
assessment
.
In
this
case
,
the
reasoned
assessment
decision
of
the
notif
ied
body
refusing
to
issue
the
Union
technical
documentation
assessment
certificate
shall
contain
specific
considerations
on
the
quality
data
used
to
train
the
AI
syste
m
,
in
particular
on
the
reasons
for
non
-
compliance
.
purpose
shall
be
assessed
by
the
notified
body
which
issued
the
Union
technical
documentation
assessment
certificate
.
The
provider
shall
inform
such
notif
ied
body
of
its
intention
to
introduce
any
of
the
abovementioned
changes
,
or
if
it
other
wise
becomes
aware
of
the
occur
rence
of
such
changes
.
The
intended
changes
shall
be
assessed
by
the
notif
ied
body
,
which
shall
decide
whether
those
changes
require
a
new
conf
ormity
assessment
in
accordance
with
Article
43(4
)
or
whether
they
could
be
addressed
by
means
of
a
supplement
to
the
Union
technical
documentation
assessment
certificate
.
In
the
latter
case
,
the
notif
ied
body
shall
assess
the
chang
es
,
notify
the
provid
er
of
its
decision
and
,
where
the
change
s
are
approve
d
,
issue
to
the
provider
a
supplement
to
the
Union
technical
documentation
assessment
certificate
.
provid
er
duly
complie
s
with
the
terms
and
conditions
of
the
approve
d
quality
managem
ent
syste
m.
development
,
testing
of
the
AI
systems
is
taking
place
.
The
provid
er
shall
further
share
with
the
notif
ied
body
all
necessary
information
.
managem
ent
system
and
shall
provide
the
provid
er
with
an
audit
repor
t.
In
the
context
of
those
audits
,
the
notif
ied
body
may
carry
out
additional
tests
of
the
AI
systems
for
which
a
Union
technical
documentation
assessment
certificate
was
issued
.
Infor
mation
to
be
submitted
upon
the
registration
of
higher
isk
AI
systems
in
accordance
with
Article
49
Section
A
—
Information
to
be
submitted
by
provid
ers
of
higher
isk
AI
systems
in
accordance
with
Article
49(1
)
The
following
information
shall
be
provid
ed
and
thereaf
ter
kept
up
to
date
with
rega
rd
to
higher
isk
AI
systems
to
be
register
ed
in
accordance
with
Article
49(1
):
contact
details
of
that
person
;
the
AI
syste
m
;
this
AI
system
;
number
of
that
notif
ied
body
,
where
applicable
;
Union
;
enforcement
or
migration
,
asylum
and
border
control
management
referred
to
in
Annex
III
,
points
1
,
6
and
7
;
Section
B
—
Information
to
be
submitt
ed
by
provider
s
of
higher
isk
AI
systems
in
accordance
with
Article
49(2
)
The
following
information
shall
be
provided
and
thereaf
ter
kept
up
to
date
with
regard
to
AI
systems
to
be
regist
ered
in
accordance
with
Article
49(2
):
contact
details
of
that
person
;
the
AI
syste
m
;
procedure
under
Article
6(3
)
;
Union
.
C
—
Information
to
be
submitt
ed
by
deplo
yers
of
higher
isk
AI
systems
in
accordance
with
Article
49(3
)
The
following
information
shall
be
provid
ed
and
thereaf
ter
kept
up
to
date
with
rega
rd
to
higher
isk
AI
systems
to
be
register
ed
in
accordance
with
Article
49(3
):
or
Article
27
of
Directive
(
EU
)
as
specified
in
Article
26(8
)
of
this
Regulation
,
where
applicable
.
Information
to
be
submitted
upon
the
registration
of
higher
isk
AI
systems
listed
in
Annex
III
in
relation
to
testi
ng
in
real
world
conditions
in
accordance
with
Article
60
The
following
information
shall
be
provid
ed
and
thereaf
ter
kept
up
to
date
with
regard
to
testing
in
real
world
conditions
to
be
registered
in
accordance
with
Article
60
:
real
world
conditions
;
the
syste
m
;
X
Union
legislative
acts
on
large
-
scale
IT
systems
in
the
area
of
Freedom
,
security
and
Justice
the
Schengen
Information
System
for
the
retur
n
of
illegally
staying
third
-
countr
y
nationals
.
(
b)Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
28
November
2018
on
the
establishment
,
operation
and
use
of
the
Schengen
Information
System
(
SIS
)
in
the
field
of
border
checks
,
and
amending
the
Convention
implementing
the
Scheng
en
Agreement
,
and
amending
and
repealing
Regulation
(
EC
)
No
(
12.2018
,
p.
14
)
.
(
c)Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
28
November
2018
on
the
establishment
,
operation
and
use
of
the
Sche
ngen
Information
Syste
m
(
SIS
)
in
the
field
of
police
cooperation
and
judicial
cooperation
in
criminal
matt
ers
,
amending
and
repealing
Council
Decision
/JHA
,
and
repealing
Regulation
(
EC
)
No
of
the
European
Parliame
nt
and
of
the
Council
and
Commission
Decision
/EU
(
12.2018
,
p.
56
)
.
Regulations
(
EU
)
No
,
(
EU
)
,
(
EU
)
,
(
EU
)
and
(
EU
)
as
regard
s
the
establishment
of
the
conditions
for
accessing
other
EU
information
systems
for
the
purposes
of
the
Visa
Information
Syste
m
(
7.2021
,
p.
1
)
.
(
b)Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
7
July
2021
amending
Regulations
(
EC
)
No
,
(
EC
)
No
,
(
EU
)
,
(
EU
)
,
(
EU
)
,
(
EU
)
,
(
EU
)
,
(
EU
)
and
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
repealing
Council
Decisions
/EC
and
/JHA
,
for
the
purpose
of
reforming
the
Visa
Information
Syst
em
(
7.2021
,
p.
11
)
.
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
14
May
2024
on
the
establishment
of
‘
Eurodac
’
for
the
comp
arison
of
biometric
data
in
order
to
effectively
apply
Regulations
(
EU
)
and
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
Council
Directive
/EC
and
to
identify
illegally
staying
third
-
countr
y
nationals
and
stateless
persons
and
on
requests
for
the
compari
son
with
Eurodac
data
by
Member
States
’
law
enforcement
authorities
and
Europol
for
law
enforcement
purposes
,
amending
Regulations
(
EU
)
and
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
repealing
Regulation
(
EU
)
No
of
the
European
Parliament
and
of
the
Council
.
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
30
November
2017
establishing
an
Entr
y
/
Exit
System
(
EES
)
to
regist
er
entr
y
and
exit
data
and
refusal
of
entr
y
data
of
third
-
countr
y
nationals
crossing
the
exte
rnal
borders
of
the
Member
States
and
determining
the
conditions
for
access
to
the
EES
for
law
enforcement
purposes
,
and
amending
the
Conve
ntion
implementing
the
Schengen
Agreement
and
Regulations
(
EC
)
No
and
(
EU
)
No
(
12.2017
,
p.
20
)
.
a
European
Travel
Information
and
Author
isation
Syst
em
(
ETIAS
)
and
amending
Regulations
(
EU
)
No
,
(
EU
)
No
,
(
EU
)
,
(
EU
)
and
(
EU
)
.
(
b)Regulation
(
EU
)
of
the
European
Parliame
nt
and
of
the
Council
of
12
Sept
ember
2018
amending
Regulation
(
EU
)
for
the
purpose
of
establishing
a
European
Trave
l
Information
and
Author
isation
Syste
m
(
ETIAS
)
(
9.2018
,
p.
72
)
.
.
European
Criminal
Records
Information
Syst
em
on
third
-
countr
y
nationals
and
stateless
persons
Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
17
Apr
il
2019
establishing
a
centralised
system
for
the
identifica
tion
of
Member
States
holding
conviction
information
on
third
-
countr
y
nationals
and
statele
ss
persons
(
ECRIS-
TCN
)
to
supplement
the
European
Criminal
Records
Information
Syste
m
and
amending
Regulation
(
EU
)
(
5.2019
,
p.
1
)
.
a
framework
for
interoperability
between
EU
information
systems
in
the
field
of
borders
and
visa
and
amending
Regulations
(
EC
)
No
,
(
EU
)
,
(
EU
)
,
(
EU
)
,
(
EU
)
and
(
EU
)
of
the
European
Parliament
and
of
the
Council
and
Council
Decisions
/EC
and
/JHA
(
5.2019
,
p.
27
)
.
(
b)Regulation
(
EU
)
of
the
European
Parliament
and
of
the
Council
of
20
May
2019
on
establishing
a
framework
for
interoperability
between
EU
information
systems
in
the
field
of
police
and
judicial
cooperation
,
asylum
and
migration
and
amending
Regulations
(
EU
)
,
(
EU
)
and
(
EU
)
(
5.2019
,
p.
85
)
.
XI
Technical
document
ation
referred
to
in
Article
53(1
)
,
point
(
a
)
—
technical
document
ation
for
providers
of
general
-
pur
pose
AI
models
Section
1
Information
to
be
provided
by
all
providers
of
general
-
pur
pose
AI
models
The
technical
documentation
refer
red
to
in
Article
53(1
)
,
point
(
a
)
shall
contain
at
least
the
following
information
as
appropriate
to
the
size
and
risk
prof
ile
of
the
model
:
integrat
ed
;
(
b)the
accep
table
use
policies
applicable
;
(
c)the
date
of
release
and
methods
of
distr
ibution
;
(
d)the
arch
itecture
and
number
of
parameter
s
;
(
e)the
modality
(
e.g.
text
,
imag
e
)
and
format
of
inputs
and
outputs
;
(
f)the
licence
.
for
the
development
,
including
the
followi
ng
elements
:
(
a)the
technical
means
(
e.g.
instr
uctions
of
use
,
infrastructure
,
tools
)
required
for
the
general
-
pur
pose
AI
model
to
be
integrat
ed
in
AI
systems
;
(
b)the
design
specific
ations
of
the
model
and
training
process
,
including
training
methodologies
and
techniques
,
the
key
design
choices
including
the
rationale
and
assum
ptions
made
;
what
the
model
is
designed
to
optimise
for
and
the
relevance
of
the
diffe
rent
parameters
,
as
applicable
;
(
c)information
on
the
data
used
for
training
,
testing
and
validation
,
where
applicable
,
including
the
type
and
prove
nance
of
data
and
curation
methodologies
(
e.g.
cleaning
,
filter
ing
,
etc
.
)
,
the
number
of
data
points
,
their
scope
and
main
charact
eristics
;
how
the
data
was
obtained
and
selecte
d
as
well
as
all
other
measures
to
detect
the
unsuitability
of
data
sources
and
methods
to
detect
identifia
ble
biases
,
where
applicable
;
(
d)the
computational
resources
used
to
train
the
model
(
e.g.
number
of
floating
point
operations
)
,
training
time
,
and
other
relevant
details
related
to
the
training
;
(
e)kno
wn
or
estimated
energy
consump
tion
of
the
model
.
With
regard
to
point
(
e
)
,
where
the
energy
consump
tion
of
the
model
is
unknow
n
,
the
energy
consump
tion
may
be
based
on
information
about
comp
utational
resources
used
.
Section
2
Additional
information
to
be
provided
by
provid
ers
of
general
-
pur
pose
AI
models
with
syste
mic
risk
evaluation
prot
ocols
and
tools
or
other
wise
of
other
evaluation
methodologies
.
Evaluation
strategies
shall
include
evaluation
criteria
,
metr
ics
and
the
methodology
on
the
identifica
tion
of
limitations
.
external
adversar
ial
testing
(
e.g.
red
teaming
)
,
model
adaptations
,
including
alignment
and
fine
-
tuning
.
.
Where
applicable
,
a
detailed
descr
iption
of
the
system
arch
itecture
explaining
how
software
com
ponents
build
or
feed
into
each
other
and
integrat
e
into
the
overa
ll
processing
.
XII
Transparency
infor
mation
refe
rred
to
in
Article
53(1
)
,
point
(
b
)
—
technical
document
ation
for
providers
of
general
-
pur
pose
AI
models
to
downstre
am
providers
that
integ
rate
the
model
into
their
AI
system
The
information
refer
red
to
in
Article
53(1
)
,
point
(
b
)
shall
contain
at
least
the
following
:
integrat
ed
;
(
b)the
accep
table
use
policies
applicable
;
(
c)the
date
of
release
and
methods
of
distr
ibution
;
(
d)how
the
model
interac
ts
,
or
can
be
used
to
interact
,
with
hardware
or
software
that
is
not
part
of
the
model
itself
,
where
applicable
;
(
e)the
versions
of
relevant
software
related
to
the
use
of
the
general
-
pur
pose
AI
model
,
where
applicable
;
(
f)the
arch
itecture
and
number
of
parameter
s
;
(
g)the
modality
(
e.g.
text
,
imag
e
)
and
format
of
inputs
and
outputs
;
(
h)the
licence
for
the
model
.
be
integrat
ed
into
AI
systems
;
(
b)the
modality
(
e.g.
text
,
image
,
etc
.
)
and
format
of
the
inputs
and
outputs
and
their
maximum
size
(
e.g.
cont
ext
window
length
,
etc
.
)
;
(
c)information
on
the
data
used
for
training
,
testing
and
validation
,
where
applicable
,
including
the
type
and
prove
nance
of
data
and
curation
methodologies
.
Criter
ia
for
the
designation
of
general
-
pur
pose
AI
models
with
systemic
risk
refe
rred
to
in
Article
51
For
the
purpose
of
determining
that
a
general
-
pur
pose
AI
model
has
capabilities
or
an
imp
act
equivalent
to
those
set
out
in
Article
51(1
)
,
point
(
a
)
,
the
Commission
shall
take
into
account
the
following
criteria
:
(
a
)
the
number
of
parameter
s
of
the
model
;
(
b
)
the
quality
or
size
of
the
data
set
,
for
example
measured
through
tokens
;
(
c
)
the
amount
of
comp
utation
used
for
training
the
model
,
measured
in
floating
point
operations
or
indicate
d
by
a
combination
of
other
variables
such
as
estimated
cost
of
training
,
estimat
ed
time
required
for
the
training
,
or
estimated
energy
consump
tion
for
the
training
;
(
d
)
the
input
and
output
modalities
of
the
model
,
such
as
text
to
text
(
large
languag
e
models
)
,
text
to
imag
e
,
multi
-
modality
,
and
the
state
of
the
art
thresholds
for
determi
ning
high
-
impact
capabilities
for
each
modality
,
and
the
specific
type
of
inputs
and
outputs
(
e.g.
biological
sequences
)
;
(
e
)
the
benchmarks
and
evaluations
of
capabilities
of
the
model
,
including
consider
ing
the
number
of
tasks
without
additional
training
,
adap
tability
to
learn
new
,
distinct
tasks
,
its
level
of
autonom
y
and
scalability
,
the
tools
it
has
access
to
;
(
f
)
whether
it
has
a
high
impact
on
the
internal
market
due
to
its
reac
h
,
which
shall
be
presumed
when
it
has
been
made
available
to
at
least
10
000
registered
business
users
established
in
the
Union
;
(
g
)
the
number
of
register
ed
end
-
users
.
