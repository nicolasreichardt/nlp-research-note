REGUL ATION (EU) 2024/1689 OF THE EUR OPEAN PARLIAMENT AND OF THE COUNCIL
of 13 June 2024
laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, 
(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and 
Directiv es 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)
(Text with EEA relevance)
THE EUR OPEAN PARLIAMENT AND THE COUNCIL OF THE EUR OPEAN UNION,
Having regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof,
Having regard to the proposal from the European Commission,
After transmission of the draf t legislative act to the national parliaments,
Having regard to the opinion of the European Economic and Social Committe e (1),
Having regard to the opinion of the European Central Bank (2),
Having regard to the opinion of the Committee of the Regions (3),
Acting in accordance with the ordinar y legislative procedure (4),
Whereas:
(1) The purpose of this Regulation is to imp rove the functioning of the internal marke t by laying down a unif orm legal 
framew ork in particular for the development, the placing on the market, the putting into service and the use of 
artificial intelligence syste ms (AI systems) in the Union, in accordance with Union values, to promote the uptak e of 
human centr ic and trustwor thy artificial intellig ence (AI) while ensur ing a high level of prot ection of health, safety, 
fundamental rights as enshr ined in the Char ter of Fundamental Rights of the European Union (the ‘Char ter’), 
including democracy , the rule of law and environmental prot ection, to protect against the harmful effects of AI 
syste ms in the Union, and to suppor t inno vation. This Regulation ensures the free moveme nt, cross-border , of 
AI-based goods and services, thus preventing Member States from imp osing restr ictions on the development, 
mark eting and use of AI systems, unless explicitly author ised by this Regulation.
(2) This Regulation should be applied in accordance with the values of the Union enshr ined as in the Char ter, facilitating 
the protection of natural persons, under takings, democracy , the rule of law and environmental prot ection, while 
boosting innovation and emplo yment and making the Union a leader in the uptake of trustwo rthy AI.
(3) AI systems can be easily deplo yed in a large variety of sectors of the economy and many parts of society , including 
across borders, and can easily circulat e throughout the Union. Certain Member States have already explored the 
adop tion of national rules to ensure that AI is trustwor thy and safe and is developed and used in accordance with 
fundamental rights obliga tions. Diverging national rules may lead to the fragmentation of the inter nal market and 
may decrease legal certainty for operators that develop, import or use AI systems. A consistent and high level of 
prot ection throughout the Union should theref ore be ensured in order to achi eve trustw orthy AI, while divergence s 
ham pering the free circulation, inno vation, deplo yment and the uptak e of AI systems and related products and 
services within the inter nal marke t should be prevent ed by laying down unif orm obligations for operato rs and 
Offi cial Jour nal 
of the European UnionEN 
L series
2024/1689 12.7.2024
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 1/144(1) OJ C 517, 22.12.2021, p. 56.
(2) OJ C 115, 11.3.2022, p. 5.
(3) OJ C 97, 28.2.2022, p. 60.
(4) Position of the European Parliament of 13 March 2024 (not yet published in the Official Jour nal) and decision of the Council of 
21 May 2024.guarante eing the unif orm protect ion of overriding reasons of public interest and of rights of persons throughout the 
internal marke t on the basis of Article 114 of the Treaty on the Functioning of the European Union (TFEU). To the 
exte nt that this Regulation contains specif ic rules on the protect ion of individuals with regard to the processing of 
personal data concer ning restr ictions of the use of AI syste ms for remote biometr ic identification for the purpose of 
law enforcement, of the use of AI systems for risk assessments of natural persons for the purpose of law 
enforcement and of the use of AI systems of biometr ic cate gorisation for the purpose of law enforcement, it is 
appropr iate to base this Regulation, in so far as those specific rules are concer ned, on Article 16 TFEU. In light of 
those specific rules and the recourse to Article 16 TFEU, it is appropr iate to consult the European Data Protection 
Board.
(4) AI is a fast evolving family of technologies that contr ibut es to a wide array of economic, environmental and societal 
benefits across the entire spectr um of industr ies and social activities. By imp roving prediction, optimising operations 
and resource allocation, and personalising digital solutions available for individuals and organisations, the use of AI 
can provide key comp etitive advantages to under takings and suppor t socially and environmentally beneficial 
outcomes, for exam ple in healthcare, agriculture, food safety , education and training, media, spor ts, culture, 
infrastr ucture manag ement, energy , transpor t and logistics, public services, secur ity, justice, resource and energy 
efficiency , environmental monitoring, the conser vation and restoration of biodiversity and ecosystems and climate 
change mitig ation and adap tation.
(5) At the same time, depending on the circumstances regarding its specific application, use, and level of technological 
development, AI may generate risks and cause harm to public interests and fundamental rights that are protect ed by 
Union law. Such harm might be material or immater ial, including physical, psychological, societal or economic 
harm.
(6) Given the major impact that AI can have on society and the need to build trust, it is vital for AI and its regulator y 
framew ork to be developed in accordance with Union values as enshr ined in Article 2 of the Treaty on European 
Union (TEU), the fundamental rights and freedoms enshr ined in the Treaties and, pursuant to Article 6 TEU, the 
Char ter. As a prerequisite , AI should be a human-centr ic technology . It should serve as a tool for people, with the 
ultimate aim of increasing human well-being.
(7) In order to ensure a consistent and high level of protection of public intere sts as regard s health, safety and 
fundamental rights, common rules for high-r isk AI syste ms should be established. Those rules should be consiste nt 
with the Char ter, non-discr iminat ory and in line with the Union’s international trade commitments. They should 
also take into account the European Declaration on Digital Rights and Principles for the Digital Decade and the 
Ethics guidelines for trustw orthy AI of the High-Level Exper t Group on Artificial Intellig ence (AI HLEG).
(8) A Union lega l framew ork laying down harmonised rules on AI is theref ore needed to foster the development, use 
and uptake of AI in the inter nal marke t that at the same time meets a high level of protect ion of public interests, such 
as health and safety and the prot ection of fundamental rights, including democracy , the rule of law and 
environmental protect ion as recognised and protect ed by Union law. To achieve that objective, rules regulating the 
placing on the marke t, the putting into service and the use of certain AI systems should be laid down, thus ensur ing 
the smooth functioning of the internal market and allowi ng those systems to benefit from the principle of free 
move ment of goods and services. Those rules should be clear and robust in protecting fundamental rights, 
suppor tive of new innovative solutions, enabling a European ecosyste m of public and private actor s creating AI 
syste ms in line with Union values and unlocking the potential of the digital transf ormation across all regions of the 
Union. By laying down those rules as well as measures in suppor t of inno vation with a particular focus on small and 
medium enterprises (SMEs), including startups, this Regulation suppor ts the objective of promoting the European 
human-centr ic approach to AI and being a global leader in the development of secure, trustwor thy and ethical AI as 
stated by the European Council (5), and it ensures the prot ection of ethical principles, as specific ally request ed by the 
European Parliament (6).EN OJ L, 12.7.2024
2/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(5) European Council, Special meeting of the European Council (1 and 2 October 2020) — Conclusions, EUC O 13/20, 2020, p. 6.
(6) European Parliament resolution of 20 October 2020 with recommendations to the Commission on a framew ork of ethical aspects 
of artificia l intellig ence, robotics and relat ed technologies, 2020/2012(INL).(9) Harmonised rules applicable to the placing on the mark et, the putting into service and the use of high-r isk AI 
syste ms should be laid down consistently with Regulation (EC) No 765/2008 of the European Parliament and of the 
Council (7), Decision No 768/2008/EC of the European Parliament and of the Council (8) and Regulation (EU) 
2019/1020 of the European Parliament and of the Council (9) (New Legislative Framewo rk). The harmonised rules 
laid down in this Regulation should apply across sectors and, in line with the New Legislative Framework, should be 
without prejudice to existing Union law, in particular on data protect ion, consumer prot ection, fundamental rights, 
employment, and prot ection of work ers, and product safety , to which this Regulation is comp lementar y. As 
a consequence, all rights and remedies provid ed for by such Union law to consumers, and other persons on whom 
AI syste ms may have a nega tive impact, including as rega rds the comp ensation of possible damage s pursuant to 
Council Directive 85/374/EEC (10) remain unaff ected and fully applicable. Further more, in the context of 
employment and protect ion of workers, this Regulation should theref ore not affect Union law on social policy and 
national labour law, in compliance with Union law, concer ning emplo yment and working conditions, including 
health and safety at work and the relationship between emp loyers and workers. This Regulation should also not 
affect the exercise of fundamental rights as recognised in the Member States and at Union level, including the right or 
freedom to strike or to take other action covered by the specif ic industr ial relations syste ms in Member States as well 
as the right to negotiat e, to conclude and enforce collective agreements or to take collective action in accordance 
with national law. This Regulation should not affect the provisions aiming to improve working conditions in 
platf orm work laid down in a Directive of the European Parliament and of the Council on imp roving working 
conditions in platf orm work. Moreover , this Regulation aims to strengthen the effectiveness of such existing rights 
and remedies by establishing specific requirements and obliga tions, including in respect of the transparency , 
technical documentation and record-keepi ng of AI syste ms. Further more, the obliga tions placed on various 
operat ors involved in the AI value chain under this Regulation should apply without prejudice to national law, in 
com pliance with Union law, having the effect of limiting the use of certain AI systems where such law falls outside 
the scope of this Regulation or pursues legitimate public interest objectives other than those pursued by this 
Regulation. For exam ple, national labour law and law on the protection of minors, namely persons below the age of 
18, taking into account the UNCR C General Comment No 25 (2021) on children’s rights in relation to the digital 
environment, insofa r as they are not specif ic to AI syste ms and pursue other legitimate public interest objectives, 
should not be affected by this Regulation.
(10) The fundamental right to the prot ection of personal data is safegua rded in particular by Regulations (EU) 
2016/679 (11) and (EU) 2018/1725 (12) of the European Parliament and of the Council and Directive (EU) 2016/680 
of the European Parliament and of the Council (13). Directive 2002/58/EC of the European Parliament and of the 
Council (14) additionally prot ects private life and the confi dentiality of communications, including by way of 
providing conditions for any storing of personal and non-personal data in, and access from, terminal equipment. 
Those Union lega l acts provide the basis for sustainable and responsible data processing, including where data sets 
include a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing 
Union law gover ning the processing of personal data, including the tasks and powers of the independent super visor y 
author ities compet ent to monito r compliance with those instr uments. It also does not affect the obliga tions of 
providers and deplo yers of AI syste ms in their role as data controllers or processors stemming from Union or 
national law on the prot ection of personal data in so far as the design, the development or the use of AI syste ms 
involves the processing of personal data. It is also appropr iate to clarify that data subjects continue to enjo y all the OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 3/144(7) Regulation (EC) No 765/2008 of the European Parliament and of the Council of 9 July 2008 setting out the requirements for 
accreditation and repealing Regulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30).
(8) Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common framework for the 
mark eting of products, and repealing Council Decision 93/465/EEC (OJ L 218, 13.8.2008, p. 82).
(9) Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market surveillance and compliance 
of products and amending Directive 2004/42/EC and Regulations (EC) No 765/2008 and (EU) No 305/2011 (OJ L 169, 25.6.2019, 
p. 1).
(10) Council Directive 85/374/EEC of 25 July 1985 on the appro ximation of the laws, regulations and administrative provisions of the 
Member States concer ning liability for defe ctive products (OJ L 210, 7.8.1985, p. 29).
(11) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 Apr il 2016 on the prot ection of natural persons 
with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General 
Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1).
(12) Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of natural 
persons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the free 
movement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39).
(13) Directive (EU) 2016/680 of the European Parliament and of the Council of 27 Apr il 2016 on the prot ection of natural persons with 
regard to the processing of personal data by compet ent author ities for the purposes of the prevention, investig ation, detection or 
prosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing 
Council Framework Decision 2008/977/JHA (OJ L 119, 4.5.2016, p. 89).
(14) Directive 2002/58/EC of the European Parliament and of the Council of 12 July 2002 concer ning the processing of personal data 
and the prot ection of privacy in the electronic communications sector (Directive on privacy and electronic communications) (OJ 
L 201, 31.7.2002, p. 37).rights and guarantees awarded to them by such Union law, including the rights related to solely automat ed individual 
decision-making, including prof iling. Harmonised rules for the placing on the marke t, the putting into service and 
the use of AI systems established under this Regulation should facilitate the effective imp lementation and enable the 
exercise of the data subjects’ rights and other remedies guaranteed under Union law on the protect ion of personal 
data and of other fundamental rights.
(11) This Regulation should be without prejudice to the provisions rega rding the liability of provider s of intermediar y 
services as set out in Regulation (EU) 2022/2065 of the European Parliament and of the Council (15).
(12) The notion of ‘AI system’ in this Regulation should be clearly defined and should be closely aligned with the work of 
international organisations working on AI to ensure legal certainty , facilitate inter national conve rgence and wide 
accep tance, while providing the flexibility to accommodate the rapid technologi cal developments in this field. 
Moreo ver, the definition should be based on key char acter istics of AI systems that distinguish it from simpler 
traditional software syste ms or programming approac hes and should not cover syste ms that are based on the rules 
defined solely by natural persons to auto matically execute operations. A key character istic of AI syste ms is their 
capability to infer . This capability to infer refers to the process of obtaining the outputs, such as predictions, cont ent, 
recommendations, or decisions, which can influence physica l and virtual envir onments, and to a capability of AI 
syste ms to derive models or algor ithms, or both, from inputs or data. The techniques that enable inference while 
building an AI syste m include machi ne learning approaches that learn from data how to achieve certain objectives, 
and logic- and kno wledge-based approac hes that infer from encoded knowle dge or symbolic representation of the 
task to be solved. The capacity of an AI syste m to infer transcends basic data processing by enabling learning, 
reasoning or modelling. The term ‘mach ine-based’ refers to the fact that AI systems run on machi nes. The reference 
to explicit or implicit objectives underscores that AI syste ms can operat e according to explicit defined objectives or 
to imp licit objectives. The objectives of the AI system may be different from the intended purpose of the AI system 
in a specific cont ext. For the purposes of this Regulation, envir onments should be understood to be the contexts in 
which the AI systems operate, whereas outputs generated by the AI syste m reflect diffe rent functions perfo rmed by 
AI syste ms and include predictions, cont ent, recommendations or decisions. AI syste ms are designed to operate with 
varying levels of autonom y, meaning that they have some degree of independence of actions from human 
involvement and of capabilities to operat e without human intervention. The adap tiveness that an AI syste m could 
exhibit after deplo yment, refers to self-lear ning capabilities, allowi ng the syste m to change while in use. AI syste ms 
can be used on a stand-alone basis or as a compo nent of a product, irrespective of whether the syste m is phys ically 
integrated into the product (embedded) or serves the functionality of the product without being integrat ed therein 
(non-embedded).
(13) The notion of ‘deplo yer’ referred to in this Regulation should be interpreted as any natural or lega l person, including 
a public author ity, agency or other body , using an AI system under its author ity, excep t where the AI syste m is used 
in the course of a personal non-profess ional activity . Depending on the type of AI syste m, the use of the system may 
affect persons other than the deplo yer.
(14) The notion of ‘biometr ic data’ used in this Regulation should be inter preted in light of the notion of biometr ic data 
as defined in Article 4, point (14) of Regulation (EU) 2016/679, Article 3, point (18) of Regulation (EU) 2018/1725 
and Article 3, point (13) of Directive (EU) 2016/680. Biometr ic data can allow for the authentication, identification 
or cate gorisation of natural persons and for the recognition of emotions of natural persons.
(15) The notion of ‘biometr ic identifi cation’ referred to in this Regulation should be defined as the auto mated recognition 
of physical, physiological and behavi oural human features such as the face, eye move ment, body shape, voice, 
prosody , gait, posture, hear t rate, blood pressure, odour , keystrok es character istics, for the purpose of establishing an 
individual’s identity by compari ng biometr ic data of that individual to stored biometr ic data of individuals in 
a refere nce database, irrespective of whether the individual has given its consent or not. This excludes AI syste ms 
intended to be used for biometr ic verificati on, which includes authentication, whose sole purpose is to confi rm that 
a specific natural person is the person he or she claims to be and to confi rm the identity of a natural person for the 
sole purpose of having access to a service, unlocking a device or having secur ity access to premises.EN OJ L, 12.7.2024
4/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(15) Regulation (EU) 2022/2065 of the European Parliament and of the Council of 19 October 2022 on a Sing le Market For Digital 
Services and amending Directive 2000/31/EC (Digital Services Act) (OJ L 277, 27.10.2022, p. 1).(16) The notion of ‘biometr ic catego risation’ refer red to in this Regulation should be defined as assigning natural persons 
to specif ic cate gories on the basis of their biometr ic data. Such specif ic catego ries can relate to aspects such as sex, 
age, hair colour , eye colour , tattoos, behavio ural or personality traits, languag e, religion, membership of a national 
minor ity, sexual or political orientation. This does not include biometr ic catego risation systems that are a purely 
ancillar y feature intrinsically linked to another commercial service, meaning that the feature cannot, for objective 
technical reasons, be used without the principal service, and the integration of that feature or functionality is not 
a means to circum vent the applicability of the rules of this Regulation. For example, filters cate gorising facial or body 
features used on online marke tplaces could constitute such an ancillar y feature as they can be used only in relation to 
the principal service which consists in selling a product by allowing the consumer to preview the displa y of the 
product on him or herself and help the consumer to mak e a purcha se decision. Filte rs used on online social netw ork 
services which cate gorise facial or body features to allow users to add or modify pictures or videos could also be 
considered to be ancillar y feature as such filter cannot be used without the principal service of the social netw ork 
services consisting in the shar ing of cont ent online.
(17) The notion of ‘remot e biometr ic identifica tion syste m’ referred to in this Regulation should be defined functionally , 
as an AI syste m intended for the identifica tion of natural persons without their active involvement, typically at 
a distance, through the comp arison of a person’s biometr ic data with the biometr ic data contained in a reference 
database, irrespectively of the particular technology , processes or types of biometr ic data used. Such remote 
biometr ic identifica tion systems are typically used to perceive multiple persons or their behavio ur simultaneously in 
order to facilitate signif icantly the identification of natural persons without their active involvement. This excludes 
AI syste ms intended to be used for biometr ic verification, which includes authentication, the sole purpose of which 
is to confi rm that a specif ic natural person is the person he or she claims to be and to confi rm the identity of 
a natural person for the sole purpose of having access to a service, unloc king a device or having secur ity access to 
premises. That exclusion is justified by the fact that such syste ms are likely to have a minor imp act on fundamental 
rights of natural persons comp ared to the remote biometr ic identifi cation syste ms which may be used for the 
processing of the biometr ic data of a large number of persons without their active involvement. In the case of 
‘real-time’ syste ms, the captu ring of the biometr ic data, the comp arison and the identifi cation occur all 
instantaneously , near -instantaneously or in any event without a significant dela y. In this regard, there should be no 
scope for circum venting the rules of this Regulation on the ‘real-time’ use of the AI systems concer ned by providing 
for minor dela ys. ‘Real-time’ syste ms involve the use of ‘live’ or ‘near -live’ material, such as video footage , generat ed 
by a camera or other device with similar functionality . In the case of ‘post’ systems, in contrast, the biometr ic data 
has already been captured and the comp arison and identifica tion occur only after a significant dela y. This involves 
mat erial, such as pictures or video footage generated by closed circuit television cameras or private devices, which 
has been generated before the use of the syste m in respect of the natural persons concer ned.
(18) The notion of ‘emotion recognition system’ referred to in this Regulation should be defined as an AI syste m for the 
purpose of identifying or inferr ing emotions or intentions of natural persons on the basis of their biometr ic data. 
The notion refers to emotions or intent ions such as happiness, sadness, anger , surprise, disgust, embar rassment, 
excitement, shame, contem pt, satisfaction and amusement. It does not include physical states, such as pain or 
fatigue, including, for exam ple, syste ms used in detecting the state of fatigue of profe ssional pilots or drivers for the 
purpose of preventing accidents. This does also not include the mere detection of readily apparent expressions, 
gestures or movements, unless they are used for identifying or inferr ing emotions. Those expressions can be basic 
facial expressions, such as a frown or a smile, or gestures such as the moveme nt of hands, arms or head, or 
characteristics of a person’s voice, such as a raised voice or whisper ing.
(19) For the purposes of this Regulation the notion of ‘publicly accessible space’ should be understood as referring to any 
phys ical space that is accessible to an undeter mined number of natural persons, and irrespective of whether the 
space in question is privately or publicly owned, irrespective of the activity for which the space may be used, such as 
for commerce, for exam ple, shops, restaurants, cafés; for services, for exam ple, banks, professional activities, 
hospitality ; for spor t, for exam ple, swimming pools, gyms, stadiums; for transpor t, for exam ple, bus, metro and 
railwa y stations, airports, means of transpor t; for entertainment, for exam ple, cinemas, theatres, museums, concer t 
and confe rence halls; or for leisure or other wise, for example, public roads and squares, park s, forests, playgrounds. 
A space should also be classified as being publicly accessible if, regard less of pote ntial capacity or secur ity 
restr ictions, access is subject to certain predete rmined conditions which can be fulfilled by an undeter mined number 
of persons, such as the purc hase of a ticket or title of transpor t, prior registration or having a certain age. In contrast, 
a space should not be considered to be publicly accessible if access is limited to specif ic and defined natural persons 
through either Union or national law directly related to public safety or secur ity or through the clear manif estation OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 5/144of will by the person having the relevant author ity over the space. The factual possibility of access alone, such as an 
unloc ked door or an open gate in a fence, does not imp ly that the space is publicly accessible in the presence of 
indications or circumstances suggesting the contrar y, such as. signs prohibiting or restr icting access. Compan y and 
factory premises, as well as offices and workplaces that are intende d to be accessed only by relevant emp loyees and 
service providers, are spaces that are not publicly accessible. Publicly accessible spaces should not include prisons or 
border control. Some other spaces may compr ise both publicly accessible and non-publicly accessible spaces, such as 
the hallwa y of a private residential building necessar y to access a docto r’s office or an airport. Online spaces are not 
covered, as they are not physical spaces. Whether a given space is accessible to the public should howe ver be 
determined on a case-by-c ase basis, having regard to the specif icities of the individual situation at hand.
(20) In order to obtain the great est benefits from AI syste ms while prot ecting fundamental rights, health and safety and to 
enable democratic control, AI literacy should equip providers, deplo yers and affect ed persons with the necessar y 
notions to mak e informed decisions regarding AI systems. Those notions may vary with regard to the relevant 
cont ext and can include understanding the correct application of technical elements during the AI system’s 
development phase, the measures to be applied during its use, the suitable ways in which to inter pret the AI system’s 
output, and, in the case of affected persons, the kno wledge necessar y to understand how decisions taken with the 
assistance of AI will have an imp act on them. In the context of the application this Regulation, AI literacy should 
provide all relevant actors in the AI value chain with the insights required to ensure the appropr iate compliance and 
its correct enforcement. Further more, the wide implementation of AI literac y measures and the introduction of 
appropr iate follow-up actions could contr ibut e to impro ving work ing conditions and ultimately sustain the 
consolidation, and inno vation path of trustwor thy AI in the Union. The European Artificial Intellig ence Board (the 
‘Board’) should suppor t the Commission, to promot e AI litera cy tools, public awareness and understanding of the 
benefits, risks, safegua rds, rights and obliga tions in relation to the use of AI syste ms. In cooperation with the relevant 
stak eholders, the Commission and the Member States should facilitate the drawing up of voluntar y codes of conduct 
to advance AI literacy among persons dealing with the development, operation and use of AI.
(21) In order to ensure a level playing field and an effective protect ion of rights and freedoms of individuals across the 
Union, the rules established by this Regulation should apply to provid ers of AI syste ms in a non-discr iminat ory 
manner , irrespective of whether they are established within the Union or in a third countr y, and to deplo yers of AI 
syste ms established within the Union.
(22) In light of their digital nature, certain AI syste ms should fall within the scope of this Regulation even when they are 
not placed on the marke t, put into service, or used in the Union. This is the case, for example, where an operator 
established in the Union contracts certain services to an operator established in a third countr y in relation to an 
activity to be perform ed by an AI system that would qualify as high-r isk. In those circumstances, the AI syste m used 
in a third countr y by the operator could process data lawfully collected in and transferre d from the Union, and 
provide to the contracting operato r in the Union the output of that AI system resulting from that processing, 
without that AI syste m being placed on the marke t, put into service or used in the Union. To prevent the 
circum vention of this Regulation and to ensure an effective prot ection of natural persons located in the Union, this 
Regulation should also apply to providers and deplo yers of AI systems that are established in a third countr y, to the 
exte nt the output produced by those syste ms is intende d to be used in the Union. Nonetheless, to take into account 
existing arrang ements and special needs for future cooperation with foreign partners with whom information and 
evidence is exchang ed, this Regulation should not apply to public author ities of a third countr y and inter national 
organisations when acting in the framework of cooperation or inter national agreements concluded at Union or 
national level for law enforcement and judicial cooperation with the Union or the Member States, provided that the 
relevant third countr y or international organisation provides adequate safegua rds with respect to the prot ection of 
fundamental rights and freedoms of individuals. Where relevant, this may cover activities of entities entr usted by the 
third countr ies to carry out specif ic tasks in suppor t of such law enforcement and judicial cooperation. Such 
framew ork for cooperation or agreements have been established bilat erally between Member States and third 
countr ies or between the European Union, Europol and other Union agencies and third countr ies and international 
organisations. The author ities compet ent for super vision of the law enforcement and judicial author ities under this 
Regulation should assess whether those framew orks for cooperation or inter national agreements include adequate 
safeguards with respect to the protect ion of fundamental rights and freedoms of individuals. Recipient national EN OJ L, 12.7.2024
6/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojauthor ities and Union institutions, bodies, offices and agencies making use of such outputs in the Union remain 
accountable to ensure their use comp lies with Union law. When those inter national agreements are revised or new 
ones are concluded in the future, the contracting parties should make utmost efforts to align those agreements with 
the requirements of this Regulation.
(23) This Regulation should also apply to Union institutions, bodies, offices and agencies when acting as a provid er or 
deplo yer of an AI syste m.
(24) If, and insofa r as, AI systems are placed on the marke t, put into service, or used with or without modification of such 
syste ms for militar y, defe nce or national secur ity purposes, those should be excluded from the scope of this 
Regulation regard less of which type of entity is carrying out those activities, such as whether it is a public or private 
entity . As regard s militar y and defe nce purposes, such exclusion is justif ied both by Article 4(2) TEU and by the 
specificities of the Member States’ and the common Union defence policy covered by Chapt er 2 of Title V TEU that 
are subject to public inter national law, which is theref ore the more appropr iate legal framew ork for the regulation of 
AI systems in the cont ext of the use of lethal force and other AI syste ms in the context of militar y and defence 
activities. As rega rds national secur ity purposes, the exclusion is justified both by the fact that national secur ity 
remains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specif ic nature and 
operational needs of national secur ity activities and specific national rules applicable to those activities. Nonetheless, 
if an AI system developed, placed on the marke t, put into service or used for militar y, defence or national secur ity 
purposes is used outside those temporar ily or permanently for other purposes, for exam ple, civilian or humanitar ian 
purposes, law enforcement or public secur ity purposes, such a system would fall within the scope of this Regulation. 
In that case, the entity using the AI syste m for other than militar y, defe nce or national secur ity purposes should 
ensure the compliance of the AI syste m with this Regulation, unless the syste m is already compliant with this 
Regulation. AI syste ms placed on the marke t or put into service for an excluded purpose, namely militar y, defe nce or 
national secur ity, and one or more non-exclu ded purposes, such as civilian purposes or law enforcement, fall within 
the scope of this Regulation and providers of those systems should ensure compliance with this Regulation. In those 
cases, the fact that an AI system may fall within the scope of this Regulation should not affect the possibility of 
entities carrying out national secur ity, defence and militar y activities, rega rdless of the type of entity carrying out 
those activities, to use AI systems for national secur ity, militar y and defe nce purposes, the use of which is excluded 
from the scope of this Regulation. An AI system placed on the marke t for civilian or law enforcement purposes 
which is used with or without modifi cation for militar y, defence or national secur ity purposes should not fall within 
the scope of this Regulation, rega rdless of the type of entity carrying out those activities.
(25) This Regulation should suppor t inno vation, should respect freedom of science, and should not under mine research 
and development activity . It is theref ore necessar y to exclude from its scope AI syste ms and models specifically 
developed and put into service for the sole purpose of scientifi c research and development. Moreove r, it is necessar y 
to ensure that this Regulation does not other wise affect scientific research and development activity on AI syste ms or 
models prior to being placed on the market or put into service. As regards product-or iented research, testing and 
development activity rega rding AI syste ms or models, the provisions of this Regulation should also not apply prior 
to those syste ms and models being put into service or placed on the marke t. That exclusion is without prejudice to 
the obligation to comp ly with this Regulation where an AI system falling into the scope of this Regulation is placed 
on the marke t or put into service as a result of such research and development activity and to the application of 
provisions on AI regulato ry sandbo xes and testing in real world conditions. Further more, without prejudice to the 
exclusion of AI syste ms specif ically developed and put into service for the sole purpose of scientifi c researc h and 
development, any other AI system that may be used for the conduct of any research and development activity should 
remain subject to the provisions of this Regulation. In any event, any research and development activity should be 
carried out in accordance with recognised ethical and profe ssional standards for scientific research and should be 
conducte d in accordance with applicable Union law.
(26) In order to introduce a propor tionate and effective set of binding rules for AI syste ms, a clearly defined risk-based 
approac h should be follo wed. That approac h should tailor the type and content of such rules to the intensity and 
scope of the risks that AI syste ms can generate. It is theref ore necessar y to prohibit certain unaccept able AI practices, 
to lay down requirements for high-r isk AI systems and obligati ons for the relevant operators, and to lay down 
transparency obligations for certain AI syste ms.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 7/144(27) While the risk-based approac h is the basis for a propor tionate and effective set of binding rules, it is imp ortant to 
recall the 2019 Ethics guidelines for trustwo rthy AI developed by the independent AI HLEG appoint ed by the 
Commission. In those guidelines, the AI HLEG developed seven non-binding ethical principles for AI which are 
intended to help ensure that AI is trustwor thy and ethically sound. The seven principles include human agency and 
oversight ; technical robustness and safety; privacy and data gover nance; transparency ; diversity , non-discr imination 
and fairness; societal and envir onmental well-being and accountability . Without prejudice to the legally binding 
requirements of this Regulation and any other applicable Union law, those guidelines contr ibut e to the design of 
coherent, trustwor thy and human-centr ic AI, in line with the Char ter and with the values on which the Union is 
founded. According to the guidelines of the AI HLEG, human agency and oversight means that AI systems are 
developed and used as a tool that serves people, respects human dignity and personal autonom y, and that is 
functioning in a way that can be appropr iately controlled and overseen by humans. Technical robustness and safety 
means that AI syste ms are developed and used in a way that allows robustness in the case of problems and resilience 
against attempts to alter the use or perform ance of the AI syste m so as to allow unla wful use by third parties, and 
minimise uninte nded harm. Privacy and data gove rnance means that AI syste ms are developed and used in 
accordance with privacy and data prot ection rules, while processing data that meets high standards in terms of 
quality and integrity. Transparency means that AI syste ms are developed and used in a way that allows appropr iate 
traceability and explainability , while making humans aware that they communicate or interact with an AI syste m, as 
well as duly informing deplo yers of the capabilities and limitations of that AI system and affected persons about their 
rights. Diversity , non-discr imination and fairne ss means that AI syste ms are developed and used in a way that 
includes diverse actor s and promotes equal access, gender equality and cultural diversity , while avoiding 
discr iminatory imp acts and unfa ir biases that are prohibite d by Union or national law. Social and envir onmental 
well-being means that AI systems are developed and used in a sustainable and environmentally friendly manner as 
well as in a way to benefi t all human beings, while monitori ng and assessing the long-term imp acts on the 
individual, society and democracy . The application of those principles should be translat ed, when possible, in the 
design and use of AI models. They should in any case serve as a basis for the draf ting of codes of conduct under this 
Regulation. All stakeholders, including industr y, academia, civil society and standardisation organisations, are 
encourag ed to take into account, as appropr iate, the ethical principles for the development of voluntary best 
practices and standards.
(28) Aside from the many beneficial uses of AI, it can also be misused and provide novel and powerful tools for 
manipulative, exploitative and social control practices. Such practices are particularly harmful and abusive and 
should be prohibite d because they contradict Union values of respect for human dignity , freedom, equality , 
democracy and the rule of law and fundamental rights enshr ined in the Char ter, including the right to 
non-discr imination, to data prot ection and to privacy and the rights of the child.
(29) AI-enabled manipulative techniques can be used to persuade persons to engage in unwant ed behaviours, or to 
deceive them by nudging them into decisions in a way that subver ts and imp airs their autonom y, decision-making 
and free choices. The placing on the marke t, the putting into service or the use of certain AI syste ms with the 
objective to or the effect of mat erially distor ting human behavio ur, whereb y signifi cant harms, in particular having 
sufficiently imp ortant adverse impacts on phys ical, psychological health or financ ial intere sts are likely to occur , are 
particularly dangerous and should theref ore be prohibite d. Such AI syste ms deplo y subliminal comp onents such as 
audio, image, video stimuli that persons cannot perceive, as those stimuli are beyond human percep tion, or other 
manipulative or decept ive techniques that subver t or imp air person’s auto nomy , decision-making or free choice in 
ways that people are not consciously aware of those techniques or, where they are aware of them, can still be 
deceived or are not able to control or resist them. This could be facilitated, for example, by machine-brain interfaces 
or virtual reality as they allow for a higher degree of control of what stimuli are presented to persons, insofa r as they 
may materially distor t their behavio ur in a significantly harmful manner . In addition, AI syste ms may also other wise 
exploit the vulnerabilities of a person or a specif ic group of persons due to their age, disability within the meaning of 
Directive (EU) 2019/882 of the European Parliament and of the Council (16), or a specific social or economic 
situation that is likely to mak e those persons more vulnerable to exploitation such as persons living in extreme 
poverty, ethnic or religious minor ities. Such AI syste ms can be placed on the marke t, put into service or used with 
the objective to or the effect of materially distor ting the behavi our of a person and in a manner that causes or is 
reasonably likely to cause signifi cant harm to that or another person or groups of persons, including harms that may 
be accumulated over time and should theref ore be prohibite d. It may not be possible to assume that there is an EN OJ L, 12.7.2024
8/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(16) Directive (EU) 2019/882 of the European Parliament and of the Council of 17 Apr il 2019 on the accessibility requirements for 
products and services (OJ L 151, 7.6.2019, p. 70).intention to distor t behavio ur where the distor tion results from factors exter nal to the AI syste m which are outside 
the control of the provid er or the deplo yer, namely factors that may not be reasonably foreseeable and theref ore not 
possible for the provider or the deplo yer of the AI syste m to mitigat e. In any case, it is not necessar y for the provid er 
or the deplo yer to have the intention to cause signifi cant harm, provided that such harm results from the 
manipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complement ary to the 
provisions contained in Directive 2005/29/EC of the European Parliame nt and of the Council (17), in particular unfa ir 
commercial practices leading to economic or financial harms to consumers are prohibited under all circumstances, 
irrespective of whether they are put in place through AI systems or other wise. The prohibitions of manipulative and 
exploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such as 
psychologi cal treatment of a mental disease or phys ical rehabilitation, when those practices are carried out in 
accordance with the applicable law and medical standards, for exam ple explicit consent of the individuals or their 
legal representatives. In addition, common and legitimate commercial practices, for example in the field of 
adver tising, that comply with the applicable law should not, in themselves, be rega rded as constituting harmful 
manipulative AI-enabled practices.
(30) Biometr ic categor isation syste ms that are based on natural persons’ biometr ic data, such as an individual person’s 
face or finger print, to deduce or infer an individuals ’ political opinions, trade union membership, religious or 
philosophical beliefs, race, sex life or sexual orientation should be prohibite d. That prohibition should not cover the 
lawful labelling, filter ing or categori sation of biometr ic data sets acquired in line with Union or national law 
according to biometr ic data, such as the sorting of imag es according to hair colour or eye colour , which can for 
exam ple be used in the area of law enforcement.
(31) AI systems provid ing social scor ing of natural persons by public or private actors may lead to discr iminat ory 
outcomes and the exclusion of certain groups. They may violate the right to dignity and non-discr imination and the 
values of equality and justice. Such AI syste ms evaluate or classify natural persons or groups thereof on the basis of 
multiple data points related to their social behavio ur in multiple contexts or known, inferred or predicted personal 
or personality character istics over certain periods of time. The social score obtained from such AI systems may lead 
to the detr imental or unfa vourable treatment of natural persons or whole groups thereof in social cont exts, which 
are unrelate d to the context in which the data was originally generat ed or collect ed or to a detr imental treatment that 
is dispropor tionate or unjustifie d to the gravity of their social behavio ur. AI systems entailing such unaccept able 
scor ing practices and leading to such detr imental or unfa vourable outcomes should theref ore be prohibited. That 
prohibition should not affect lawful evaluation practices of natural persons that are carried out for a specific purpose 
in accordance with Union and national law.
(32) The use of AI syste ms for ‘real-time’ remote biometr ic identification of natural persons in publicly accessible spaces 
for the purpose of law enforcement is particularly intrusive to the rights and freedoms of the concer ned persons, to 
the extent that it may affect the private life of a large part of the population, evok e a feeling of constant surveillance 
and indirectly dissuade the exercise of the freedom of assembly and other fundamental rights. Technical inaccuracies 
of AI syste ms intende d for the remote biometr ic identifi cation of natural persons can lead to biased results and entail 
discr iminatory effects. Such possible biased results and discr iminatory effects are particularly relevant with regard to 
age, ethnicity , race, sex or disabilities. In addition, the immediacy of the impact and the limited oppor tunities for 
further chec ks or corrections in relation to the use of such systems operating in real-time carry height ened risks for 
the rights and freedoms of the persons concer ned in the cont ext of, or impacted by, law enforcement activities.
(33) The use of those systems for the purpose of law enforcement should theref ore be prohibited, except in exhaustively 
listed and narrowly defined situations, where the use is strictly necessar y to achieve a substantial public interest, the 
importance of which outweighs the risks. Those situations involve the search for certain victims of crime including 
missing persons; certain threats to the life or to the phys ical safety of natural persons or of a terrorist attac k; and the 
localisation or identification of perpetrat ors or suspects of the criminal offences listed in an annex to this Regulation, 
where those criminal offences are punishable in the Member State concer ned by a custo dial sentence or a detention OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 9/144(17) Directive 2005/29/EC of the European Parliament and of the Council of 11 May 2005 concer ning unfair business-to-consumer 
commercial practices in the inter nal market and amending Council Directive 84/450/EEC, Directives 97/7/EC, 98/27/EC and 
2002/65/EC of the European Parliament and of the Council and Regulation (EC) No 2006/2004 of the European Parliament and of 
the Council (‘Unf air Commercial Practices Directive’) (OJ L 149, 11.6.2005, p. 22).order for a maximum period of at least four years and as they are defined in the law of that Member State. Such 
a threshold for the custo dial sent ence or detention order in accordance with national law contr ibut es to ensur ing 
that the offence should be serious enough to potentially justify the use of ‘real-time’ remot e biometr ic identification 
syste ms. Moreover , the list of criminal offences provided in an annex to this Regulation is based on the 32 criminal 
offences listed in the Council Framework Decision 2002/584/JHA (18), taking into account that some of those 
offences are, in practice, likel y to be more relevant than others, in that the recourse to ‘real-time’ remote biometr ic 
identification could, foreseeably , be necessar y and propor tionate to highly varying degrees for the practical pursuit 
of the localisation or identification of a perpetrat or or suspect of the different criminal offences listed and having 
rega rd to the likely differences in the seriousness, probability and scale of the harm or possible negati ve 
consequences. An imminent threat to life or the physical safety of natural persons could also result from a serious 
disruption of critical infrastr ucture, as defined in Article 2, point (4) of Directive (EU) 2022/2557 of the European 
Parliament and of the Council (19), where the disruption or destr uction of such critical infrastr ucture would result in 
an imminent threat to life or the physical safety of a person, including through serious harm to the provision of basic 
supplies to the population or to the exercise of the core function of the State. In addition, this Regulation should 
preser ve the ability for law enforcement, border control, immigration or asylum author ities to carry out identity 
checks in the presence of the person concer ned in accordance with the conditions set out in Union and national law 
for such chec ks. In particular , law enforcement, border control, immigration or asylum author ities should be able to 
use information syste ms, in accordance with Union or national law, to identify persons who, during an identity 
check, either refuse to be identified or are unable to state or prove their identity , without being required by this 
Regulation to obtain prior author isation. This could be, for exam ple, a person involved in a crime, being unwilling, 
or unable due to an accident or a medical condition, to disclose their identity to law enforcement author ities.
(34) In order to ensure that those syste ms are used in a responsible and propor tionate manner , it is also impor tant to 
establish that, in each of those exhaustively listed and narrowly defined situations, certain elements should be take n 
into account, in particular as regards the nature of the situation giving rise to the request and the consequences of 
the use for the rights and freedoms of all persons concer ned and the safeguards and conditions provid ed for with the 
use. In addition, the use of ‘real-time’ remot e biometr ic identifi cation systems in publicly accessible spaces for the 
purpose of law enforcement should be deplo yed only to confir m the specif ically targe ted individual’s identity and 
should be limit ed to what is strictly necessar y concer ning the period of time, as well as the geographic and personal 
scope, having rega rd in particular to the evidence or indications rega rding the threats, the victims or perpetrat or. The 
use of the real-time remote biometr ic identifica tion system in publicly accessible spaces should be author ised only if 
the relevant law enforcement author ity has comp leted a fundamental rights impact assessment and, unless provid ed 
other wise in this Regulation, has register ed the syste m in the database as set out in this Regulation. The reference 
database of persons should be appropr iate for each use case in each of the situations mentioned above.
(35) Each use of a ‘real-time’ remote biometr ic identification system in publicly accessible spaces for the purpose of law 
enforcement should be subject to an express and specif ic author isation by a judicial author ity or by an independent 
administrative author ity of a Member State whose decision is binding. Such author isation should, in principle, be 
obtained prior to the use of the AI system with a view to identifying a person or persons. Exceptions to that rule 
should be allowed in duly justifi ed situations on grounds of urgency , namely in situations where the need to use the 
syste ms concer ned is such as to mak e it effectively and objectively impossible to obtain an author isation before 
commencing the use of the AI system. In such situations of urgency , the use of the AI syste m should be restr icted to 
the absolute minimum necessar y and should be subject to appropr iate safeguards and conditions, as determined in 
national law and specif ied in the cont ext of each individual urgent use case by the law enforcement author ity itself. 
In addition, the law enforcement author ity should in such situations request such author isation while provid ing the 
reasons for not having been able to request it earlier , without undue dela y and at the latest within 24 hours. If such 
an author isation is rejected, the use of real-time biometr ic identification systems linked to that author isation should 
cease with immediate effect and all the data related to such use should be discarded and delete d. Such data includes EN OJ L, 12.7.2024
10/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(18) Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender procedures 
between Member States (OJ L 190, 18.7.2002, p. 1).
(19) Directive (EU) 2022/2557 of the European Parliament and of the Council of 14 December 2022 on the resilience of critical entities 
and repealing Council Directive 2008/114/EC (OJ L 333, 27.12.2022, p. 164).input data directly acquired by an AI syste m in the course of the use of such system as well as the results and outputs 
of the use linked to that author isation. It should not include input that is lega lly acquired in accordance with another 
Union or national law. In any case, no decision producing an adverse lega l effect on a person should be taken based 
solely on the output of the remot e biometr ic identification system.
(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in national 
rules, the relevant marke t surveillance author ity and the national data prot ection author ity should be notif ied of each 
use of the real-time biometr ic identification system. Market surveillance author ities and the national data prot ection 
author ities that have been notif ied should submit to the Commission an annual repor t on the use of real-time 
biometr ic identifi cation syste ms.
(37) Further more, it is appropr iate to provid e, within the exhaustive framework set by this Regulation that such use in 
the territory of a Member State in accordance with this Regulation should only be possible where and in as far as the 
Member State concer ned has decided to expressly provide for the possibility to author ise such use in its detailed rules 
of national law. Consequently , Member States remain free under this Regulation not to provide for such a possibility 
at all or to only provid e for such a possibility in respect of some of the objectives capable of justifying author ised use 
identified in this Regulation. Such national rules should be notified to the Commission within 30 days of their 
adop tion.
(38) The use of AI syste ms for real-time remote biometr ic identification of natural persons in publicly accessible spaces 
for the purpose of law enforcement necessar ily involves the processing of biometr ic data. The rules of this 
Regulation that prohibit, subject to certain exceptions, such use, which are based on Article 16 TFEU, should apply 
as lex specialis in respect of the rules on the processing of biometr ic data contained in Article 10 of Directive (EU) 
2016/680, thus regulating such use and the processing of biometr ic data involved in an exhaustive manner . 
Theref ore, such use and processing should be possible only in as far as it is compatible with the framework set by 
this Regulation, without there being scope, outside that framework, for the comp etent author ities, where they act for 
purpose of law enforcement, to use such syste ms and process such data in connection thereto on the grounds listed 
in Article 10 of Directive (EU) 2016/680. In that cont ext, this Regulation is not intended to provide the legal basis 
for the processing of personal data under Article 8 of Directive (EU) 2016/680. However , the use of real-time remote 
biometr ic identifica tion syste ms in publicly accessible spaces for purposes other than law enforcement, including by 
com petent author ities, should not be covered by the specif ic framew ork regard ing such use for the purpose of law 
enforcement set by this Regulation. Such use for purposes other than law enforcement should theref ore not be 
subject to the requirement of an author isation under this Regulation and the applicable detailed rules of national law 
that may give effect to that author isation.
(39) Any processing of biometr ic data and other personal data involved in the use of AI syste ms for biometr ic 
identification, other than in connection to the use of real-time remote biometr ic identification syste ms in publicly 
accessible spaces for the purpose of law enforcement as regulated by this Regulation, should continue to comply 
with all requirements resulting from Article 10 of Directive (EU) 2016/680. For purposes other than law 
enforcement, Article 9(1) of Regulation (EU) 2016/679 and Article 10(1) of Regulation (EU) 2018/1725 prohibit the 
processing of biometr ic data subject to limit ed exceptions as provided in those Articles. In the application of Article 
9(1) of Regulation (EU) 2016/679, the use of remot e biometr ic identifi cation for purposes other than law 
enforcement has already been subject to prohibition decisions by national data protection author ities.
(40) In accordance with Article 6a of Protocol No 21 on the position of the United Kingdom and Ireland in respect of the 
area of freedom, secur ity and justice, as annexe d to the TEU and to the TFEU, Ireland is not bound by the rules laid 
down in Article 5(1), first subparagraph, point (g), to the exte nt it applies to the use of biometr ic categor isation 
syste ms for activities in the field of police cooperation and judicial cooperation in criminal matt ers, Article 5(1), first 
subparagraph, point (d), to the exte nt it applies to the use of AI syste ms covered by that provision, Article 5(1), first 
subparagraph, point (h), Article 5(2) to (6) and Article 26(10) of this Regulation adop ted on the basis of Article 16 
TFEU which relate to the processing of personal data by the Member States when carrying out activities falling 
within the scope of Chapt er 4 or Chapt er 5 of Title V of Part Three of the TFEU, where Ireland is not bound by the 
rules gover ning the forms of judicial cooperation in criminal matt ers or police cooperation which require 
com pliance with the provisions laid down on the basis of Article 16 TFEU.
(41) In accordance with Articles 2 and 2a of Proto col No 22 on the position of Denmark, annexe d to the TEU and to the 
TFEU, Denmark is not bound by rules laid down in Article 5(1), first subparagraph, point (g), to the exte nt it applies 
to the use of biometr ic categor isation syste ms for activities in the field of police cooperation and judicial cooperation 
in criminal matt ers, Article 5(1), first subparagraph, point (d), to the extent it applies to the use of AI syste ms 
covered by that provision, Article 5(1), first subparagraph, point (h), (2) to (6) and Article 26(10) of this Regulation OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 11/144adop ted on the basis of Article 16 TFEU, or subject to their application, which relate to the processing of personal 
data by the Member States when carrying out activities falling within the scope of Chap ter 4 or Chapt er 5 of 
Title V of Part Three of the TFEU.
(42) In line with the presump tion of innocence, natural persons in the Union should alwa ys be judged on their actual 
behavi our. Natural persons should never be judg ed on AI-predicted behavio ur based solely on their prof iling, 
personality traits or char acter istics, such as nationality , place of birth, place of residence, number of children, level of 
debt or type of car, without a reasonable suspicion of that person being involved in a criminal activity based on 
objective verifiable facts and without human assessment thereof. Theref ore, risk assessments carried out with rega rd 
to natural persons in order to assess the likelihood of their offending or to predict the occur rence of an actual or 
pote ntial criminal offence based solely on prof iling them or on assessing their personality traits and character istics 
should be prohibited. In any case, that prohibition does not refer to or touch upon risk analytics that are not based 
on the profil ing of individuals or on the personality traits and charact eristics of individuals, such as AI systems using 
risk analytics to assess the likelihood of financ ial fraud by under takings on the basis of suspicious transactions or 
risk analytic tools to predict the likelihood of the localisation of narcotics or illicit goods by customs author ities, for 
exam ple on the basis of known traff icking routes.
(43) The placing on the mark et, the putting into service for that specific purpose, or the use of AI syste ms that create or 
expand facial recognition databases through the untarg eted scraping of facial images from the inter net or CCT V 
footage, should be prohibite d because that practice adds to the feeling of mass surveillance and can lead to gross 
violations of fundamental rights, including the right to privacy .
(44) There are serious concer ns about the scientific basis of AI systems aiming to identify or infer emotions, particularly 
as expression of emotions vary considerably across cultures and situations, and even within a sing le individual. 
Among the key shor tcomings of such syste ms are the limited reliability , the lack of specificity and the limit ed 
generalisability . Theref ore, AI systems identifying or inferr ing emotions or intentions of natural persons on the basis 
of their biometr ic data may lead to discr iminat ory outcomes and can be intrusive to the rights and freedoms of the 
concer ned persons. Consider ing the imbalance of power in the cont ext of work or education, combined with the 
intrusive nature of these syste ms, such systems could lead to detr imental or unfa vourable treatment of certain 
natural persons or whole groups thereof. Theref ore, the placing on the marke t, the putting into service, or the use of 
AI syste ms intended to be used to detect the emotional state of individuals in situations related to the workplace and 
education should be prohibite d. That prohibition should not cover AI systems placed on the mark et strictly for 
medical or safety reasons, such as systems intended for therapeutical use.
(45) Practices that are prohibited by Union law, including data prot ection law, non-discr imination law, consumer 
prot ection law, and competition law, should not be affected by this Regulation.
(46) High-r isk AI systems should only be placed on the Union marke t, put into service or used if they comply with 
certain mandato ry requirements. Those requirements should ensure that high-r isk AI systems available in the Union 
or whose output is other wise used in the Union do not pose unaccepta ble risks to important Union public interests 
as recognised and protect ed by Union law. On the basis of the New Legislative Framework, as clarified in the 
Commission notice ‘The “Blue Guide” on the imp lementation of EU product rules 2022’ (20), the general rule is that 
more than one lega l act of Union harmonisation legislation, such as Regulations (EU) 2017/745 (21) and (EU) 
2017/746 (22) of the European Parliament and of the Council or Directive 2006/42/EC of the European Parliament 
and of the Council (23), may be applicable to one product, since the making availa ble or putting into service can take EN OJ L, 12.7.2024
12/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(20) OJ C 247, 29.6.2022, p. 1.
(21) Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 Apr il 2017 on medical devices, amending Directive 
2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 
93/42/EEC (OJ L 117, 5.5.2017, p. 1).
(22) Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 Apr il 2017 on in vitro diagnostic medical devices and 
repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176).
(23) Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machiner y, and amending Directive 
95/16/EC (OJ L 157, 9.6.2006, p. 24).place only when the product comp lies with all applicable Union harmonisation legislation. To ensure consiste ncy 
and avoid unnecessar y administrative burdens or costs, provid ers of a product that contains one or more high-r isk 
AI syste ms, to which the requirements of this Regulation and of the Union harmonisation legislation listed in an 
annex to this Regulation apply , should have flexibility with regard to operational decisions on how to ensure 
com pliance of a product that contains one or more AI syste ms with all applicable requirements of the Union 
harmonisation legislation in an optima l manner . AI syste ms identified as high-r isk should be limit ed to those that 
have a significant harmful impact on the health, safety and fundamental rights of persons in the Union and such 
limitation should minimise any pote ntial restr iction to international trade.
(47) AI syste ms could have an adverse impact on the health and safety of persons, in particular when such syste ms 
operat e as safety comp onents of products. Consiste nt with the objectives of Union harmonisation legislation to 
facilitate the free move ment of products in the inter nal market and to ensure that only safe and other wise comp liant 
products find their way into the market, it is imp ortant that the safety risks that may be generated by a product as 
a whole due to its digital com ponents, including AI syste ms, are duly prevented and mitigat ed. For instance, 
increasing ly autono mous robots, whether in the cont ext of manuf actur ing or personal assistance and care should be 
able to safely operate and perfo rms their functions in complex environments. Similarly , in the health secto r where 
the stakes for life and health are particularly high, increasing ly sophisticat ed diagnostics systems and syste ms 
suppor ting human decisions should be reliable and accurate.
(48) The extent of the adverse imp act caused by the AI syste m on the fundamental rights protect ed by the Char ter is of 
particular relevance when classifying an AI syste m as high risk. Those rights include the right to human dignity , 
respect for private and family life, protection of personal data, freedom of expression and information, freedom of 
assembly and of association, the right to non-discr imination, the right to education, consumer protection, work ers’ 
rights, the rights of persons with disabilities, gender equality , intellectual proper ty rights, the right to an effective 
remedy and to a fair trial, the right of defence and the presump tion of innocence, and the right to good 
administration. In addition to those rights, it is impor tant to highlight the fact that children have specif ic rights as 
enshr ined in Article 24 of the Char ter and in the Unit ed Nations Convention on the Rights of the Child, further 
developed in the UNCR C General Comment No 25 as regard s the digital environment, both of which require 
consideration of the children’s vulnerabilities and provision of such protection and care as necessar y for their 
well-being. The fundamental right to a high level of envir onmental protection enshr ined in the Char ter and 
implemented in Union policies should also be considered when assessing the sever ity of the harm that an AI system 
can cause, including in relation to the health and safety of persons.
(49) As regard s high-r isk AI systems that are safety com ponents of products or syste ms, or which are themselves 
products or systems falling within the scope of Regulation (EC) No 300/2008 of the European Parliame nt and of the 
Council (24), Regulation (EU) No 167/2013 of the European Parliame nt and of the Council (25), Regulation 
(EU) No 168/2013 of the European Parliament and of the Council (26), Directive 2014/90/EU of the European 
Parliament and of the Council (27), Directive (EU) 2016/797 of the European Parliament and of the Council (28), 
Regulation (EU) 2018/858 of the European Parliament and of the Council (29), Regulation (EU) 2018/1139 of the OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 13/144(24) Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the field of 
civil aviation secur ity and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72).
(25) Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 Febr uary 2013 on the approv al and market 
surveillance of agricultural and forestr y vehicles (OJ L 60, 2.3.2013, p. 1).
(26) Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 Januar y 2013 on the approval and market 
surveillance of two- or three-wheel vehicles and quadr icycles (OJ L 60, 2.3.2013, p. 52).
(27) Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on mar ine equipment and repealing Council 
Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146).
(28) Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of the rail system 
within the European Union (OJ L 138, 26.5.2016, p. 44).
(29) Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance 
of motor vehicles and their trailers, and of systems, componen ts and separate technical units intended for such vehicles, amending 
Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC (OJ L 151, 14.6.2018, p. 1).European Parliament and of the Council (30), and Regulation (EU) 2019/2144 of the European Parliament and of the 
Council (31), it is appropr iate to amend those acts to ensure that the Commission takes into account, on the basis of 
the technical and regulato ry specif icities of each secto r, and without interfer ing with existing gover nance, conf ormity 
assessment and enforcement mechanisms and author ities established therein, the mandato ry requirements for 
high-r isk AI systems laid down in this Regulation when adopti ng any relevant deleg ated or imp lementing acts on the 
basis of those acts.
(50) As regards AI syste ms that are safety comp onents of products, or which are themselves products, falling within the 
scope of certain Union harmonisation legislation listed in an annex to this Regulation, it is appropr iate to classify 
them as high-r isk under this Regulation if the product concer ned undergoes the conf ormity assessment procedure 
with a third-par ty conf ormity assessment body pursuant to that relevant Union harmonisation legislation. In 
particular , such products are machi nery, toys, lifts, equipment and prot ective systems intended for use in pote ntially 
explosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cablewa y installations, 
appliances burning gaseous fuels, medical devices, in vitro diagnostic medical devices, automot ive and aviation.
(51) The classif ication of an AI syste m as high-r isk pursuant to this Regulation should not necessar ily mean that the 
product whose safety component is the AI system, or the AI system itself as a product, is considered to be high-r isk 
under the criteria established in the relevant Union harmonisation legislation that applies to the product. This is, in 
particular , the case for Regulations (EU) 2017/745 and (EU) 2017/746, where a third-par ty conf ormity assessment is 
provided for medium-r isk and high-r isk products.
(52) As rega rds stand-alone AI syste ms, namely high-r isk AI syste ms other than those that are safety com ponents of 
products, or that are themselves products, it is appropr iate to classify them as high-r isk if, in light of their intended 
purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into 
account both the sever ity of the possible harm and its probability of occur rence and they are used in a number of 
specifically pre-defi ned areas specified in this Regulation. The identification of those systems is based on the same 
methodology and criteria envisag ed also for any future amendments of the list of high-r isk AI syste ms that the 
Commission should be empo wered to adop t, via delegat ed acts, to take into account the rapid pace of technological 
development, as well as the potential change s in the use of AI systems.
(53) It is also impor tant to clarify that there may be specific cases in which AI syste ms refer red to in pre-define d areas 
specified in this Regulation do not lead to a signifi cant risk of harm to the lega l intere sts protect ed under those areas 
because they do not materi ally influence the decision-making or do not harm those interests substantially . For the 
purposes of this Regulation, an AI system that does not mat erially influence the outcome of decision-making should 
be understood to be an AI syste m that does not have an impact on the substance, and thereby the outcome, of 
decision-making, whether human or automa ted. An AI system that does not materially influence the outcome of 
decision-making could include situations in which one or more of the follo wing conditions are fulfilled. The first 
such condition should be that the AI syste m is intende d to perfo rm a narrow procedural task, such as an AI system 
that transf orms unstr uctured data into structured data, an AI system that classif ies incoming documents into 
cate gories or an AI syste m that is used to detect duplicate s among a large number of applications. Those tasks are of 
such narrow and limited nature that they pose only limited risks which are not increased through the use of an AI 
syste m in a cont ext that is listed as a high-r isk use in an annex to this Regulation. The second condition should be EN OJ L, 12.7.2024
14/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(30) Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civil 
aviation and establishing a European Union Aviation Safety Agency , and amending Regulations (EC) No 2111/2005, (EC) 
No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament 
and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the 
Council and Council Regulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).
(31) Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirements 
for motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regards 
their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the 
European Parliament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of 
the European Parliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) 
No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) 
No 109/2011, (EU) No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) 
No 1230/2012 and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1).that the task perfo rmed by the AI system is intended to imp rove the result of a previously comp leted human activity 
that may be relevant for the purposes of the high-r isk uses listed in an annex to this Regulation. Consider ing those 
characteristics , the AI system provid es only an additional layer to a human activity with consequently lower ed risk. 
That condition would, for exam ple, apply to AI syste ms that are intended to impro ve the language used in previously 
draf ted documents, for exam ple in relation to profe ssional tone, academic style of languag e or by aligning text to 
a certain brand messaging. The third condition should be that the AI syste m is intended to detect decision-making 
patt erns or deviations from prior decision-making patterns . The risk would be lower ed because the use of the AI 
syste m follo ws a previously complet ed human assessment which it is not meant to replace or influence, without 
proper human review . Such AI syste ms include for instance those that, given a certain grading patt ern of a teac her, 
can be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potent ial 
inconsiste ncies or anomalies. The fourth condition should be that the AI syste m is intended to perf orm a task that is 
only preparat ory to an assessment relevant for the purposes of the AI systems listed in an annex to this Regulation, 
thus making the possible imp act of the output of the system very low in terms of representing a risk for the 
assessment to follow . That condition covers, inter alia, smar t solutions for file handling, which include various 
functions from indexing, search ing, text and speech processing or linking data to other data sources, or AI syste ms 
used for translation of initial documents. In any case, AI systems used in high-r isk use-cases listed in an annex to this 
Regulation should be considered to pose signifi cant risks of harm to the health, safety or fundamental rights if the AI 
syste m imp lies profiling within the meaning of Article 4, point (4) of Regulation (EU) 2016/679 or Article 3, 
point (4) of Directive (EU) 2016/680 or Article 3, point (5) of Regulation (EU) 2018/1725. To ensure traceability 
and transparency , a provid er who considers that an AI syste m is not high-r isk on the basis of the conditions referred 
to above should draw up documentation of the assessment before that syste m is placed on the marke t or put into 
service and should provide that documentation to national compet ent author ities upon request. Such a provid er 
should be obliged to register the AI syste m in the EU database established under this Regulation. With a view to 
providing further guidance for the practical imp lementation of the conditions under which the AI syste ms listed in 
an annex to this Regulation are, on an excep tional basis, non-high-r isk, the Commission should, after consulting the 
Board, provid e guidelines specifying that practical implementation, comp leted by a compre hensive list of practical 
exam ples of use cases of AI syste ms that are high-r isk and use cases that are not.
(54) As biometr ic data constitute s a special categor y of personal data, it is appropr iate to classify as high-r isk several 
critical-use cases of biometr ic syste ms, insofar as their use is permitte d under relevant Union and national law. 
Technical inaccuracies of AI systems intende d for the remote biometr ic identifica tion of natural persons can lead to 
biased results and entail discr iminat ory effects. The risk of such biased results and discr iminat ory effects is 
particularly relevant with regard to age, ethnicity , race, sex or disabilities. Remote biometr ic identifica tion syste ms 
should theref ore be classif ied as high-r isk in view of the risks that they pose. Such a classif ication excludes AI 
syste ms intended to be used for biometr ic verification, including authentication, the sole purpose of which is to 
confir m that a specif ic natural person is who that person claims to be and to confir m the identity of a natural person 
for the sole purpose of having access to a service, unlocking a device or having secure access to premises. In addition, 
AI syste ms intended to be used for biometr ic categor isation according to sensitive attribut es or character istics 
prot ected under Article 9(1) of Regulation (EU) 2016/679 on the basis of biometr ic data, in so far as these are not 
prohibite d under this Regulation, and emotion recognition systems that are not prohibite d under this Regulation, 
should be classif ied as high-r isk. Biometr ic syste ms which are intende d to be used solely for the purpose of enabling 
cybersecur ity and personal data prot ection measures should not be considered to be high-r isk AI syste ms.
(55) As regards the management and operation of critical infrastr ucture, it is appropr iate to classify as high-r isk the AI 
syste ms intended to be used as safety components in the managem ent and operation of critical digital infrastr ucture 
as listed in point (8) of the Annex to Directive (EU) 2022/2557, road traffic and the supply of wate r, gas, heating and 
electr icity , since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead to 
appreciable disruptions in the ordinar y conduct of social and economic activities. Safety compo nents of critical 
infrastr ucture, including critical digital infrastr ucture, are systems used to directly protect the physica l integrity of 
critical infrastr ucture or the health and safety of persons and proper ty but which are not necessar y in order for the OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 15/144syste m to function. The failure or malfunctioning of such comp onents might directly lead to risks to the phys ical 
integrity of critical infrastr ucture and thus to risks to health and safety of persons and proper ty. Compo nents 
intended to be used solely for cybersecur ity purposes should not qualify as safety compo nents. Examples of safety 
com ponents of such critical infrastr ucture may include syste ms for monitoring wate r pressure or fire alarm 
controlling systems in cloud comp uting centres.
(56) The deplo yment of AI systems in education is impor tant to promot e high-quality digital education and training and 
to allow all learners and teache rs to acquire and share the necessar y digital skills and comp etences, including media 
literacy , and critical thinking, to take an active part in the economy , society , and in democratic processes. How ever , 
AI systems used in education or vocational training, in particular for determining access or admission, for assigning 
persons to educational and vocational training institutions or programmes at all levels, for evaluating learning 
outcomes of persons, for assessing the appropr iate level of education for an individual and mat erially influencing the 
level of education and training that individuals will receive or will be able to access or for monitoring and detecting 
prohibite d behaviour of students during tests should be classif ied as high-r isk AI systems, since they may determi ne 
the educational and profe ssional course of a person’s life and theref ore may affect that person’s ability to secure 
a livelihood. When improperly designed and used, such syste ms may be particularly intrusive and may violate the 
right to education and training as well as the right not to be discr iminated against and perpetuate histor ical patt erns 
of discr imination, for exam ple against women, certain age groups, persons with disabilities, or persons of certain 
racial or ethnic origins or sexual orientation.
(57) AI systems used in emplo yment, workers management and access to self-emplo yment, in particular for the 
recr uitment and selection of persons, for making decisions affecting terms of the work -related relationship, 
promotion and term ination of work-relate d contractual relationships, for allocating tasks on the basis of individual 
behavi our, personal traits or character istics and for monitoring or evaluation of persons in work -relate d contractual 
relationships, should also be classif ied as high-r isk, since those systems may have an appreciable impact on future 
career prospects, livelihoods of those persons and work ers’ rights. Relevant work-related contractual relationships 
should, in a meaningful manner , involve emp loyees and persons provid ing services through platf orms as referred to 
in the Commission Work Programme 2021. Throughout the recr uitment process and in the evaluation, promotion, 
or retention of persons in work-related contractual relationships, such syste ms may perpetuat e historical patterns of 
discr imination, for exam ple against women, certain age groups, persons with disabilities, or persons of certain racial 
or ethnic origins or sexual orientation. AI syste ms used to monitor the perf ormance and behaviour of such persons 
may also under mine their fundamental rights to data protection and privacy .
(58) Another area in which the use of AI syste ms deser ves special consideration is the access to and enjo yment of certain 
essential private and public services and benef its necessar y for people to fully participate in society or to imp rove 
one’s standard of living. In particular , natural persons applying for or receiving essential public assistance benef its 
and services from public author ities namely healthcare services, social secur ity benef its, social services providing 
prot ection in cases such as maternity , illness, industr ial accidents, dependency or old age and loss of emp loyment 
and social and housing assistance, are typically dependent on those benef its and services and in a vulnerable position 
in relation to the responsible author ities. If AI syste ms are used for determining whether such benefits and services 
should be granted, denied, reduced, revok ed or reclaimed by author ities, including whether beneficiar ies are 
legitimat ely entitled to such benefi ts or services, those syste ms may have a significant impact on persons’ livelihood 
and may infringe their fundamental rights, such as the right to social protection, non-discr imination, human dignity 
or an effective remedy and should theref ore be classif ied as high-r isk. Nonetheless, this Regulation should not 
ham per the development and use of innovative approac hes in the public administration, which would stand to 
benefit from a wider use of compliant and safe AI systems, provid ed that those syste ms do not entail a high risk to 
legal and natural persons. In addition, AI syste ms used to evaluate the credit score or creditwor thiness of natural 
persons should be classif ied as high-r isk AI systems, since they determi ne those persons’ access to financ ial resources 
or essential services such as housing, electr icity , and telecommunication services. AI syste ms used for those purposes 
may lead to discr imination between persons or groups and may perpetuat e histor ical patt erns of discr imination, 
such as that based on racial or ethnic origins, gender , disabilities, age or sexual orientation, or may create new forms 
of discr iminatory impacts. However , AI systems provid ed for by Union law for the purpose of detecting fraud in the 
offering of financ ial services and for prudential purposes to calculate credit institutions’ and insurance under takings ’ 
capital requirements should not be considered to be high-r isk under this Regulation. Moreove r, AI systems intended EN OJ L, 12.7.2024
16/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojto be used for risk assessment and pricing in relation to natural persons for health and life insurance can also have 
a significant impact on persons’ livelihood and if not duly designed, developed and used, can infringe their 
fundamental rights and can lead to serious consequences for people ’s life and health, including financ ial exclusion 
and discr imination. Finally , AI systems used to evaluate and classify emerge ncy calls by natural persons or to 
dispatc h or establish priority in the dispatch ing of emerg ency first response services, including by police, firefighters 
and medical aid, as well as of emerge ncy healthcare patient triage syste ms, should also be classif ied as high-r isk since 
they make decisions in very critical situations for the life and health of persons and their proper ty.
(59) Given their role and responsibility , actions by law enforcement author ities involving certain uses of AI systems are 
characterised by a signifi cant degree of power imbalance and may lead to surveillance, arrest or depr ivation of 
a natural person’s liber ty as well as other adverse impacts on fundamental rights guaranteed in the Char ter. In 
particular , if the AI syste m is not trained with high-quality data, does not meet adequate requirements in terms of its 
perf ormance, its accuracy or robustness, or is not properly designed and tested before being put on the market or 
other wise put into service, it may sing le out people in a discr iminat ory or other wise incor rect or unjust manner . 
Further more, the exercise of imp ortant procedural fundamental rights, such as the right to an effective remedy and 
to a fair trial as well as the right of defence and the presump tion of innocence, could be hampered, in particular , 
where such AI syste ms are not suffi ciently transparent, explainable and documente d. It is theref ore appropr iate to 
classify as high-r isk, insofa r as their use is permitte d under relevant Union and national law, a number of AI syste ms 
intended to be used in the law enforcement cont ext where accuracy , reliability and transparency is particularly 
important to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of the 
nature of the activities and the risks relating thereto , those high-r isk AI systems should include in particular AI 
syste ms intended to be used by or on behalf of law enforcement author ities or by Union institutions, bodies, offices, 
or agencies in suppor t of law enforcement author ities for assessing the risk of a natural person to become a victim of 
criminal offenc es, as polygraphs and similar tools, for the evaluation of the reliability of evidence in in the course of 
investig ation or prosecution of criminal offences, and, insofar as not prohibited under this Regulation, for assessing 
the risk of a natural person offending or reoffe nding not solely on the basis of the prof iling of natural persons or the 
assessment of personality traits and character istics or the past criminal behavio ur of natural persons or groups, for 
profiling in the course of detection, investig ation or prosecution of criminal offences. AI syste ms specifically 
intended to be used for administrative proceedings by tax and custo ms author ities as well as by financial intellig ence 
units carrying out administrative tasks analysing information pursuant to Union anti-money launder ing law should 
not be classif ied as high-r isk AI syste ms used by law enforcement author ities for the purpose of prevention, 
detection, investig ation and prosecution of criminal offenc es. The use of AI tools by law enforcement and other 
relevant author ities should not become a factor of inequality , or exclus ion. The imp act of the use of AI tools on the 
defe nce rights of suspects should not be ignored, in particular the difficulty in obtaining meaningful information on 
the functioning of those systems and the resulting difficulty in challenging their results in cour t, in particular by 
natural persons under investig ation.
(60) AI systems used in migration, asylum and border control managem ent affect persons who are often in particularly 
vulnerable position and who are dependent on the outcome of the actions of the comp etent public author ities. The 
accuracy , non-discr iminat ory nature and transparency of the AI systems used in those contexts are theref ore 
particularly important to guarante e respect for the fundamental rights of the affected persons, in particular their 
rights to free move ment, non-discr imination, prot ection of private life and personal data, international prot ection 
and good administration. It is theref ore appropr iate to classify as high-r isk, insofa r as their use is permitted under 
relevant Union and national law, AI syste ms intended to be used by or on behalf of compet ent public author ities or 
by Union institutions, bodies, offices or agencies charged with tasks in the fields of migration, asylum and border 
control management as polygraphs and similar tools, for assessing certain risks posed by natural persons entering 
the territory of a Member State or applying for visa or asylum, for assisting compet ent public author ities for the 
examination, including related assessment of the reliability of evidence, of applications for asylum, visa and residence 
permits and associat ed comp laints with regard to the objective to establish the eligibility of the natural persons 
applying for a status, for the purpose of detecting, recognising or identifying natural persons in the context of 
migration, asylum and border control manag ement, with the excep tion of verification of travel documents. AI 
syste ms in the area of migration, asylum and border control management covered by this Regulation should comply 
with the relevant procedural requirements set by the Regulation (EC) No 810/2009 of the European Parliament and OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 17/144of the Council (32), the Directive 2013/32/EU of the European Parliament and of the Council (33), and other relevant 
Union law. The use of AI systems in migration, asylum and border control management should, in no circumstances, 
be used by Member States or Union institutions, bodies, offices or agencies as a means to circum vent their 
international obligations under the UN Conve ntion relating to the Status of Refuge es done at Geneva on 28 July 
1951 as amended by the Prot ocol of 31 Januar y 1967. Nor should they be used to in any way infringe on the 
principle of non-ref oulement, or to deny safe and effective legal avenues into the territory of the Union, including 
the right to inter national prot ection.
(61) Certain AI systems intended for the administration of justice and democratic processes should be classified as 
high-r isk, consider ing their potent ially signifi cant impact on democracy , the rule of law, individual freedoms as well 
as the right to an effective remedy and to a fair trial. In particular , to address the risks of potential biases, errors and 
opacity , it is appropr iate to qualify as high-r isk AI syste ms intended to be used by a judicial author ity or on its behalf 
to assist judicial author ities in researching and interpreting facts and the law and in applying the law to a concrete set 
of facts. AI syste ms intended to be used by alternative dispute resolution bodies for those purposes should also be 
considered to be high-r isk when the outcomes of the alter native dispute resolution proceedings produce legal effects 
for the parties. The use of AI tools can suppor t the decision-making power of judges or judicial independence, but 
should not replace it: the final decision-making must remain a human-dr iven activity . The classification of AI 
syste ms as high-r isk should not, however , extend to AI syste ms intended for purely ancillar y administrative activities 
that do not affect the actual administration of justice in individual cases, such as anonymisa tion or 
pseudon ymisation of judicial decisions, documents or data, communication between personnel, administrative tasks.
(62) Without prejudice to the rules provided for in Regulation (EU) 2024/900 of the European Parliament and of the 
Council (34), and in order to address the risks of undue exte rnal interfere nce with the right to vote enshr ined in 
Article 39 of the Char ter, and of adverse effects on democracy and the rule of law, AI syste ms intended to be used to 
influence the outcome of an election or referendum or the voting behaviour of natural persons in the exercise of 
their vote in elections or referenda should be classif ied as high-r isk AI systems with the exception of AI syste ms 
whose output natural persons are not directly exposed to, such as tools used to organise, optimise and structure 
political cam paigns from an administrative and logistical point of view .
(63) The fact that an AI syste m is classif ied as a high-r isk AI system under this Regulation should not be inter preted as 
indicating that the use of the system is lawful under other acts of Union law or under national law compatible with 
Union law, such as on the prot ection of personal data, on the use of polygraphs and similar tools or other systems to 
detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the 
applicable requirements resulting from the Char ter and from the applicable acts of secondar y Union law and 
national law. This Regulation should not be understood as provid ing for the lega l ground for processing of personal 
data, including special categor ies of personal data, where relevant, unless it is specifically other wise provid ed for in 
this Regulation.
(64) To mitigat e the risks from high-r isk AI syste ms placed on the marke t or put into service and to ensure a high level of 
trustwo rthiness, certain mandat ory requirements should apply to high-r isk AI systems, taking into account the 
intended purpose and the cont ext of use of the AI syste m and according to the risk-management system to be 
established by the provider . The measures adop ted by the providers to comply with the mandato ry requirements of 
this Regulation should take into account the generally ackno wledged state of the art on AI, be propor tionate and 
effective to meet the objectives of this Regulation. Based on the New Legislative Framework, as clarified in 
Commission notice ‘The “Blue Guide” on the implementation of EU product rules 2022’, the general rule is that 
more than one legal act of Union harmonisation legislation may be applicable to one product, since the making 
available or putting into service can take place only when the product complie s with all applicable Union 
harmonisation legislation. The hazards of AI systems covered by the requirements of this Regulation concer n 
different aspects than the existing Union harmonisation legislation and theref ore the requirements of this Regulation 
would complement the existing body of the Union harmonisation legislation. For exam ple, machi nery or medical 
devices products incor porating an AI system might present risks not addressed by the essential health and safety EN OJ L, 12.7.2024
18/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(32) Regulation (EC) No 810/2009 of the European Parliament and of the Council of 13 July 2009 establishing a Community Code on 
Visas (Visa Code) (OJ L 243, 15.9.2009, p. 1).
(33) Directive 2013/32/EU of the European Parliament and of the Council of 26 June 2013 on common procedures for granting and 
withdraw ing international prot ection (OJ L 180, 29.6.2013, p. 60).
(34) Regulation (EU) 2024/900 of the European parliament and of the Council of 13 March 2024 on the transparency and targe ting of 
political adver tising (OJ L, 2024/900, 20.3.2024, ELI: http://data.europa.eu/eli/reg/2024/900/oj).requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risks 
specific to AI systems. This calls for a simultaneous and comp lementar y application of the various legislative acts. To 
ensure consist ency and to avoid an unnecessar y administrative burden and unnecessar y costs, provid ers of a product 
that contains one or more high-r isk AI system, to which the requirements of this Regulation and of the Union 
harmonisation legislation based on the New Legislative Framewo rk and listed in an annex to this Regulation apply , 
should have flexibility with regard to operational decisions on how to ensure compliance of a product that contains 
one or more AI syste ms with all the applicable requirements of that Union harmonised legislation in an optimal 
manner . That flexibility could mean, for exam ple a decision by the provider to integrate a part of the necessar y 
testing and repor ting processes, information and documentation required under this Regulation into already existing 
documentation and procedures required under existing Union harmonisation legislation based on the New 
Legislative Framework and listed in an annex to this Regulation. This should not, in any way, under mine the 
oblig ation of the provid er to comply with all the applicable requirements.
(65) The risk-managem ent syste m should consist of a continuous, iterative process that is planned and run throughout 
the entire lifecycle of a high-r isk AI syste m. That process should be aimed at identifying and mitigating the relevant 
risks of AI syste ms on health, safety and fundamental rights. The risk-manag ement system should be regularly 
reviewed and update d to ensure its continuing effectiveness, as well as justifi cation and documentation of any 
significant decisions and actions take n subject to this Regulation. This process should ensure that the provid er 
identifies risks or adverse impacts and imp lements mitig ation measures for the kno wn and reasonably foreseeable 
risks of AI syste ms to the health, safety and fundamental rights in light of their intended purpose and reasonably 
foreseeable misuse, including the possible risks arising from the interaction between the AI system and the 
environment within which it operates. The risk-management syste m should adop t the most appropr iate 
risk-manag ement measures in light of the state of the art in AI. When identifying the most appropr iate 
risk-manag ement measures, the provid er should document and explain the choices made and, when relevant, 
involve exper ts and exter nal stak eholders. In identifying the reasonably foreseeable misuse of high-r isk AI systems, 
the provid er should cover uses of AI syste ms which, while not directly covered by the intende d purpose and 
provided for in the instr uction for use may never theless be reasonably expected to result from readily predictable 
human behavi our in the cont ext of the specif ic charact eristics and use of a particular AI syste m. Any kno wn or 
foreseeable circumstances related to the use of the high-r isk AI syste m in accordance with its intended purpose or 
under conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamental 
rights should be included in the instr uctions for use that are provid ed by the provider . This is to ensure that the 
deplo yer is aware and takes them into account when using the high-r isk AI syste m. Identifying and implementing 
risk mitigation measures for foreseeable misuse under this Regulation should not require specific additional training 
for the high-r isk AI syste m by the provid er to address foreseeable misuse. The provider s however are encourage d to 
consider such additional training measures to mitiga te reasonable foreseeable misuses as necessar y and appropr iate.
(66) Requirements should apply to high-r isk AI syste ms as regard s risk management, the quality and relevance of data 
sets used, technical documentation and record-k eeping, transparency and the provision of information to deplo yers, 
human oversight, and robustness, accuracy and cybersecur ity. Those requirements are necessar y to effectively 
mitig ate the risks for health, safety and fundamental rights. As no other less trade restr ictive measures are reasonably 
available those requirements are not unjustified restr ictions to trade.
(67) High-quality data and access to high-quality data plays a vital role in providing structure and in ensur ing the 
perf ormance of many AI systems, especially when techniques involving the training of models are used, with a view 
to ensure that the high-r isk AI syste m performs as intended and safely and it does not become a source of 
discr imination prohibite d by Union law. High-quality data sets for training, validation and testing require the 
implementation of appropr iate data gover nance and managem ent practices. Data sets for training, validation and 
testing, including the labels, should be relevant, sufficiently representative, and to the best extent possible free of 
errors and complet e in view of the intended purpose of the system. In order to facilitate comp liance with Union data 
prot ection law, such as Regulation (EU) 2016/679, data gove rnance and management practices should include, in 
the case of personal data, transparency about the original purpose of the data collection. The data sets should also 
have the appropr iate statistical proper ties, including as regard s the persons or groups of persons in relation to whom 
the high-r isk AI syste m is intended to be used, with specific attention to the mitigation of possible biases in the data 
sets, that are likely to affect the health and safety of persons, have a negative imp act on fundamental rights or lead to 
discr imination prohibite d under Union law, especially where data outputs influence inputs for future operations OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 19/144(feedbac k loops). Biases can for example be inherent in underlying data sets, especially when histor ical data is being 
used, or generated when the syste ms are implement ed in real world settings. Results provided by AI systems could be 
influenced by such inherent biases that are inclined to gradually increase and thereby perpetuate and amplify existing 
discr imination, in particular for persons belonging to certain vulnerable groups, including racial or ethnic groups. 
The requirement for the data sets to be to the best exte nt possible complet e and free of errors should not affect the 
use of privacy-preser ving techniques in the cont ext of the development and testing of AI systems. In particular , data 
sets should take into account, to the extent required by their intended purpose, the features, character istics or 
elements that are particular to the specif ic geographical, contextual, behavi oural or functional setting which the AI 
syste m is intended to be used. The requirements related to data gove rnance can be complied with by having recourse 
to third parties that offer certified compliance services including verification of data gover nance, data set integrity, 
and data training, validation and testing practices, as far as comp liance with the data requirements of this Regulation 
are ensured.
(68) For the development and assessment of high-r isk AI systems, certain actors, such as provid ers, notif ied bodies and 
other relevant entities, such as European Digital Innovation Hubs, testing exper imentation facilities and researchers , 
should be able to access and use high-quality data sets within the fields of activities of those actor s which are related 
to this Regulation. European common data spaces established by the Commission and the facilitation of data shar ing 
between businesses and with gover nment in the public intere st will be instr umental to provide trustful, accountable 
and non-discr iminat ory access to high-quality data for the training, validation and testing of AI syste ms. For 
exam ple, in health, the European health data space will facilitat e non-discr iminatory access to health data and the 
training of AI algor ithms on those data sets, in a privacy-preser ving, secure, timely , transparent and trustw orthy 
manner , and with an appropr iate institutional gove rnance. Relevant comp etent author ities, including sectoral ones, 
providing or suppor ting the access to data may also suppor t the provision of high-quality data for the training, 
validation and testing of AI syste ms.
(69) The right to privacy and to protection of personal data must be guarante ed throughout the entire lifecycle of the AI 
syste m. In this regard , the principles of data minimisation and data protection by design and by default, as set out in 
Union data protect ion law, are applicable when personal data are processed. Measures take n by provid ers to ensure 
com pliance with those principles may include not only anonymisati on and encr yption, but also the use of 
technology that permits algor ithms to be brought to the data and allows training of AI systems without the 
transmission between parties or copying of the raw or structured data themselves, without prejudice to the 
requirements on data gover nance provid ed for in this Regulation.
(70) In order to protect the right of others from the discr imination that might result from the bias in AI syste ms, the 
providers should, exceptiona lly, to the extent that it is strictly necessar y for the purpose of ensur ing bias detection 
and correction in relation to the high-r isk AI systems, subject to appropr iate safeguards for the fundamental rights 
and freedoms of natural persons and followi ng the application of all applicable conditions laid down under this 
Regulation in addition to the conditions laid down in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive 
(EU) 2016/680, be able to process also special cate gories of personal data, as a matt er of substantial public interest 
within the meaning of Article 9(2), point (g) of Regulation (EU) 2016/679 and Article 10(2), point (g) of Regulation 
(EU) 2018/1725.
(71) Having comp rehensible information on how high-r isk AI syste ms have been developed and how they perf orm 
throughout their lifetime is essential to enable traceability of those syste ms, verify compliance with the requirements 
under this Regulation, as well as monitoring of their operations and post mark et monitoring. This requires keeping 
records and the availability of technical documentation, containing information which is necessar y to assess the 
com pliance of the AI syste m with the relevant requirements and facilitate post market monitorin g. Such information 
should include the general charact eristics, capabilities and limitations of the system, algor ithms, data, training, 
testing and validation processes used as well as documentation on the relevant risk-manag ement system and drawn 
in a clear and comp rehensive form. The technical documentation should be kept up to date, appropr iately 
throughout the lifetime of the AI syste m. Further more, high-r isk AI systems should technically allow for the 
auto matic recording of events, by means of logs, over the duration of the lifetime of the syste m.EN OJ L, 12.7.2024
20/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(72) To address concer ns relate d to opacity and complexity of certain AI systems and help deplo yers to fulfil their 
oblig ations under this Regulation, transparency should be required for high-r isk AI syste ms before they are placed 
on the market or put it into service. High-r isk AI systems should be designed in a manner to enable deplo yers to 
understand how the AI syste m works, evaluate its functionality , and comp rehend its strengths and limitations. 
High-r isk AI systems should be accom panied by appropr iate information in the form of instr uctions of use. Such 
information should include the charact eristics, capabilities and limitations of perfo rmance of the AI system. Those 
would cover information on possible kno wn and foreseeable circumstances related to the use of the high-r isk AI 
syste m, including deplo yer action that may influence syste m behavio ur and perfor mance, under which the AI system 
can lead to risks to health, safety, and fundamental rights, on the changes that have been pre-deter mined and 
assessed for conf ormity by the provider and on the relevant human oversight measures, including the measures to 
facilitate the inter pretation of the outputs of the AI syste m by the deplo yers. Transparency , including the 
accom panying instr uctions for use, should assist deplo yers in the use of the syste m and suppor t informed decision 
making by them. Deplo yers should, inter alia, be in a better position to mak e the correct choice of the syste m that 
they intend to use in light of the obligations applicable to them, be educat ed about the intended and precluded uses, 
and use the AI system correctly and as appropr iate. In order to enhance legibility and accessibility of the information 
included in the instr uctions of use, where appropr iate, illustrative exam ples, for instance on the limitations and on 
the intende d and precluded uses of the AI syste m, should be included. Provi ders should ensure that all 
documentation, including the instr uctions for use, contains meaningful, compre hensive, accessible and 
understandable information, taking into account the needs and foreseeable kno wledge of the targe t deplo yers. 
Instr uctions for use should be made available in a language which can be easily understood by targ et deplo yers, as 
determined by the Member State concer ned.
(73) High-r isk AI systems should be designed and developed in such a way that natural persons can oversee their 
functioning, ensure that they are used as intended and that their imp acts are addressed over the system’s lifecy cle. To 
that end, appropr iate human oversight measures should be identifie d by the provider of the syste m before its placing 
on the market or putting into service. In particular , where appropr iate, such measures should guarantee that the 
syste m is subject to in-built operational constraints that cannot be overr idden by the system itself and is responsive 
to the human operat or, and that the natural persons to whom human oversight has been assigned have the necessar y 
com petence, training and author ity to carry out that role. It is also essential, as appropr iate, to ensure that high-r isk 
AI syste ms include mechanisms to guide and inform a natural person to whom human oversight has been assigned 
to mak e informed decisions if, when and how to intervene in order to avoid nega tive consequences or risks, or stop 
the syste m if it does not perform as intended. Consider ing the significant consequences for persons in the case of an 
incor rect matc h by certain biometr ic identification systems, it is appropr iate to provide for an enhanced human 
oversight requirement for those systems so that no action or decision may be take n by the deplo yer on the basis of 
the identifica tion resulting from the system unless this has been separate ly verified and conf irmed by at least two 
natural persons. Those persons could be from one or more entities and include the person operating or using the 
syste m. This requirement should not pose unnecessar y burden or dela ys and it could be suffi cient that the separate 
verificati ons by the diffe rent persons are auto matically recorded in the logs generated by the syste m. Given the 
specificities of the areas of law enforcement, migration, border control and asylum, this requirement should not 
apply where Union or national law considers the application of that requirement to be dispropor tionate.
(74) High-r isk AI syste ms should perf orm consiste ntly throughout their lifecycle and meet an appropr iate level of 
accuracy , robustness and cybersecur ity, in light of their intended purpose and in accordance with the generally 
acknowledg ed state of the art. The Commission and relevant organisations and stak eholders are encourage d to take 
due consideration of the mitiga tion of risks and the nega tive impacts of the AI system. The expecte d level of 
perf ormance metr ics should be declared in the accom panying instr uctions of use. Providers are urged to 
communicate that information to deplo yers in a clear and easily understandable way, free of misunderstandings or 
misleading statements. Union law on lega l metrology , including Directives 2014/31/EU (35) and 2014/32/EU (36) of 
the European Parliament and of the Council, aims to ensure the accuracy of measurements and to help the 
transparency and fairne ss of commercial transactions. In that context, in cooperation with relevant stakeh olders and 
organisation, such as metrology and bench marking author ities, the Commission should encourage , as appropr iate, 
the development of bench mark s and measurement methodologies for AI systems. In doing so, the Commission 
should take note and collaborate with inter national partners working on metrology and relevant measurement 
indicato rs relating to AI.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 21/144(35) Directive 2014/31/EU of the European Parliament and of the Council of 26 Febr uary 2014 on the harmonisation of the laws of the 
Member State s relating to the making available on the market of non-aut omatic weighing instr uments (OJ L 96, 29.3.2014, p. 107).
(36) Directive 2014/32/EU of the European Parliament and of the Council of 26 Febr uary 2014 on the harmonisation of the laws of the 
Member States relating to the making available on the market of measur ing instr uments (OJ L 96, 29.3.2014, p. 149).(75) Technical robustness is a key requirement for high-r isk AI systems. They should be resilient in relation to harmful or 
other wise undesirable behavi our that may result from limitations within the syste ms or the envir onment in which 
the systems operate (e.g. errors, faults, inconsiste ncies, unexpected situations). Theref ore, technical and 
organisational measures should be take n to ensure robustness of high-r isk AI systems, for exam ple by designing 
and developing appropr iate technical solutions to prevent or minimise harmful or other wise undesirable behavi our. 
Those technical solution may include for instance mec hanisms enabling the system to safely inter rupt its operation 
(fail-saf e plans) in the presence of certain anomalies or when operation take s place outside certain predeter mined 
boundar ies. Failure to protect against these risks could lead to safety impacts or nega tively affect the fundamental 
rights, for exam ple due to erroneous decisions or wrong or biased outputs generated by the AI syste m.
(76) Cybersecur ity plays a crucial role in ensur ing that AI syste ms are resilient against attem pts to alter their use, 
behavi our, perf ormance or comp romise their secur ity proper ties by malicious third parties exploiting the syste m’s 
vulnerabilities. Cyberattacks against AI syste ms can leverage AI specific assets, such as training data sets (e.g. data 
poisoning) or trained models (e.g. adversar ial attacks or membership inference), or exploit vulnerabilities in the AI 
syste m’s digital assets or the underlying ICT infrastr ucture. To ensure a level of cybersecur ity appropr iate to the risks, 
suitable measures, such as secur ity controls, should theref ore be take n by the provid ers of high-r isk AI systems, also 
taking into account as appropr iate the underlying ICT infrastr ucture.
(77) Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, high-r isk AI 
syste ms which fall within the scope of a regulation of the European Parliament and of the Council on horizontal 
cybersecur ity requirements for products with digital elements, in accordance with that regulation may demonstrate 
com pliance with the cybersecur ity requirements of this Regulation by fulfilling the essential cybersecur ity 
requirements set out in that regulation. When high-r isk AI systems fulfil the essential requirements of a regulation of 
the European Parliament and of the Council on horizontal cybersecur ity requirements for products with digital 
elements, they should be deemed compliant with the cybersecur ity requirements set out in this Regulation in so far 
as the achievement of those requirements is demonstrated in the EU declaration of conf ormity or parts thereof 
issued under that regulation. To that end, the assessment of the cybersecur ity risks, associated to a product with 
digital elements classif ied as high-r isk AI syste m according to this Regulation, carried out under a regulation of the 
European Parliament and of the Council on horizontal cybersecur ity requirements for products with digital 
elements, should consider risks to the cyber resilience of an AI system as regard s attem pts by unauthor ised third 
parties to alter its use, behavio ur or perf ormance, including AI specif ic vulnerabilities such as data poisoning or 
adversar ial attacks, as well as, as relevant, risks to fundamental rights as required by this Regulation.
(78) The conf ormity assessment procedure provid ed by this Regulation should apply in relation to the essential 
cybersecur ity requirements of a product with digital elements covered by a regulation of the European Parliament 
and of the Council on horizontal cybersecur ity requirements for products with digital elements and classified as 
a high-r isk AI system under this Regulation. However , this rule should not result in reducing the necessar y level of 
assurance for critical products with digital elements covered by a regulation of the European Parliament and of the 
Council on horizontal cybersecur ity requirements for products with digital elements. Theref ore, by way of 
deroga tion from this rule, high-r isk AI syste ms that fall within the scope of this Regulation and are also qualified as 
important and critical products with digital elements pursuant to a regulation of the European Parliament and of the 
Council on horizontal cybersecur ity requirements for products with digital elements and to which the conf ormity 
assessment procedure based on inter nal control set out in an annex to this Regulation applies, are subject to the 
conf ormity assessment provisions of a regulation of the European Parliament and of the Council on horizontal 
cybersecur ity requirements for products with digital elements insofar as the essential cybersecur ity requirements of 
that regulation are concer ned. In this case, for all the other aspects covered by this Regulation the respective 
provisions on conf ormity assessment based on inter nal control set out in an annex to this Regulation should apply . 
Building on the kno wledge and exper tise of ENISA on the cybersecur ity policy and tasks assigned to ENISA under 
the Regulation (EU) 2019/881 of the European Parliament and of the Council (37), the Commission should cooperate 
with ENISA on issues related to cybersecur ity of AI syste ms.EN OJ L, 12.7.2024
22/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(37) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 Apr il 2019 on ENISA (the European Union Agency 
for Cybersecur ity) and on information and communications technology cybersecur ity certification and repealing Regulation 
(EU) No 526/2013 (Cybersecur ity Act) (OJ L 151, 7.6.2019, p. 15).(79) It is appropr iate that a specif ic natural or lega l person, defined as the provider , takes responsibility for the placing on 
the marke t or the putting into service of a high-r isk AI syste m, regardless of whether that natural or lega l person is 
the person who designed or developed the syste m.
(80) As signat ories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the 
Member States are legally oblige d to prot ect persons with disabilities from discr imination and promot e their equality , 
to ensure that persons with disabilities have access, on an equal basis with others, to information and 
communications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given the 
growing impor tance and use of AI systems, the application of universal design principles to all new technologi es and 
services should ensure full and equal access for ever yone potentially affect ed by or using AI technologies, including 
persons with disabilities, in a way that take s full account of their inherent dignity and diversity . It is theref ore 
essential that provid ers ensure full compliance with accessibility requirements, including Directive (EU) 2016/2102 
of the European Parliament and of the Council (38) and Directive (EU) 2019/882. Providers should ensure 
com pliance with these requirements by design. Theref ore, the necessar y measures should be integrat ed as much as 
possible into the design of the high-r isk AI syste m.
(81) The provider should establish a sound quality manag ement syste m, ensure the accom plishment of the required 
conf ormity assessment procedure, draw up the relevant documentation and establish a robust post-mark et 
monito ring syste m. Provi ders of high-r isk AI systems that are subject to oblig ations regarding quality management 
syste ms under relevant secto ral Union law should have the possibility to include the elements of the quality 
manag ement syste m provid ed for in this Regulation as part of the existing quality management system provid ed for 
in that other sectoral Union law. The complement arity between this Regulation and existing secto ral Union law 
should also be take n into account in future standardisation activities or guidance adop ted by the Commission. Public 
author ities which put into service high-r isk AI syste ms for their own use may adop t and implement the rules for the 
quality managem ent syste m as part of the quality managem ent syste m adop ted at a national or regional level, as 
appropr iate, taking into account the specificities of the secto r and the comp etences and organisation of the public 
author ity concer ned.
(82) To enable enforcement of this Regulation and create a level playing field for operat ors, and, taking into account the 
different forms of making available of digital products, it is impor tant to ensure that, under all circumstances, 
a person established in the Union can provide author ities with all the necessar y information on the comp liance of an 
AI syste m. Theref ore, prior to making their AI systems available in the Union, provid ers established in third 
countr ies should, by written mandat e, appoint an author ised representative established in the Union. This author ised 
representative plays a pivot al role in ensur ing the comp liance of the high-r isk AI systems placed on the marke t or 
put into service in the Union by those providers who are not established in the Union and in serving as their contact 
person established in the Union.
(83) In light of the nature and comp lexity of the value chain for AI syste ms and in line with the New Legislative 
Framewo rk, it is essential to ensure lega l certainty and facilitate the compliance with this Regulation. Theref ore, it is 
necessar y to clarify the role and the specific obliga tions of relevant operat ors along that value chain, such as 
importers and distr ibutors who may contr ibut e to the development of AI systems. In certain situations those 
operat ors could act in more than one role at the same time and should theref ore fulfil cumulatively all relevant 
oblig ations associate d with those roles. For exam ple, an operator could act as a distr ibutor and an imp orter at the 
same time.
(84) To ensure legal certainty , it is necessar y to clarify that, under certain specific conditions, any distr ibutor , imp orter, 
deplo yer or other third-par ty should be considered to be a provider of a high-r isk AI syste m and theref ore assume all 
the relevant obliga tions. This would be the case if that party puts its name or trademark on a high-r isk AI system 
already placed on the marke t or put into service, without prejudice to contractual arrang ements stipulating that the 
oblig ations are allocated other wise. This would also be the case if that party makes a substantial modifi cation to 
a high-r isk AI syste m that has already been placed on the market or has already been put into service in a way that it 
remains a high-r isk AI syste m in accordance with this Regulation, or if it modif ies the intended purpose of an AI 
syste m, including a general-purpo se AI system, which has not been classif ied as high-r isk and has already been 
placed on the mark et or put into service, in a way that the AI syste m becomes a high-r isk AI syste m in accordance 
with this Regulation. Those provis ions should apply without prejudice to more specific provisions established in 
certain Union harmonisation legislation based on the New Legislative Framewo rk, together with which this OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 23/144(38) Directive (EU) 2016/2102 of the European Parliament and of the Council of 26 October 2016 on the accessibility of the websit es 
and mobile applications of public sector bodies (OJ L 327, 2.12.2016, p. 1).Regulation should apply . For example, Article 16(2) of Regulation (EU) 2017/745, establishing that certain change s 
should not be considered to be modifi cations of a device that could affect its comp liance with the applicable 
requirements, should continue to apply to high-r isk AI syste ms that are medical devices within the meaning of that 
Regulation.
(85) General-pur pose AI syste ms may be used as high-r isk AI syste ms by themselves or be comp onents of other high-r isk 
AI syste ms. Theref ore, due to their particular nature and in order to ensure a fair shar ing of responsibilities along the 
AI value chain, the provider s of such syste ms should, irrespective of whether they may be used as high-r isk AI 
syste ms as such by other providers or as compo nents of high-r isk AI syste ms and unless provid ed other wise under 
this Regulation, closely cooperate with the providers of the relevant high-r isk AI systems to enable their compliance 
with the relevant obligations under this Regulation and with the compet ent author ities established under this 
Regulation.
(86) Where, under the conditions laid down in this Regulation, the provid er that initially placed the AI syste m on the 
mark et or put it into service should no longe r be considered to be the provid er for the purposes of this Regulation, 
and when that provid er has not expressly excluded the chang e of the AI syste m into a high-r isk AI syste m, the 
former provid er should nonetheless closely cooperate and make available the necessar y information and provid e the 
reasonably expecte d technical access and other assistance that are required for the fulfilment of the obligations set 
out in this Regulation, in particular regarding the comp liance with the conf ormity assessment of high-r isk AI 
syste ms.
(87) In addition, where a high-r isk AI syste m that is a safety comp onent of a product which falls within the scope of 
Union harmonisation legislation based on the New Legislative Framework is not placed on the market or put into 
service independently from the product, the product manufa cturer defined in that legislation should comp ly with 
the obliga tions of the provider established in this Regulation and should, in particular , ensure that the AI system 
embedded in the final product comp lies with the requirements of this Regulation.
(88) Along the AI value chain multiple parties often supply AI syste ms, tools and services but also compo nents or 
processes that are incor porate d by the provid er into the AI syste m with various objectives, including the model 
training, model retraining, model testing and evaluation, integration into software, or other aspects of model 
development. Those parties have an imp ortant role to play in the value chain towards the provid er of the high-r isk 
AI syste m into which their AI systems, tools, services, comp onents or processes are integrat ed, and should provide 
by written agreement this provid er with the necessar y information, capabilities, technical access and other assistance 
based on the generally ackno wledged state of the art, in order to enable the provid er to fully comply with the 
oblig ations set out in this Regulation, without com promising their own intellect ual proper ty rights or trade secrets.
(89) Third parties making accessible to the public tools, services, processes, or AI comp onents other than 
general-pur pose AI models, should not be mandat ed to comp ly with requirements targ eting the responsibilities 
along the AI value chain, in particular towards the provid er that has used or integrat ed them, when those tools, 
services, processes, or AI comp onents are made accessible under a free and open-source licence. Developers of free 
and open-source tools, services, processes, or AI comp onents other than general-pur pose AI models should be 
encourag ed to implement widely adop ted documentation practices, such as model cards and data sheets, as a way to 
accelerat e information shar ing along the AI value chain, allowi ng the promotion of trustwor thy AI systems in the 
Union.
(90) The Commission could develop and recommend voluntary model contractual term s between providers of high-r isk 
AI syste ms and third parties that supply tools, services, comp onents or processes that are used or integrat ed in 
high-r isk AI systems, to facilitate the cooperation along the value chain. When developing voluntar y model 
contractual terms, the Commission should also take into account possible contractual requirements applicable in 
specific sectors or business cases.
(91) Given the nature of AI systems and the risks to safety and fundamental rights possibly associate d with their use, 
including as rega rds the need to ensure proper monitori ng of the perfor mance of an AI syste m in a real-life setting, it 
is appropr iate to set specific responsibilities for deplo yers. Deplo yers should in particular take appropr iate technical 
and organisational measures to ensure they use high-r isk AI syste ms in accordance with the instr uctions of use and 
certain other obliga tions should be provided for with regard to monitoring of the functioning of the AI systems and 
with regard to record-keeping, as appropr iate. Further more, deplo yers should ensure that the persons assigned to 
implement the instr uctions for use and human oversight as set out in this Regulation have the necessar y EN OJ L, 12.7.2024
24/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojcom petence, in particular an adequate level of AI literacy , training and author ity to properly fulfil those tasks. Those 
oblig ations should be without prejudice to other deplo yer obliga tions in relation to high-r isk AI syste ms under 
Union or national law.
(92) This Regulation is without prejudice to obligations for emplo yers to inform or to inform and consult workers or 
their representatives under Union or national law and practice, including Directive 2002/14/EC of the European 
Parliament and of the Council (39), on decisions to put into service or use AI syste ms. It remains necessar y to ensure 
information of workers and their representatives on the planned deplo yment of high-r isk AI syste ms at the 
workplace where the conditions for those information or information and consultation obliga tions in other legal 
instr uments are not fulfilled. Moreo ver, such information right is ancillar y and necessar y to the objective of 
prot ecting fundamental rights that underlies this Regulation. Theref ore, an information requirement to that effect 
should be laid down in this Regulation, without affecting any existing rights of work ers.
(93) Whilst risks relate d to AI systems can result from the way such systems are designed, risks can as well stem from 
how such AI systems are used. Deplo yers of high-r isk AI syste m theref ore play a critical role in ensur ing that 
fundamental rights are protect ed, compl ementing the obligati ons of the provider when developing the AI syste m. 
Deplo yers are best placed to understand how the high-r isk AI syste m will be used concretely and can theref ore 
identify potent ial signif icant risks that were not foreseen in the development phase, due to a more precise kno wledge 
of the context of use, the persons or groups of persons likely to be affect ed, including vulnerable groups. Deplo yers 
of high-r isk AI syste ms listed in an annex to this Regulation also play a critical role in informing natural persons and 
should, when they mak e decisions or assist in making decisions related to natural persons, where applicable, inform 
the natural persons that they are subject to the use of the high-r isk AI system. This information should include the 
intended purpose and the type of decisions it makes. The deplo yer should also inform the natural persons about 
their right to an explanation provided under this Regulation. With regard to high-r isk AI syste ms used for law 
enforcement purposes, that obligation should be imp lemented in accordance with Article 13 of Directive (EU) 
2016/680.
(94) Any processing of biometr ic data involved in the use of AI systems for biometr ic identifica tion for the purpose of 
law enforcement needs to comp ly with Article 10 of Directive (EU) 2016/680, that allows such processing only 
where strictly necessar y, subject to appropr iate safeguards for the rights and freedoms of the data subject, and where 
author ised by Union or Member State law. Such use, when author ised, also needs to respect the principles laid down 
in Article 4 (1) of Directive (EU) 2016/680 including lawfulness, fairness and transparency , purpose limitation, 
accuracy and storag e limitation.
(95) Without prejudice to applicable Union law, in particular Regulation (EU) 2016/679 and Directive (EU) 2016/680, 
consider ing the intrusive nature of post-remote biometr ic identifica tion systems, the use of post-remote biometr ic 
identification syste ms should be subject to safeguards. Post-remot e biometr ic identification systems should alwa ys be 
used in a way that is propor tionate , legitimate and strictly necessar y, and thus targe ted, in terms of the individuals to 
be identified, the location, temporal scope and based on a closed data set of legally acquired video footage. In any 
case, post-remote biometr ic identifica tion syste ms should not be used in the framework of law enforcement to lead 
to indiscr iminate surveillance. The conditions for post-remote biometr ic identifica tion should in any case not 
provide a basis to circumvent the conditions of the prohibition and strict excep tions for real time remote biometr ic 
identification.
(96) In order to efficiently ensure that fundamental rights are protect ed, deplo yers of high-r isk AI syste ms that are bodies 
gove rned by public law, or private entities providing public services and deplo yers of certain high-r isk AI syste ms 
listed in an annex to this Regulation, such as banking or insurance entities, should carry out a fundamental rights 
impact assessment prior to putting it into use. Services imp ortant for individuals that are of public nature may also 
be provided by private entities. Private entities providing such public services are linked to tasks in the public interest 
such as in the areas of education, healthcare, social services, housing, administration of justice. The aim of the 
fundamental rights impact assessment is for the deplo yer to identify the specif ic risks to the rights of individuals or 
groups of individuals likely to be affected , identify measures to be take n in the case of a mat erialisation of those risks. 
The impact assessment should be perf ormed prior to deplo ying the high-r isk AI syste m, and should be updated OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 25/144(39) Directive 2002/14/EC of the European Parliament and of the Council of 11 March 2002 establishing a general framew ork for 
informing and consulting employees in the European Community (OJ L 80, 23.3.2002, p. 29).when the deplo yer considers that any of the relevant factors have changed. The impact assessment should identify 
the deplo yer’s relevant processes in which the high-r isk AI system will be used in line with its intended purpose, and 
should include a descr iption of the period of time and frequency in which the syste m is intended to be used as well 
as of specific categor ies of natural persons and groups who are likely to be affect ed in the specif ic context of use. The 
assessment should also include the identifi cation of specific risks of harm likel y to have an impact on the 
fundamental rights of those persons or groups. While performing this assessment, the deplo yer should take into 
account information relevant to a proper assessment of the imp act, including but not limit ed to the information 
given by the provid er of the high-r isk AI system in the instr uctions for use. In light of the risks identified, deplo yers 
should determine measures to be taken in the case of a materialis ation of those risks, including for exam ple 
gove rnance arrangements in that specific context of use, such as arrang ements for human oversight according to the 
instr uctions of use or, complai nt handling and redress procedures, as they could be instr umental in mitigating risks 
to fundamental rights in concrete use-cases. After perfor ming that impact assessment, the deplo yer should notify the 
relevant marke t surveillance author ity. Where appropr iate, to collect relevant information necessar y to perfo rm the 
impact assessment, deplo yers of high-r isk AI syste m, in particular when AI systems are used in the public sector, 
could involve relevant stakeh olders, including the representatives of groups of persons likely to be affect ed by the AI 
syste m, independent exper ts, and civil society organisations in conducting such impact assessments and designing 
measures to be take n in the case of materi alisation of the risks. The European Artificial Intellig ence Office (AI Offi ce) 
should develop a templat e for a questionnaire in order to facilitate comp liance and reduce the administrative burden 
for deplo yers.
(97) The notion of general-pur pose AI models should be clearly defined and set apar t from the notion of AI systems to 
enable legal certainty . The definition should be based on the key functional character istics of a general-pur pose AI 
model, in particular the generality and the capability to com petently perf orm a wide rang e of distinct tasks. These 
models are typically trained on large amounts of data, through various methods, such as self-super vised, 
unsuper vised or reinf orcement learning. General-pur pose AI models may be placed on the market in various ways, 
including through librar ies, application programming interfaces (APIs), as direct download, or as physica l copy. 
These models may be further modif ied or fine-tuned into new models. Although AI models are essential 
com ponents of AI systems, they do not constitute AI systems on their own. AI models require the addition of further 
com ponents, such as for exam ple a user interface, to become AI syste ms. AI models are typically integrat ed into and 
form part of AI systems. This Regulation provid es specific rules for general-pur pose AI models and for 
general-pur pose AI models that pose syste mic risks, which should apply also when these models are integrat ed or 
form part of an AI syste m. It should be understood that the obliga tions for the providers of gene ral-pur pose AI 
models should apply once the general-pur pose AI models are placed on the marke t. When the provid er of 
a general-pur pose AI model integrates an own model into its own AI syste m that is made available on the market or 
put into service, that model should be considered to be placed on the marke t and, theref ore, the obliga tions in this 
Regulation for models should continue to apply in addition to those for AI syste ms. The obligations laid down for 
models should in any case not apply when an own model is used for purely inter nal processes that are not essential 
for providing a product or a service to third parties and the rights of natural persons are not affected . Consider ing 
their potent ial signifi cantly nega tive effects, the general-pur pose AI models with syste mic risk should alwa ys be 
subject to the relevant obliga tions under this Regulation. The definition should not cover AI models used before their 
placing on the mark et for the sole purpose of researc h, development and prot otyping activities. This is without 
prejudice to the obligation to com ply with this Regulation when, follo wing such activities, a model is placed on the 
mark et.
(98) Whereas the generality of a model could, inter alia, also be determined by a number of paramet ers, models with at 
least a billion of paramet ers and trained with a large amount of data using self-super vision at scale should be 
considered to displa y significant generality and to compet ently perf orm a wide rang e of distinctive tasks .
(99) Larg e generative AI models are a typical exam ple for a general-pur pose AI model, given that they allow for flexible 
generation of cont ent, such as in the form of text, audio, imag es or video, that can readily accommodate a wide 
rang e of distinctive tasks.
(100) When a general-pur pose AI model is integrated into or forms part of an AI syste m, this syste m should be considered 
to be general-pur pose AI syste m when, due to this integration, this system has the capability to serve a variety of 
purposes. A general-purpo se AI system can be used directly , or it may be integrat ed into other AI syste ms.EN OJ L, 12.7.2024
26/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(101) Provi ders of general-purpo se AI models have a particular role and responsibility along the AI value chain, as the 
models they provide may form the basis for a rang e of downstream syste ms, often provided by downstream 
providers that necessitate a good understanding of the models and their capabilities, both to enable the integration of 
such models into their products, and to fulfil their obligations under this or other regulations. Theref ore, 
propor tionate transparency measures should be laid down, including the drawi ng up and keeping up to date of 
documentation, and the provision of information on the general-pur pose AI model for its usage by the downstream 
providers. Technical documentation should be prepared and kept up to date by the general-pur pose AI model 
provider for the purpose of making it available, upon request, to the AI Offi ce and the national compet ent 
author ities. The minimal set of elements to be included in such documentation should be set out in specific annexe s 
to this Regulation. The Commission should be empo wered to amend those annexe s by means of delegat ed acts in 
light of evolving technological developments.
(102) Software and data, including models, released under a free and open-source licence that allows them to be openly 
shared and where users can freely access, use, modify and redistr ibute them or modif ied versions thereof, can 
contr ibut e to research and inno vation in the mark et and can provide significant growth oppor tunities for the Union 
economy . General-pur pose AI models released under free and open-source licences should be considered to ensure 
high levels of transparency and openness if their parameter s, including the weights, the information on the model 
architecture, and the information on model usage are made publicly available. The licence should be considered to be 
free and open-source also when it allows users to run, copy, distr ibut e, study , change and improve software and data, 
including models under the condition that the original provider of the model is credited, the identical or compar able 
terms of distr ibution are respected.
(103) Free and open-source AI comp onents covers the software and data, including models and general-pur pose AI 
models, tools, services or processes of an AI syste m. Free and open-source AI components can be provided through 
different channels, including their development on open repositor ies. For the purposes of this Regulation, AI 
com ponents that are provid ed against a price or other wise monetised, including through the provision of technical 
suppor t or other services, including through a software platf orm, related to the AI comp onent, or the use of 
personal data for reasons other than exclusively for imp roving the secur ity, compatibility or interoperability of the 
software, with the exception of transactions between microenter prises, should not benef it from the excep tions 
provided to free and open-source AI components. The fact of making AI components availa ble through open 
reposit ories should not, in itself, constitute a monetisation.
(104) The provid ers of general-pur pose AI models that are released under a free and open-source licence, and whose 
paramet ers, including the weights, the information on the model archit ecture, and the information on model usage, 
are made publicly available should be subject to exceptions as rega rds the transparency-related requirements 
imposed on general-pur pose AI models, unless they can be considered to present a systemic risk, in which case the 
circumstance that the model is transparent and accompanied by an open-source license should not be considered to 
be a suffi cient reason to exclude compliance with the obligati ons under this Regulation. In any case, given that the 
release of general-pur pose AI models under free and open-source licence does not necessar ily reveal substantial 
information on the data set used for the training or fine-tuning of the model and on how compliance of copyright 
law was thereby ensured, the excep tion provid ed for general-pur pose AI models from compliance with the 
transparency-relate d requirements should not concer n the obligati on to produce a summar y about the cont ent used 
for model training and the obligation to put in place a policy to comply with Union copyright law, in particular to 
identify and comply with the reser vation of rights pursuant to Article 4(3) of Directive (EU) 2019/790 of the 
European Parliament and of the Council (40).
(105) General-pur pose AI models, in particular large generative AI models, capable of generating text, imag es, and other 
cont ent, present unique innovation oppor tunities but also challeng es to artists, authors, and other creators and the 
way their creative content is create d, distr ibut ed, used and consumed. The development and training of such models 
require access to vast amounts of text, images, videos and other data. Text and data mining techniques may be used 
exte nsively in this context for the retrieval and analysis of such cont ent, which may be protect ed by copyright and 
relate d rights. Any use of copyright protect ed cont ent requires the author isation of the rightsholder concer ned 
unless relevant copyright excep tions and limitations apply . Directive (EU) 2019/790 introduced exceptions and 
limitations allowing reproductions and extractions of works or other subject matter , for the purpose of text and data OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 27/144(40) Directive (EU) 2019/790 of the European Parliament and of the Council of 17 Apr il 2019 on copyright and relat ed rights in the 
Digital Sing le Market and amending Directives 96/9/EC and 2001/29/EC (OJ L 130, 17.5.2019, p. 92).mining, under certain conditions. Und er these rules, rightsholders may choose to reser ve their rights over their 
works or other subject matter to prevent text and data mining, unless this is done for the purposes of scientific 
researc h. Where the rights to opt out has been expressly reser ved in an appropr iate manner , provid ers of 
general-pur pose AI models need to obtain an author isation from rightsholders if they want to carry out text and 
data mining over such works.
(106) Provi ders that place general-pur pose AI models on the Union market should ensure com pliance with the relevant 
oblig ations in this Regulation. To that end, providers of general-pur pose AI models should put in place a policy to 
com ply with Union law on copyright and related rights, in particular to identify and com ply with the reser vation of 
rights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. Any provider placing 
a general-pur pose AI model on the Union market should comply with this obligation, regard less of the jurisdiction 
in which the copyright-relevant acts under pinning the training of those general-pur pose AI models take place. This 
is necessar y to ensure a level playing field among provider s of gene ral-pur pose AI models where no provider should 
be able to gain a comp etitive advantage in the Union market by applying lower copyright standards than those 
provided in the Union.
(107) In order to increase transparency on the data that is used in the pre-training and training of general-pur pose AI 
models, including text and data protect ed by copyright law, it is adequate that provid ers of such models draw up and 
mak e publicly available a sufficiently detailed summar y of the content used for training the general-pur pose AI 
model. While taking into due account the need to protect trade secrets and conf idential business information, this 
summar y should be generally comp rehensive in its scope instead of technically detailed to facilitat e parties with 
legitimat e interests, including copyright holders, to exercise and enforce their rights under Union law, for exam ple by 
listing the main data collections or sets that went into training the model, such as large private or public databases or 
data archives, and by providing a narrative explanation about other data sources used. It is appropr iate for the AI 
Office to provid e a template for the summar y, which should be simple, effective, and allow the provid er to provide 
the required summar y in narrative form.
(108) With rega rd to the obliga tions imp osed on providers of general-pur pose AI models to put in place a policy to 
com ply with Union copyright law and make publicly available a summar y of the cont ent used for the training, the AI 
Office should monitor whether the provider has fulfilled those obliga tions without verifying or proceeding to 
a work-by-w ork assessment of the training data in terms of copyright comp liance. This Regulation does not affect 
the enforcement of copyright rules as provided for under Union law.
(109) Comp liance with the obliga tions applicable to the providers of general-pur pose AI models should be commensurate 
and propor tionate to the type of model provider , excluding the need for compliance for persons who develop or use 
models for non-profess ional or scientific research purposes, who should never theless be encouraged to voluntar ily 
com ply with these requirements. Without prejudice to Union copyright law, comp liance with those obligations 
should take due account of the size of the provider and allow simplified ways of compliance for SMEs, including 
start-ups, that should not represent an excessive cost and not discourage the use of such models. In the case of 
a modifi cation or fine-tuning of a model, the obligations for provid ers of general-pur pose AI models should be 
limit ed to that modifi cation or fine-tuning, for exam ple by complementing the already existing technical 
documentation with information on the modifi cations, including new training data sources, as a means to com ply 
with the value chain oblig ations provided in this Regulation.
(110) General-pur pose AI models could pose systemic risks which include, but are not limit ed to, any actual or reasonably 
foreseeable nega tive effects in relation to major accidents, disruptions of critical sectors and serious consequences to 
public health and safety; any actual or reasonably foreseeable negative effects on democratic processes, public and 
economic secur ity; the dissemination of illega l, false, or discr iminat ory cont ent. Systemic risks should be understood 
to increase with model capabilities and model reach, can arise along the entire lifecy cle of the model, and are 
influenced by conditions of misuse, model reliability , model fairness and model secur ity, the level of autonom y of EN OJ L, 12.7.2024
28/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojthe model, its access to tools, novel or combined modalities, release and distr ibution strategies, the potential to 
remo ve guardrails and other factor s. In particular , international approac hes have so far identifie d the need to pay 
attention to risks from pote ntial intent ional misuse or uninte nded issues of control relating to alignment with 
human intent; chemical, biological, radiological, and nuclear risks, such as the ways in which barriers to entr y can be 
lowered, including for weapons development, design acquisition, or use; offensive cyber capabilities, such as the 
ways in vulnerability discove ry, exploitation, or operational use can be enabled; the effects of interac tion and tool 
use, including for exam ple the capacity to control physical systems and interfere with critical infrastr ucture; risks 
from models of making copies of themselves or ‘self-replicating’ or training other models; the ways in which models 
can give rise to harmful bias and discr imination with risks to individuals, communities or societies; the facili tation of 
disinf ormation or harming privacy with threats to democratic values and human rights; risk that a particular event 
could lead to a chain reaction with considerable nega tive effects that could affect up to an entire city, an entire 
domain activity or an entire community .
(111) It is appropr iate to establish a methodology for the classif ication of general-pur pose AI models as general-pur pose 
AI model with systemic risks. Since syste mic risks result from particularly high capabilities, a general-pur pose AI 
model should be considered to present systemic risks if it has high-impact capabilities, evaluate d on the basis of 
appropr iate technical tools and methodologies, or significant impact on the internal market due to its reac h. 
High-im pact capabilities in general-pur pose AI models means capabilities that matc h or exceed the capabilities 
recorded in the most advanced general-pur pose AI models. The full range of capabilities in a model could be better 
understo od after its placing on the marke t or when deplo yers interact with the model. According to the state of the 
art at the time of entr y into force of this Regulation, the cumulative amount of comp utation used for the training of 
the general-pur pose AI model measured in floating point operations is one of the relevant appro ximations for model 
capabilities. The cumulative amount of comp utation used for training includes the computation used across the 
activities and methods that are intende d to enhance the capabilities of the model prior to deplo yment, such as 
pre-training, synthetic data generation and fine-tuning. Theref ore, an initial threshold of floating point operations 
should be set, which , if met by a general-pur pose AI model, leads to a presump tion that the model is 
a general-pur pose AI model with systemic risks. This threshold should be adjuste d over time to reflect technological 
and industr ial chang es, such as algor ithmic improvements or increased hardware efficiency , and should be 
supplement ed with bench marks and indicator s for model capability . To inform this, the AI Offi ce should enga ge 
with the scientific community , industr y, civil society and other exper ts. Thresholds, as well as tools and bench mark s 
for the assessment of high-imp act capabilities, should be strong predict ors of generality , its capabilities and 
associat ed systemic risk of general-pur pose AI models, and could take into account the way the model will be placed 
on the market or the number of users it may affect. To com plement this system, there should be a possibility for the 
Commission to take individual decisions designating a general-pur pose AI model as a general-pur pose AI model 
with systemic risk if it is found that such model has capabilities or an impact equivalent to those capture d by the set 
threshold. That decision should be take n on the basis of an overa ll assessment of the criteria for the designation of 
a general-pur pose AI model with systemic risk set out in an annex to this Regulation, such as quality or size of the 
training data set, number of business and end users, its input and output modalities, its level of autonom y and 
scalability , or the tools it has access to. Upon a reasoned request of a provider whose model has been designated as 
a general-pur pose AI model with systemic risk, the Commission should take the request into account and may 
decide to reassess whether the general-pur pose AI model can still be considered to present syste mic risks.
(112) It is also necessar y to clarify a procedure for the classific ation of a general-pur pose AI model with systemic risks. 
A general-pur pose AI model that meets the applicable threshold for high-imp act capabilities should be presumed to 
be a general-pur pose AI models with syste mic risk. The provid er should notify the AI Offi ce at the latest two weeks 
after the requirements are met or it becomes kno wn that a general-pur pose AI model will meet the requirements 
that lead to the presump tion. This is especially relevant in relation to the threshold of floating point operations 
because training of gene ral-pur pose AI models take s considerable planning which includes the upfront allocation of 
com pute resources and, theref ore, provid ers of general-pur pose AI models are able to kno w if their model would 
meet the threshold before the training is complet ed. In the cont ext of that notif ication, the provid er should be able to 
demonstrat e that, because of its specif ic character istics, a general-pur pose AI model exceptionally does not present 
syste mic risks, and that it thus should not be classified as a general-pur pose AI model with syste mic risks. That 
information is valuable for the AI Office to anticipate the placing on the market of general-pur pose AI models with 
syste mic risks and the provider s can start to engag e with the AI Office early on. That information is especially OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 29/144important with regard to general-pur pose AI models that are planned to be released as open-source, given that, after 
the open-source model release, necessar y measures to ensure compliance with the obliga tions under this Regulation 
may be more diffic ult to imp lement.
(113) If the Commission becomes awar e of the fact that a general-pur pose AI model meets the requirements to classify as 
a general-pur pose AI model with syste mic risk, which previously had either not been kno wn or of which the 
relevant provider has failed to notify the Commission, the Commission should be empo wered to designate it so. 
A system of qualified alerts should ensure that the AI Offi ce is made aware by the scientifi c panel of general-pur pose 
AI models that should possibly be classified as general-pur pose AI models with syste mic risk, in addition to the 
monito ring activities of the AI Office.
(114) The providers of general-pur pose AI models presenting syste mic risks should be subject, in addition to the 
oblig ations provid ed for provid ers of general-pur pose AI models, to oblig ations aimed at identifying and mitig ating 
those risks and ensur ing an adequate level of cybersecur ity protection, regardless of whether it is provided as 
a standalone model or embedded in an AI system or a product. To achi eve those objectives, this Regulation should 
require provid ers to perfo rm the necessar y model evaluations, in particular prior to its first placing on the mark et, 
including conducting and documenting adversar ial testing of models, also, as appropr iate, through inter nal or 
independent exte rnal testing. In addition, provider s of general-pur pose AI models with systemic risks should 
continuously assess and mitig ate systemic risks, including for exam ple by putting in place risk-management policies, 
such as accountability and gover nance processes, imp lementing post-mark et monitoring, taking appropr iate 
measures along the entire model’s lifecycle and cooperating with relevant actor s along the AI value chai n.
(115) Provi ders of general-pur pose AI models with systemic risks should assess and mitiga te possible systemic risks. If, 
despite efforts to identify and prevent risks related to a general-pur pose AI model that may present syste mic risks, 
the development or use of the model causes a serious incident, the general-pur pose AI model provid er should 
without undue dela y keep track of the incident and repor t any relevant information and possible corrective measures 
to the Commission and national compet ent author ities. Further more, provider s should ensure an adequate level of 
cybersecur ity prot ection for the model and its physical infrastr ucture, if appropr iate, along the entire model lifecy cle. 
Cybersecur ity protection related to systemic risks associated with malicious use or attacks should duly consider 
accidental model leakage, unauthor ised releases, circum vention of safety measures, and defe nce against cyberattacks, 
unauthor ised access or model thef t. That prot ection could be facili tated by secur ing model weights, algor ithms, 
servers, and data sets, such as through operational secur ity measures for information secur ity, specif ic cybersecur ity 
policies, adequat e technical and established solutions, and cyber and phys ical access controls, appropr iate to the 
relevant circumstances and the risks involved.
(116) The AI Office should encourage and facilitate the drawing up, review and adap tation of codes of practice, taking into 
account inter national approaches. All providers of gene ral-pur pose AI models could be invit ed to participate. To 
ensure that the codes of practice reflect the state of the art and duly take into account a diverse set of perspectives, 
the AI Office should collaborate with relevant national comp etent author ities, and could, where appropr iate, consult 
with civil society organisations and other relevant stakeholders and exper ts, including the Scientif ic Panel, for the 
drawing up of such codes. Codes of practice should cover oblig ations for provider s of general-pur pose AI models 
and of general-pur pose AI models presenting systemic risks. In addition, as regard s systemic risks, codes of practice 
should help to establish a risk taxonomy of the type and nature of the syste mic risks at Union level, including their 
sources. Codes of practice should also be focused on specific risk assessment and mitiga tion measures.
(117) The codes of practice should represent a central tool for the proper compliance with the obliga tions provided for 
under this Regulation for provider s of general-pur pose AI models. Provi ders should be able to rely on codes of 
practice to demonstrate compliance with the obliga tions. By means of imp lementing acts, the Commission may 
decide to approve a code of practice and give it a general validity within the Union, or, alternatively , to provide 
common rules for the imp lementation of the relevant obliga tions, if, by the time this Regulation becomes applicable, 
a code of practice cannot be final ised or is not deemed adequate by the AI Office. Once a harmonised standard is EN OJ L, 12.7.2024
30/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojpublished and assessed as suitable to cover the relevant oblig ations by the AI Office, comp liance with a European 
harmonised standard should grant providers the presump tion of conf ormity . Provi ders of general-pur pose AI 
models should further more be able to demonstrate comp liance using alter native adequate means, if codes of practice 
or harmonised standards are not available, or they choose not to rely on those.
(118) This Regulation regulate s AI systems and AI models by imposing certain requirements and obliga tions for relevant 
mark et actor s that are placing them on the market, putting into service or use in the Union, thereby complementing 
oblig ations for provid ers of inter mediar y services that embed such systems or models into their services regulate d by 
Regulation (EU) 2022/2065. To the extent that such syste ms or models are embedded into designated very large 
online platf orms or very large online search engines, they are subject to the risk-manag ement framew ork provid ed 
for in Regulation (EU) 2022/2065. Consequently , the corresponding obliga tions of this Regulation should be 
presumed to be fulfilled, unless significant syste mic risks not covered by Regulation (EU) 2022/2065 emerg e and are 
identified in such models. Within this framework, providers of very large online platf orms and very large online 
searc h engines are oblige d to assess potential systemic risks stemming from the design, functioning and use of their 
services, including how the design of algor ithmic systems used in the service may contr ibut e to such risks, as well as 
syste mic risks stemming from potent ial misuses. Those provid ers are also oblige d to take appropr iate mitig ating 
measures in obser vance of fundamental rights.
(119) Consider ing the quick pace of inno vation and the technologi cal evolution of digital services in scope of different 
instr uments of Union law in particular having in mind the usage and the percep tion of their recipients, the AI 
syste ms subject to this Regulation may be provid ed as interm ediar y services or parts thereof within the meaning of 
Regulation (EU) 2022/2065, which should be inter prete d in a technology-neutral manner . For exam ple, AI syste ms 
may be used to provide online search engines, in particular , to the exte nt that an AI syste m such as an online chatbot 
perf orms search es of, in principle, all websites, then incor porates the results into its existing knowle dge and uses the 
updat ed knowledg e to generate a sing le output that combines different sources of information.
(120) Further more, obliga tions placed on provid ers and deplo yers of certain AI syste ms in this Regulation to enable the 
detection and disclosure that the outputs of those syste ms are artificially generat ed or manipulated are particularly 
relevant to facilitate the effective imp lementation of Regulation (EU) 2022/2065. This applies in particular as regard s 
the obliga tions of provid ers of very large online platf orms or very large online search engines to identify and 
mitig ate syste mic risks that may arise from the dissemination of content that has been artificially generated or 
manipulat ed, in particular risk of the actual or foreseeable negati ve effects on democratic processes, civic discourse 
and elect oral processes, including through disinf ormation.
(121) Standardisation should play a key role to provid e technical solutions to provider s to ensure comp liance with this 
Regulation, in line with the state of the art, to promote inno vation as well as comp etitiveness and growth in the 
sing le market. Comp liance with harmonised standards as defined in Article 2, point (1)(c), of Regulation (EU) 
No 1025/2012 of the European Parliament and of the Council (41), which are normally expected to reflect the state 
of the art, should be a means for providers to demonstrate conf ormity with the requirements of this Regulation. 
A balanced representation of interests involving all relevant stak eholders in the development of standards, in 
particular SMEs, consumer organisations and envir onmental and social stakeholders in accordance with Articles 5 
and 6 of Regulation (EU) No 1025/2012 should theref ore be encouraged. In order to facilitate compliance, the 
standardisation requests should be issued by the Commission without undue dela y. When prepar ing the 
standardisation request, the Commission should consult the advisor y forum and the Board in order to collect 
relevant exper tise. However , in the absence of relevant refere nces to harmonised standards, the Commission should 
be able to establish, via implementing acts, and after consultation of the advisor y forum, common specif ications for 
certain requirements under this Regulation. The common specif ication should be an exceptional fall back solution to 
facilitate the provid er’s obliga tion to comp ly with the requirements of this Regulation, when the standardisation 
request has not been accept ed by any of the European standardisation organisations, or when the relevant 
harmonised standards insufficiently address fundamental rights concer ns, or when the harmonised standards do not 
com ply with the request, or when there are dela ys in the adoption of an appropr iate harmonised standard. Where 
such a dela y in the adoption of a harmonised standard is due to the technical complexity of that standard, this should OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 31/144(41) Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, 
amending Council Directives 89/686/EEC and 93/15/EEC and Directives 94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, 
2004/22/EC, 2007/23/EC, 2009/23/EC and 2009/105/EC of the European Parliament and of the Council and repealing Council 
Decision 87/95/EEC and Decision No 1673/2006/EC of the European Parliament and of the Council (OJ L 316, 14.11.2012, p. 12).be considered by the Commission before contem plating the establishment of common specifications. When 
developing common specific ations, the Commission is encourage d to cooperate with international partners and 
international standardisation bodies.
(122) It is appropr iate that, without prejudice to the use of harmonised standards and common specifications, providers of 
a high-r isk AI syste m that has been trained and tested on data reflecting the specific geographical, behavi oural, 
cont extual or functional setting within which the AI system is intended to be used, should be presumed to comp ly 
with the relevant measure provided for under the requirement on data gove rnance set out in this Regulation. 
Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, in accordance 
with Article 54(3) of Regulation (EU) 2019/881, high-r isk AI syste ms that have been certified or for which 
a statement of conf ormity has been issued under a cybersecur ity scheme pursuant to that Regulation and the 
references of which have been published in the Official Journal of the European Union should be presumed to comply 
with the cybersecur ity requirement of this Regulation in so far as the cybersecur ity certificate or statem ent of 
conf ormity or parts thereof cover the cybersecur ity requirement of this Regulation. This remains without prejudice 
to the voluntar y nature of that cybersecur ity scheme.
(123) In order to ensure a high level of trustw orthiness of high-r isk AI systems, those systems should be subject to 
a conf ormity assessment prior to their placing on the marke t or putting into service.
(124) It is appropr iate that, in order to minimise the burden on operat ors and avoid any possible duplication, for high-r isk 
AI syste ms related to products which are cover ed by existing Union harmonisation legislation based on the New 
Legislative Framewo rk, the compliance of those AI systems with the requirements of this Regulation should be 
assessed as part of the conf ormity assessment already provided for in that law. The applicability of the requirements 
of this Regulation should thus not affect the specif ic logic, methodology or general structure of conf ormity 
assessment under the relevant Union harmonisation legislation.
(125) Given the comp lexity of high-r isk AI systems and the risks that are associate d with them, it is impor tant to develop 
an adequate conf ormity assessment procedure for high-r isk AI syste ms involving notified bodies, so-called third 
party conf ormity assessment. However , given the current exper ience of professional pre-market certifiers in the field 
of product safety and the different nature of risks involved, it is appropr iate to limit, at least in an initial phase of 
application of this Regulation, the scope of application of third-par ty conf ormity assessment for high-r isk AI 
syste ms other than those related to products. Theref ore, the conf ormity assessment of such syste ms should be 
carried out as a general rule by the provider under its own responsibility , with the only excep tion of AI syste ms 
intended to be used for biometr ics.
(126) In order to carry out third-par ty conf ormity assessments when so required, notified bodies should be notif ied under 
this Regulation by the national comp etent author ities, provided that they comp ly with a set of requirements, in 
particular on independence, compet ence, absence of conf licts of interests and suitable cybersecur ity requirements. 
Notification of those bodies should be sent by national compet ent author ities to the Commission and the other 
Member States by means of the electronic notif ication tool developed and managed by the Commission pursuant to 
Article R23 of Annex I to Decision No 768/2008/EC.
(127) In line with Union commitments under the World Trade Organization Agreement on Technical Barriers to Trade, it is 
adequat e to facilitate the mutual recognition of conf ormity assessment results produced by comp etent conf ormity 
assessment bodies, independent of the territory in which they are established, provid ed that those conf ormity 
assessment bodies established under the law of a third countr y meet the applicable requirements of this Regulation 
and the Union has concluded an agreement to that exte nt. In this context, the Commission should actively explore 
possible international instr uments for that purpose and in particular pursue the conclusion of mutual recognition 
agreements with third countr ies.
(128) In line with the commonly established notion of substantial modif ication for products regulate d by Union 
harmonisation legislation, it is appropr iate that whenever a chang e occurs which may affect the comp liance of 
a high-r isk AI system with this Regulation (e.g. change of operating syste m or software arch itecture), or when the 
intended purpose of the syste m change s, that AI syste m should be considered to be a new AI system which should 
undergo a new conf ormity assessment. How ever , changes occur ring to the algor ithm and the perf ormance of AI 
syste ms which continue to ‘lear n’ after being placed on the market or put into service, namely automat ically 
adap ting how functions are carried out, should not constitute a substantial modifi cation, provid ed that those 
change s have been pre-dete rmined by the provider and assessed at the moment of the conf ormity assessment.EN OJ L, 12.7.2024
32/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(129) High-r isk AI systems should bear the CE marking to indicate their conf ormity with this Regulation so that they can 
move freely within the intern al mark et. For high-r isk AI systems embedded in a product, a physica l CE marking 
should be affixed, and may be comp lemented by a digital CE marking. For high-r isk AI systems only provid ed 
digitally , a digital CE marking should be used. Member States should not create unjustified obstacles to the placing 
on the marke t or the putting into service of high-r isk AI systems that comp ly with the requirements laid down in 
this Regulation and bear the CE marking.
(130) Und er certain conditions, rapid availability of inno vative technologies may be crucial for health and safety of 
persons, the prot ection of the environment and climate change and for society as a whole. It is thus appropr iate that 
under exceptional reasons of public secur ity or prot ection of life and health of natural persons, envir onmental 
prot ection and the prot ection of key industr ial and infrastr uctural assets, marke t surveillance author ities could 
author ise the placing on the market or the putting into service of AI systems which have not undergone 
a conf ormity assessment. In duly justified situations, as provided for in this Regulation, law enforcement author ities 
or civil prot ection author ities may put a specific high-r isk AI syste m into service without the author isation of the 
mark et surveillance author ity, provided that such author isation is request ed during or after the use without undue 
dela y.
(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase the 
transparency towards the public, provider s of high-r isk AI syste ms other than those related to products falling within 
the scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI system 
listed in the high-r isk use cases in an annex to this Regulation is not high-r isk on the basis of a deroga tion, should be 
required to register themselves and information about their AI syste m in an EU database, to be established and 
manag ed by the Commission. Before using an AI syste m listed in the high-r isk use cases in an annex to this 
Regulation, deplo yers of high-r isk AI systems that are public author ities, agencies or bodies, should regist er 
themselves in such database and select the syste m that they envisag e to use. Other deplo yers should be entitled to do 
so voluntar ily. This section of the EU database should be publicly accessible, free of charg e, the information should 
be easily navig able, understandable and machine-r eadable. The EU database should also be user -friendly , for exam ple 
by providing search functionalities, including through keywo rds, allowi ng the general public to find relevant 
information to be submitted upon the registration of high-r isk AI syste ms and on the use case of high-r isk AI 
syste ms, set out in an annex to this Regulation, to which the high-r isk AI systems correspond. Any substantial 
modification of high-r isk AI systems should also be registered in the EU database. For high-r isk AI syste ms in the 
area of law enforcement, migration, asylum and border control managem ent, the registration obliga tions should be 
fulfilled in a secure non-public section of the EU database. Access to the secure non-public section should be strictly 
limit ed to the Commission as well as to market surveillance author ities with regard to their national section of that 
database. High-r isk AI systems in the area of critical infrastr ucture should only be register ed at national level. The 
Commission should be the controller of the EU database, in accordance with Regulation (EU) 2018/1725. In order 
to ensure the full functionality of the EU database, when deplo yed, the procedure for setting the database should 
include the development of functional specif ications by the Commission and an independent audit repor t. The 
Commission should take into account cybersecur ity risks when carrying out its tasks as data controller on the EU 
database. In order to maximise the availability and use of the EU database by the public, the EU database, including 
the information made available through it, should comply with requirements under the Directive (EU) 2019/882.
(132) Certain AI systems intended to interac t with natural persons or to generate cont ent may pose specific risks of 
impersonation or decept ion irrespective of whether they qualify as high-r isk or not. In certain circumstances, the use 
of these systems should theref ore be subject to specif ic transparency obligations without prejudice to the 
requirements and obligations for high-r isk AI systems and subject to target ed exceptions to take into account the 
special need of law enforcement. In particular , natural persons should be notified that they are interacting with an AI 
syste m, unless this is obvious from the point of view of a natural person who is reasonably well-inf ormed, obser vant 
and circumspect taking into account the circumstances and the context of use. When implementing that obligation, 
the character istics of natural persons belonging to vulnerable groups due to their age or disability should be take n 
into account to the exte nt the AI system is intended to interact with those groups as well. Moreove r, natural persons 
should be notified when they are exposed to AI systems that, by processing their biometr ic data, can identify or infer 
the emotions or intentions of those persons or assign them to specific cate gories. Such specif ic categor ies can relate 
to aspects such as sex, age, hair colour , eye colour , tattoos, personal traits, ethnic origin, personal pref erences and 
interests. Such information and notif ications should be provided in accessible formats for persons with disabilities.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 33/144(133) A variety of AI systems can generate large quantities of synthetic cont ent that becomes increasing ly hard for humans 
to distinguish from human-generat ed and authentic cont ent. The wide availa bility and increasing capabilities of 
those systems have a significant impact on the integr ity and trust in the information ecosystem, raising new risks of 
misinf ormation and manipulation at scale, fraud, impersonation and consumer decep tion. In light of those impacts, 
the fast technological pace and the need for new methods and techniques to trace origin of information, it is 
appropr iate to require providers of those systems to embed technical solutions that enable marking in a machi ne 
readable format and detect ion that the output has been generated or manipulate d by an AI syste m and not a human. 
Such techniques and methods should be sufficiently reliable, interoperable, effective and robust as far as this is 
technically feasible, taking into account available techniques or a combination of such techniques, such as 
watermarks, metadata identifica tions, cryptographic methods for provin g prove nance and authenticity of cont ent, 
logging methods, finger prints or other techniques, as may be appropr iate. When imp lementing this obligation, 
providers should also take into account the specif icities and the limitations of the different types of cont ent and the 
relevant technological and mark et developments in the field, as reflected in the generally ackno wledged state of the 
art. Such techniques and methods can be implemented at the level of the AI syste m or at the level of the AI model, 
including general-pur pose AI models generating content, thereby facilitating fulfilment of this obliga tion by the 
downstream provid er of the AI system. To remain propor tionate , it is appropr iate to envisage that this marking 
oblig ation should not cover AI syste ms perf orming primar ily an assistive function for standard editing or AI syste ms 
not substantially alter ing the input data provided by the deplo yer or the semantics thereof.
(134) Further to the technical solutions emp loyed by the provider s of the AI system, deplo yers who use an AI syste m to 
generate or manipulate imag e, audio or video content that appreciably resembles existing persons, objects, places, 
entities or events and would falsely appear to a person to be authentic or truthful (deep fakes), should also clearly 
and distinguishably disclose that the cont ent has been artificially create d or manipulated by labelling the AI output 
according ly and disclosing its artificial origin. Compliance with this transparency obligation should not be 
interpreted as indicating that the use of the AI syste m or its output impedes the right to freedom of expression and 
the right to freedom of the arts and sciences guarante ed in the Char ter, in particular where the content is part of an 
evidently creative, satir ical, artistic, fictional or analogous work or programme, subject to appropr iate safeguards for 
the rights and freedoms of third parties. In those cases, the transparency obliga tion for deep fakes set out in this 
Regulation is limited to disclosure of the exist ence of such generated or manipulate d cont ent in an appropr iate 
manner that does not hamper the displa y or enjo yment of the work, including its normal exploitation and use, while 
maintaining the utility and quality of the work. In addition, it is also appropr iate to envisag e a similar disclosure 
oblig ation in relation to AI-g enerated or manipulate d text to the extent it is published with the purpose of informing 
the public on matters of public interest unless the AI-g enerated content has undergone a process of human review or 
editorial control and a natural or legal person holds editor ial responsibility for the publication of the content.
(135) Without prejudice to the mandat ory nature and full applicability of the transparency oblig ations, the Commission 
may also encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective 
implementation of the obliga tions regarding the detection and labelling of artificially generated or manipulated 
cont ent, including to suppor t practical arrang ements for making, as appropr iate, the detection mec hanisms 
accessible and facilitating cooperation with other actor s along the value chain, disseminating content or chec king its 
authenticity and provenance to enable the public to effectively distinguish AI-g enerated cont ent.
(136) The obliga tions placed on provid ers and deplo yers of certain AI syste ms in this Regulation to enable the detection 
and disclosure that the outputs of those systems are artificially generat ed or manipulat ed are particularly relevant to 
facilitate the effective imp lementation of Regulation (EU) 2022/2065. This applies in particular as rega rds the 
oblig ations of providers of very large online platf orms or very large online search engines to identify and mitigat e 
syste mic risks that may arise from the dissemination of cont ent that has been artificially generated or manipulate d, 
in particular the risk of the actual or foreseeable nega tive effects on democratic processes, civic discourse and 
elect oral processes, including through disinf ormation. The requirement to label content generated by AI syste ms 
under this Regulation is without prejudice to the obliga tion in Article 16(6) of Regulation (EU) 2022/2065 for 
providers of hosting services to process notices on illegal content received pursuant to Article 16(1) of that 
Regulation and should not influence the assessment and the decision on the illegality of the specif ic content. That 
assessment should be perform ed solely with reference to the rules gover ning the lega lity of the cont ent.EN OJ L, 12.7.2024
34/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(137) Comp liance with the transparency obliga tions for the AI syste ms cover ed by this Regulation should not be 
interpreted as indicating that the use of the AI syste m or its output is lawful under this Regulation or other Union 
and Member State law and should be without prejudice to other transparency obliga tions for deplo yers of AI syste ms 
laid down in Union or national law.
(138) AI is a rapidly developing family of technologies that requires regulatory oversight and a safe and controlled space 
for exper imentation, while ensur ing responsible innovation and integration of appropr iate safegua rds and risk 
mitig ation measures. To ensure a lega l framew ork that promotes innovation, is future-proof and resilient to 
disruption, Member States should ensure that their national compet ent author ities establish at least one AI 
regulat ory sandbo x at national level to facilitate the development and testing of inno vative AI syste ms under strict 
regulat ory oversight before these systems are placed on the marke t or other wise put into service. Member States 
could also fulfil this obligation through participating in already existing regulator y sandbo xes or establishing jointly 
a sandbo x with one or more Member States’ compet ent author ities, insofar as this participation provid es equivalent 
level of national coverag e for the participating Member States. AI regulator y sandbo xes could be established in 
phys ical, digital or hybri d form and may accommodate physica l as well as digital products. Establishing author ities 
should also ensure that the AI regulatory sandbo xes have the adequate resources for their functioning, including 
financial and human resources.
(139) The objectives of the AI regulato ry sandbo xes should be to foster AI inno vation by establishing a controlled 
exper imentation and testing environment in the development and pre-marketing phase with a view to ensur ing 
com pliance of the inno vative AI systems with this Regulation and other relevant Union and national law. Moreove r, 
the AI regulato ry sandbo xes should aim to enhance legal certainty for inno vators and the compet ent author ities’ 
oversight and understanding of the oppor tunities, emerging risks and the impacts of AI use, to facilitate regulator y 
learning for author ities and under takings, including with a view to future adap tions of the lega l framew ork, to 
suppor t cooperation and the shar ing of best practices with the author ities involved in the AI regulator y sandbo x, 
and to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulator y 
sandbo xes should be widely available throughout the Union, and particular attention should be given to their 
accessibility for SMEs, including start-ups. The participation in the AI regulato ry sandbo x should focus on issues that 
raise lega l uncer tainty for provid ers and prospective provider s to inno vate, exper iment with AI in the Union and 
contr ibut e to evidence-based regulator y learning. The super vision of the AI systems in the AI regulator y sandbo x 
should theref ore cover their development, training, testing and validation before the syste ms are placed on the 
mark et or put into service, as well as the notion and occur rence of substantial modifi cation that may require a new 
conf ormity assessment procedure. Any signifi cant risks identified during the development and testing of such AI 
syste ms should result in adequat e mitiga tion and, failing that, in the suspension of the development and testing 
process. Where appropr iate, national com petent author ities establishing AI regulator y sandbo xes should cooperate 
with other relevant author ities, including those super vising the prot ection of fundamental rights, and could allow for 
the involvement of other actor s within the AI ecosystem such as national or European standardisation organisations, 
notified bodies, testing and exper imentation facilities, research and exper imentation labs, European Digital 
Inno vation Hubs and relevant stak eholder and civil society organisations. To ensure unif orm implementation across 
the Union and economies of scale, it is appropr iate to establish common rules for the AI regulator y sandbo xes’ 
implementation and a framework for cooperation between the relevant author ities involved in the super vision of the 
sandbo xes. AI regulatory sandbo xes established under this Regulation should be without prejudice to other law 
allowing for the establishment of other sandbo xes aiming to ensure comp liance with law other than this Regulation. 
Where appropr iate, relevant comp etent author ities in charge of those other regulato ry sandbo xes should consider 
the benefits of using those sandbo xes also for the purpose of ensur ing comp liance of AI syste ms with this 
Regulation. Upon agreement between the national compet ent author ities and the participants in the AI regulator y 
sandbo x, testing in real world conditions may also be operated and super vised in the framework of the AI regulator y 
sandbo x.
(140) This Regulation should provid e the lega l basis for the provid ers and prospective provid ers in the AI regulator y 
sandbo x to use personal data collect ed for other purposes for developing certain AI systems in the public interest 
within the AI regulator y sandbo x, only under specif ied conditions, in accordance with Article 6(4) and Article 9(2), 
point (g), of Regulation (EU) 2016/679, and Articles 5, 6 and 10 of Regulation (EU) 2018/1725, and without 
prejudice to Article 4(2) and Article 10 of Directive (EU) 2016/680. All other obliga tions of data controllers and 
rights of data subjects under Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 remain 
applicable. In particular , this Regulation should not provide a lega l basis in the meaning of Article 22(2), point (b) of 
Regulation (EU) 2016/679 and Article 24(2), point (b) of Regulation (EU) 2018/1725. Providers and prospective OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 35/144providers in the AI regulator y sandbo x should ensure appropr iate safeguards and cooperate with the compet ent 
author ities, including by following their guidance and acting expeditiously and in good faith to adequat ely mitigat e 
any identifie d signifi cant risks to safety, health, and fundamental rights that may arise during the development, 
testing and exper imentation in that sandbo x.
(141) In order to accelerate the process of development and the placing on the marke t of the high-r isk AI syste ms listed in 
an annex to this Regulation, it is impor tant that provid ers or prospective providers of such syste ms may also benefit 
from a specif ic regime for testing those systems in real world conditions, without participating in an AI regulator y 
sandbo x. However , in such cases, taking into account the possible consequences of such testing on individuals, it 
should be ensured that appropr iate and sufficient guarantees and conditions are introduced by this Regulation for 
providers or prospective provider s. Such guarantees should include, inter alia, requesting informed consent of 
natural persons to participate in testing in real world conditions, with the exception of law enforcement where the 
seeking of informed consent would prevent the AI syste m from being tested. Consent of subjects to participate in 
such testing under this Regulation is distinct from, and without prejudice to, consent of data subjects for the 
processing of their personal data under the relevant data protection law. It is also imp ortant to minimise the risks 
and enable oversight by compet ent author ities and theref ore require prospective provid ers to have a real-wo rld 
testing plan submitted to comp etent mark et surveillance author ity, regist er the testing in dedicated sections in the EU 
database subject to some limited exceptions , set limitations on the period for which the testing can be done and 
require additional safegua rds for persons belonging to certain vulnerable groups, as well as a written agreement 
defining the roles and responsibilities of prospective providers and deplo yers and effective overs ight by compet ent 
personnel involved in the real world testing. Further more, it is appropr iate to envisag e additional safeguards to 
ensure that the predictions, recommendations or decisions of the AI system can be effectively reversed and 
disreg arded and that personal data is protect ed and is deleted when the subjects have withdra wn their consent to 
participate in the testing without prejudice to their rights as data subjects under the Union data protection law. As 
rega rds transfer of data, it is also appropr iate to envis age that data collect ed and processed for the purpose of testing 
in real-world conditions should be transf erred to third countr ies only where appropr iate and applicable safeguards 
under Union law are implemented, in particular in accordance with bases for transf er of personal data under Union 
law on data prot ection, while for non-personal data appropr iate safeguards are put in place in accordance with 
Union law, such as Regulations (EU) 2022/868 (42) and (EU) 2023/2854 (43) of the European Parliame nt and of the 
Council.
(142) To ensure that AI leads to socially and environmentally beneficial outcomes, Member States are encouraged to 
suppor t and promote research and development of AI solutions in suppor t of socially and envir onmentally 
beneficial outcomes, such as AI-based solutions to increase accessibility for persons with disabilities, tackle 
socio-economic inequalities, or meet environmental targets, by allocating sufficient resources, including public and 
Union funding, and, where appropr iate and provided that the eligibility and selection criteria are fulfilled, 
consider ing in particular projects which pursue such objectives. Such projects should be based on the principle of 
interdisciplinar y cooperation between AI developers, exper ts on inequality and non-discr imination, accessibility , 
consumer , envir onmental, and digital rights, as well as academics.
(143) In order to promot e and protect innovation, it is impor tant that the intere sts of SMEs, including start-ups, that are 
providers or deplo yers of AI systems are take n into particular account. To that end, Member States should develop 
initiatives, which are targe ted at those operat ors, including on awareness raising and information communication. 
Member States should provide SMEs, including start-ups, that have a regist ered office or a branch in the Union, with 
priority access to the AI regulator y sandbo xes provided that they fulfil the eligibility conditions and selection criteria 
and without precluding other providers and prospective providers to access the sandbo xes provided the same 
conditions and criteria are fulfilled. Member States should utilise existing channels and where appropr iate, establish 
new dedicated channels for communication with SMEs, including start-ups, deplo yers, other innovat ors and, as 
appropr iate, local public author ities, to suppor t SMEs throughout their development path by providing guidance 
and responding to quer ies about the imp lementation of this Regulation. Where appropr iate, these channels should 
work together to create synergies and ensure homog eneity in their guidance to SMEs, including start-ups, and 
deplo yers. Additionally , Member States should facilitate the participation of SMEs and other relevant stakeholders in 
the standardisation development processes. Moreover , the specific interests and needs of provid ers that are SMEs, EN OJ L, 12.7.2024
36/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(42) Regulation (EU) 2022/868 of the European Parliament and of the Council of 30 May 2022 on European data gover nance and 
amending Regulation (EU) 2018/1724 (Data Governanc e Act) (OJ L 152, 3.6.2022, p. 1).
(43) Regulation (EU) 2023/2854 of the European Parliament and of the Council of 13 December 2023 on harmonised rules on fair 
access to and use of data and amending Regulation (EU) 2017/2394 and Directive (EU) 2020/1828 (Data Act) (OJ L, 2023/2854, 
22.12.2023, ELI: http://data.europa.eu/eli/reg/2023/2854/oj).including start-ups, should be take n into account when notified bodies set conf ormity assessment fees. The 
Commission should regularly assess the certification and compliance costs for SMEs, including start-ups, through 
transparent consultations and should work with Member States to lower such costs. For exam ple, translation costs 
relate d to mandat ory documentation and communication with author ities may constitute a signif icant cost for 
providers and other operat ors, in particular those of a smaller scale. Member States should possibly ensure that one 
of the languag es determin ed and accep ted by them for relevant provider s’ documentation and for communication 
with operat ors is one which is broadly understood by the largest possible number of cross-border deplo yers. In order 
to address the specific needs of SMEs, including start-ups, the Commission should provid e standardised templat es for 
the areas covered by this Regulation, upon request of the Board. Additiona lly, the Commission should comp lement 
Member States’ efforts by providing a sing le information platf orm with easy-t o-use information with regard s to this 
Regulation for all provid ers and deplo yers, by organising appropr iate communication cam paigns to raise awareness 
about the obligations arising from this Regulation, and by evaluating and promoting the conve rgence of best 
practices in public procurement procedures in relation to AI systems. Medium-sized enterprises which until recently 
qualified as small enterprises within the meaning of the Annex to Commission Recommendation 2003/361/EC (44) 
should have access to those suppor t measures, as those new medium-sized enter prises may sometimes lack the legal 
resources and training necessar y to ensure proper understanding of, and comp liance with, this Regulation.
(144) In order to promot e and prot ect innovation, the AI-on-demand platf orm, all relevant Union funding programmes 
and projects, such as Digital Europe Programme, Hor izon Europe, implemented by the Commission and the Member 
States at Union or national level should, as appropr iate, contr ibute to the achievement of the objectives of this 
Regulation.
(145) In order to minimise the risks to imp lementation resulting from lack of knowledg e and exper tise in the marke t as 
well as to facilitat e comp liance of provid ers, in particular SMEs, including start-ups, and notif ied bodies with their 
oblig ations under this Regulation, the AI-on-demand platf orm, the European Digital Inno vation Hubs and the 
testing and exper imentation facilities established by the Commission and the Member States at Union or national 
level should contr ibute to the implementation of this Regulation. Within their respective mission and fields of 
com petence, the AI-on-demand platf orm, the European Digital Inno vation Hubs and the testing and 
exper imentation Facilities are able to provide in particular technical and scientific suppor t to provider s and 
notified bodies.
(146) Moreo ver, in light of the very small size of some operat ors and in order to ensure propor tionality regard ing costs of 
inno vation, it is appropr iate to allow microenterp rises to fulfil one of the most costly oblig ations, namely to 
establish a quality management syste m, in a simplified manner which would reduce the administrative burden and 
the costs for those enterprises without affecting the level of prot ection and the need for comp liance with the 
requirements for high-r isk AI systems. The Commission should develop guidelines to specify the elements of the 
quality managem ent syste m to be fulfilled in this simplified manner by microent erprises.
(147) It is appropr iate that the Commission facilitates, to the extent possible, access to testing and exper imentation 
facilities to bodies, groups or laborat ories established or accredit ed pursuant to any relevant Union harmonisation 
legislation and which fulfil tasks in the context of conf ormity assessment of products or devices covered by that 
Union harmonisation legislation. This is, in particular , the case as rega rds exper t panels, exper t laborator ies and 
reference laborator ies in the field of medical devices pursuant to Regulations (EU) 2017/745 and (EU) 2017/746.
(148) This Regulation should establish a gover nance framew ork that both allows to coordinate and suppor t the 
application of this Regulation at national level, as well as build capabilities at Union level and integrat e stakeholders 
in the field of AI. The effective implementation and enforcement of this Regulation require a gove rnance framework 
that allows to coordinat e and build up central exper tise at Union level. The AI Office was established by Commission 
Decision (45) and has as its mission to develop Union exper tise and capabilities in the field of AI and to contr ibute to 
the imp lementation of Union law on AI. Member States should facilitate the tasks of the AI Offi ce with a view to 
suppor t the development of Union exper tise and capabilities at Union level and to strengthen the functioning of the 
digital sing le market. Further more, a Board composed of representatives of the Member States, a scientific panel to 
integrate the scientific community and an advisor y forum to contr ibute stakeh older input to the imp lementation of 
this Regulation, at Union and national level, should be established. The development of Union exper tise and OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 37/144(44) Commission Recommendation of 6 May 2003 concer ning the definition of micro, small and medium-sized enterprises (OJ L 124, 
20.5.2003, p. 36).
(45) Commission Decision of 24.1.2024 establishing the European Artificial Intellig ence Office C(2024) 390.capabilities should also include making use of existing resources and exper tise, in particular through synergies with 
structures built up in the cont ext of the Union level enforcement of other law and synergies with related initiatives at 
Union level, such as the EuroHPC Joint Und ertaking and the AI testing and exper imentation facilities under the 
Digital Europe Programme.
(149) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a Board should be 
established. The Board should reflect the various interests of the AI eco-system and be comp osed of representatives 
of the Member States. The Board should be responsible for a number of advisor y tasks , including issuing opinions, 
recommendations, advice or contr ibuting to guidance on matt ers related to the imp lementation of this Regulation, 
including on enforcement matt ers, technical specif ications or existing standards regarding the requirements 
established in this Regulation and providing advice to the Commission and the Member States and their national 
com petent author ities on specific questions related to AI. In order to give some flexibility to Member States in the 
designation of their representatives in the Board, such representatives may be any persons belonging to public 
entities who should have the relevant compet ences and powers to facilitat e coordination at national level and 
contr ibut e to the achi evement of the Board’s tasks. The Board should establish two standing sub-groups to provide 
a platf orm for cooperation and exchang e among market surveillance author ities and notifying author ities on issues 
relate d, respectively , to marke t surveillance and notified bodies. The standing subgroup for marke t surveillance 
should act as the administrative cooperation group (ADCO) for this Regulation within the meaning of Article 30 of 
Regulation (EU) 2019/1020. In accordance with Article 33 of that Regulation, the Commission should suppor t the 
activities of the standing subgroup for marke t surveillance by under taking marke t evaluations or studies, in 
particular with a view to identifying aspects of this Regulation requir ing specific and urgent coordination among 
mark et surveillance author ities. The Board may establish other standing or temporar y sub-groups as appropr iate for 
the purpose of examining specific issues. The Board should also cooperate , as appropr iate, with relevant Union 
bodies, exper ts groups and networks active in the context of relevant Union law, including in particular those active 
under relevant Union law on data, digital products and services.
(150) With a view to ensur ing the involvement of stakeholders in the imp lementation and application of this Regulation, 
an advisor y forum should be established to advise and provide technical exper tise to the Board and the Commission. 
To ensure a varied and balanced stak eholder representation between commercial and non-commercial interest and, 
within the category of commercial interests, with rega rds to SMEs and other under takings, the advisor y forum 
should compr ise inter alia industr y, start-ups, SMEs, academia, civil society , including the social partners, as well as 
the Fundamental Rights Agency , ENISA, the European Committe e for Standardization (CEN), the European 
Committ ee for Electrotec hnical Standardization (CENELEC) and the European Telecommunications Standards 
Institut e (ETSI).
(151) To suppor t the imp lementation and enforcement of this Regulation, in particular the monitoring activities of the AI 
Office as regards general-pur pose AI models, a scientific panel of independent exper ts should be established. The 
independent exper ts constituting the scientific panel should be selected on the basis of up-to-da te scientific or 
technical exper tise in the field of AI and should perf orm their tasks with imp artiality , objectivity and ensure the 
confidentiality of information and data obtained in carrying out their tasks and activities. To allow the reinf orcement 
of national capacities necessar y for the effective enforcement of this Regulation, Member States should be able to 
request suppor t from the pool of exper ts constituting the scientific panel for their enforcement activities.
(152) In order to suppor t adequate enforcement as regard s AI systems and reinf orce the capacities of the Member States, 
Union AI testing suppor t structures should be established and made available to the Member States.
(153) Member States hold a key role in the application and enforcement of this Regulation. In that respect, each Member 
State should designate at least one notifying author ity and at least one marke t surveillance author ity as national 
com petent author ities for the purpose of super vising the application and implementation of this Regulation. 
Member States may decide to appoint any kind of public entity to perform the tasks of the national compet ent 
author ities within the meaning of this Regulation, in accordance with their specific national organisational 
characteristics and needs. In order to increase organisation efficiency on the side of Member States and to set a sing le 
point of contact vis-à-vis the public and other counte rparts at Member State and Union levels, each Member State 
should designate a marke t surveillance author ity to act as a sing le point of contact.EN OJ L, 12.7.2024
38/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(154) The national compet ent author ities should exercise their powers independently , imp artially and without bias, so as 
to safeguard the principles of objectivity of their activities and tasks and to ensure the application and 
implementation of this Regulation. The members of these author ities should refrain from any action incom patible 
with their duties and should be subject to conf identiality rules under this Regulation.
(155) In order to ensure that providers of high-r isk AI systems can take into account the exper ience on the use of high-r isk 
AI systems for impro ving their syste ms and the design and development process or can take any possible corrective 
action in a timely manner , all providers should have a post-mark et monitoring syste m in place. Where relevant, 
post-mark et monitoring should include an analysis of the interaction with other AI systems including other devices 
and software. Post-market monito ring should not cover sensitive operational data of deplo yers which are law 
enforcement author ities. This system is also key to ensure that the possible risks emerging from AI systems which 
continue to ‘lear n’ after being placed on the marke t or put into service can be more efficiently and timely addressed. 
In this cont ext, providers should also be required to have a syste m in place to repor t to the relevant author ities any 
serious incidents resulting from the use of their AI systems, meaning incident or malfunctioning leading to death or 
serious damage to health, serious and irreversible disruption of the managem ent and operation of critical 
infrastr ucture, infringements of obliga tions under Union law intended to prot ect fundamental rights or serious 
damage to proper ty or the environment.
(156) In order to ensure an appropr iate and effective enforcement of the requirements and obligations set out by this 
Regulation, which is Union harmonisation legislation, the system of marke t surveillance and comp liance of products 
established by Regulation (EU) 2019/1020 should apply in its entirety . Marke t surveillance author ities designated 
pursuant to this Regulation should have all enforcement powers laid down in this Regulation and in Regulation (EU) 
2019/1020 and should exercise their powers and carry out their duties independently , impar tially and without bias. 
Although the majorit y of AI syste ms are not subject to specific requirements and obligations under this Regulation, 
mark et surveillance author ities may take measures in relation to all AI syste ms when they present a risk in 
accordance with this Regulation. Due to the specif ic nature of Union institutions, agencies and bodies falling within 
the scope of this Regulation, it is appropr iate to designate the European Data Protect ion Super visor as a compet ent 
mark et surveillance author ity for them. This should be without prejudice to the designation of national compet ent 
author ities by the Member States. Market surveillance activities should not affect the ability of the super vised entities 
to carry out their tasks independently , when such independence is required by Union law.
(157) This Regulation is without prejudice to the compet ences, tasks , powers and independence of relevant national public 
author ities or bodies which super vise the application of Union law protect ing fundamental rights, including equality 
bodies and data prot ection author ities. Where necessar y for their mandat e, those national public author ities or 
bodies should also have access to any documentation create d under this Regulation. A specific safeguard procedure 
should be set for ensur ing adequat e and timely enforcement against AI systems presenting a risk to health, safety and 
fundamental rights. The procedure for such AI syste ms presenting a risk should be applied to high-r isk AI syste ms 
presenting a risk, prohibited systems which have been placed on the market, put into service or used in violation of 
the prohibited practices laid down in this Regulation and AI systems which have been made available in violation of 
the transparency requirements laid down in this Regulation and present a risk.
(158) Union financ ial services law includes internal govern ance and risk-managem ent rules and requirements which are 
applicable to regulate d financial institutions in the course of provision of those services, including when they make 
use of AI syste ms. In order to ensure coherent application and enforcement of the oblig ations under this Regulation 
and relevant rules and requirements of the Union financial services legal acts, the compet ent author ities for the 
super vision and enforcement of those legal acts, in particular compet ent author ities as defined in Regulation (EU) 
No 575/2013 of the European Parliament and of the Council (46) and Directives 2008/48/EC (47), 2009/138/EC (48), OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 39/144(46) Regulation (EU) No 575/2013 of the European Parliament and of the Council of 26 June 2013 on prudential requirements for credit 
institutions and investment firms and amending Regulation (EU) No 648/2012 (OJ L 176, 27.6.2013, p. 1).
(47) Directive 2008/48/EC of the European Parliament and of the Council of 23 Apr il 2008 on credit agreements for consumers and 
repealing Council Directive 87/102/EEC (OJ L 133, 22.5.2008, p. 66).
(48) Directive 2009/138/EC of the European Parliament and of the Council of 25 November 2009 on the taking-up and pursuit of the 
business of Insurance and Reinsurance (Solvency II) (OJ L 335, 17.12.2009, p. 1).2013/36/EU (49), 2014/17/EU (50) and (EU) 2016/97 (51) of the European Parliament and of the Council, should be 
designate d, within their respective comp etences, as compet ent author ities for the purpose of super vising the 
implementation of this Regulation, including for marke t surveillance activities, as regard s AI syste ms provid ed or 
used by regulated and super vised financial institutions unless Member States decide to designate another author ity to 
fulfil these marke t surveillance tasks . Those comp etent author ities should have all powers under this Regulation and 
Regulation (EU) 2019/1020 to enforce the requirements and obligations of this Regulation, including powers to 
carry our ex post marke t surveillance activities that can be integrat ed, as appropr iate, into their existing super visor y 
mec hanisms and procedures under the relevant Union financial services law. It is appropr iate to envisag e that, when 
acting as market surveillance author ities under this Regulation, the national author ities responsible for the 
super vision of credit institutions regulate d under Directive 2013/36/EU, which are participating in the Sing le 
Super visor y Mec hanism established by Council Regulation (EU) No 1024/2013 (52), should repor t, without dela y, to 
the European Central Bank any information identified in the course of their mark et surveillance activities that may 
be of potent ial interest for the European Central Bank’s prudential super visor y tasks as specified in that Regulation. 
To further enhance the consistency between this Regulation and the rules applicable to credit institutions regulated 
under Directive 2013/36/EU, it is also appropr iate to integrate some of the provid ers’ procedural obligations in 
relation to risk managem ent, post marke ting monitorin g and documentation into the existing obligations and 
procedures under Directive 2013/36/EU. In order to avoid overlaps, limit ed deroga tions should also be envisaged in 
relation to the quality management system of provid ers and the monitori ng obliga tion placed on deplo yers of 
high-r isk AI syste ms to the exte nt that these apply to credit institutions regulated by Directive 2013/36/EU. The 
same regime should apply to insurance and re-insurance under takings and insurance holding companies under 
Directive 2009/138/EC and the insurance inter mediar ies under Directive (EU) 2016/97 and other types of financial 
institutions subject to requirements regarding inter nal gover nance, arrang ements or processes established pursuant 
to the relevant Union financial services law to ensure consistency and equal treatment in the financ ial sector.
(159) Each market surveillance author ity for high-r isk AI syste ms in the area of biometr ics, as listed in an annex to this 
Regulation insofa r as those syste ms are used for the purposes of law enforcement, migration, asylum and border 
control management, or the administration of justice and democratic processes, should have effective invest igative 
and corrective powers, including at least the power to obtain access to all personal data that are being processed and 
to all information necessar y for the perf ormance of its tasks . The market surveillance author ities should be able to 
exercise their powers by acting with complet e independence. Any limitations of their access to sensitive operational 
data under this Regulation should be without prejudice to the powers confe rred to them by Directive 
(EU) 2016/680. No exclusion on disclosing data to national data prot ection author ities under this Regulation should 
affect the current or future powers of those author ities beyond the scope of this Regulation.
(160) The mark et surveillance author ities and the Commission should be able to propose joint activities, including joint 
investig ations, to be conducted by marke t surveillance author ities or market surveillance author ities jointly with the 
Commission, that have the aim of promoting comp liance, identifying non-compliance, raising awareness and 
providing guidance in relation to this Regulation with respect to specific categor ies of high-r isk AI syste ms that are 
found to present a serious risk across two or more Member States. Joint activities to promote compliance should be 
carried out in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Office should provid e coordination 
suppor t for joint investig ations.
(161) It is necessar y to clarify the responsibilities and comp etences at Union and national level as regard s AI syste ms that 
are built on general-pur pose AI models. To avoid overlapping comp etences, where an AI syste m is based on 
a general-pur pose AI model and the model and system are provid ed by the same provider , the super vision should EN OJ L, 12.7.2024
40/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(49) Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the activity of credit institutions 
and the prudential super vision of credit institutions and investment firms, amending Directive 2002/87/EC and repealing 
Directives 2006/48/EC and 2006/49/EC (OJ L 176, 27.6.2013, p. 338).
(50) Directive 2014/17/EU of the European Parliament and of the Council of 4 Febr uary 2014 on credit agreements for consumers 
relating to residential immov able proper ty and amending Directives 2008/48/EC and 2013/36/EU and Regulation (EU) 
No 1093/2010 (OJ L 60, 28.2.2014, p. 34).
(51) Directive (EU) 2016/97 of the European Parliament and of the Council of 20 Januar y 2016 on insurance distr ibution (OJ L 26, 
2.2.2016, p. 19).
(52) Council Regulation (EU) No 1024/2013 of 15 October 2013 conf erring specific tasks on the European Central Bank concer ning 
policies relating to the prudential super vision of credit institutions (OJ L 287, 29.10.2013, p. 63).take place at Union level through the AI Office, which should have the powers of a market surveillance author ity 
within the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillance 
author ities remain responsible for the super vision of AI syste ms. How ever , for general-pur pose AI systems that can 
be used directly by deplo yers for at least one purpose that is classified as high-r isk, marke t surveillance author ities 
should cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other marke t 
surveillance author ities according ly. Further more, market surveillance author ities should be able to request 
assistance from the AI Office where the marke t surveillance author ity is unable to conclude an invest igation on 
a high-r isk AI syste m because of its inability to access certain information related to the general-pur pose AI model 
on which the high-r isk AI syste m is built. In such cases, the procedure regarding mutual assistance in cross-border 
cases in Chapt er VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.
(162) To mak e best use of the centralised Union exper tise and synergies at Union level, the powers of super vision and 
enforcement of the obliga tions on provid ers of general-pur pose AI models should be a comp etence of the 
Commission. The AI Office should be able to carry out all necessar y actions to monitor the effective implementation 
of this Regulation as rega rds general-pur pose AI models. It should be able to investigat e possible infringements of 
the rules on providers of general-pur pose AI models both on its own initiative, following the results of its 
monito ring activities, or upon request from market surveillance author ities in line with the conditions set out in this 
Regulation. To suppor t effective monitoring of the AI Offi ce, it should provide for the possibility that downstream 
providers lodg e complaints about possible infringements of the rules on provid ers of general-pur pose AI models and 
syste ms.
(163) With a view to comp lementing the governance systems for general-pur pose AI models, the scientific panel should 
suppor t the monitoring activities of the AI Offi ce and may, in certain cases, provide qualified alerts to the AI Office 
which trigger follow-up s, such as invest igations. This should be the case where the scientific panel has reason to 
suspect that a general-purpo se AI model poses a concrete and identifi able risk at Union level. Further more, this 
should be the case where the scientific panel has reason to suspect that a general-pur pose AI model meets the criteria 
that would lead to a classif ication as general-pur pose AI model with systemic risk. To equip the scientific panel with 
the information necessar y for the perf ormance of those tasks , there should be a mec hanism whereby the scientific 
panel can request the Commission to require documentation or information from a provid er.
(164) The AI Offi ce should be able to take the necessar y actions to monitor the effective imp lementation of and 
com pliance with the obliga tions for provid ers of general-pur pose AI models laid down in this Regulation. The AI 
Office should be able to invest igate possible infringements in accordance with the powers provided for in this 
Regulation, including by requesting documentation and information, by conducting evaluations, as well as by 
requesting measures from provid ers of general-pur pose AI models. When conducting evaluations, in order to make 
use of independent exper tise, the AI Office should be able to involve independent exper ts to carry out the 
evaluations on its behalf. Compliance with the obliga tions should be enforceable, inter alia, through requests to take 
appropr iate measures, including risk mitiga tion measures in the case of identifie d syste mic risks as well as restr icting 
the making availa ble on the market, withdra wing or recalling the model. As a safeguard, where needed beyond the 
procedural rights provided for in this Regulation, provid ers of general-pur pose AI models should have the 
procedural rights provid ed for in Article 18 of Regulation (EU) 2019/1020, which should apply mutatis mutandis, 
without prejudice to more specif ic procedural rights provid ed for by this Regulation.
(165) The development of AI syste ms other than high-r isk AI systems in accordance with the requirements of this 
Regulation may lead to a larger uptak e of ethical and trustw orthy AI in the Union. Providers of AI syste ms that are 
not high-r isk should be encouraged to create codes of conduct, including related gover nance mechanisms, intended 
to foste r the voluntar y application of some or all of the mandator y requirements applicable to high-r isk AI systems, 
adap ted in light of the intended purpose of the systems and the lower risk involved and taking into account the 
available technical solutions and industr y best practices such as model and data cards. Provi ders and, as appropr iate, 
deplo yers of all AI systems, high-r isk or not, and AI models should also be encourage d to apply on a voluntar y basis 
additional requirements relate d, for exam ple, to the elements of the Union’s Ethics Guidelines for Trustw orthy AI, OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 41/144environmental sustainability , AI literac y measures, inclusive and diverse design and development of AI systems, 
including attention to vulnerable persons and accessibility to persons with disability , stakeholders’ participation with 
the involvement, as appropr iate, of relevant stak eholders such as business and civil society organisations, academia, 
researc h organisations, trade unions and consumer prot ection organisations in the design and development of AI 
syste ms, and diversity of the development teams, including gender balance. To ensure that the voluntar y codes of 
conduct are effective, they should be based on clear objectives and key perf ormance indicators to measure the 
achievement of those objectives. They should also be developed in an inclusive way, as appropr iate, with the 
involvement of relevant stakeholders such as business and civil society organisations, academia, research 
organisations, trade unions and consumer prot ection organisation. The Commission may develop initiatives, 
including of a sectoral nature, to facilitate the lowering of technical barriers hinder ing cross-border exchang e of data 
for AI development, including on data access infrastr ucture, semantic and technical interoperability of different types 
of data.
(166) It is important that AI systems relate d to products that are not high-r isk in accordance with this Regulation and thus 
are not required to comp ly with the requirements set out for high-r isk AI systems are never theless safe when placed 
on the marke t or put into service. To contr ibut e to this objective, Regulation (EU) 2023/988 of the European 
Parliament and of the Council (53) would apply as a safety net.
(167) In order to ensure trustful and constr uctive cooperation of comp etent author ities on Union and national level, all 
parties involved in the application of this Regulation should respect the conf identiality of information and data 
obtained in carrying out their tasks , in accordance with Union or national law. They should carry out their tasks and 
activities in such a manner as to prot ect, in particular , intellectual proper ty rights, confi dential business information 
and trade secrets, the effective implementation of this Regulation, public and national secur ity interests, the integr ity 
of criminal and administrative proceedings, and the integr ity of classif ied information.
(168) Comp liance with this Regulation should be enforceable by means of the imp osition of penalties and other 
enforcement measures. Member States should take all necessar y measures to ensure that the provisions of this 
Regulation are implement ed, including by laying down effective, propor tionate and dissuasive penalties for their 
infringement, and to respect the ne bis in idem principle. In order to strengthen and harmonise administrative 
penalties for infringem ent of this Regulation, the upper limits for setting the administrative fines for certain specific 
infringements should be laid down. When assessing the amount of the fines, Member States should, in each 
individual case, take into account all relevant circumstances of the specific situation, with due regard in particular to 
the nature, gravity and duration of the infringement and of its consequences and to the size of the provid er, in 
particular if the provider is an SME, including a start-up. The European Data Protection Super visor should have the 
power to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation.
(169) Comp liance with the obligations on providers of general-pur pose AI models imposed under this Regulation should 
be enforceable, inter alia, by means of fines. To that end, appropr iate levels of fines should also be laid down for 
infringement of those obligati ons, including the failure to comply with measures request ed by the Commission in 
accordance with this Regulation, subject to appropr iate limitation periods in accordance with the principle of 
propor tionality . All decisions taken by the Commission under this Regulation are subject to review by the Cour t of 
Justice of the European Union in accordance with the TFEU, including the unlimit ed jurisdiction of the Cour t of 
Justice with rega rd to penalties pursuant to Article 261 TFEU.
(170) Union and national law already provide effective remedies to natural and legal persons whose rights and freedoms 
are adversely affected by the use of AI syste ms. Without prejudice to those remedies, any natural or lega l person that 
has grounds to consider that there has been an infringem ent of this Regulation should be entitled to lodg e 
a complaint to the relevant mark et surveillance author ity.
(171) Affecte d persons should have the right to obtain an explanation where a deplo yer’s decision is based mainly upon 
the output from certain high-r isk AI syste ms that fall within the scope of this Regulation and where that decision 
produces legal effects or similarly significantly affects those persons in a way that they consider to have an adverse EN OJ L, 12.7.2024
42/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(53) Regulation (EU) 2023/988 of the European Parliament and of the Council of 10 May 2023 on general product safety, amending 
Regulation (EU) No 1025/2012 of the European Parliament and of the Council and Directive (EU) 2020/1828 of the European 
Parliament and the Council, and repealing Directive 2001/95/EC of the European Parliament and of the Council and Council 
Directive 87/357/EEC (OJ L 135, 23.5.2023, p. 1).impact on their health, safety or fundamental rights. That explanation should be clear and meaningful and should 
provide a basis on which the affected persons are able to exercise their rights. The right to obtain an explanation 
should not apply to the use of AI syste ms for which exceptions or restr ictions follow from Union or national law 
and should apply only to the extent this right is not already provid ed for under Union law.
(172) Persons acting as whistleblo wers on the infringements of this Regulation should be prot ected under the Union law. 
Directive (EU) 2019/1937 of the European Parliament and of the Council (54) should theref ore apply to the repor ting 
of infringements of this Regulation and the protect ion of persons repor ting such infringements.
(173) In order to ensure that the regulato ry framework can be adap ted where necessar y, the power to adopt acts in 
accordance with Article 290 TFEU should be deleg ated to the Commission to amend the conditions under which an 
AI system is not to be considered to be high-r isk, the list of high-r isk AI syste ms, the provisions regarding technical 
documentation, the cont ent of the EU declaration of conf ormity the provisions regard ing the conf ormity assessment 
procedures, the provisions establishing the high-r isk AI syste ms to which the conf ormity assessment procedure 
based on assessment of the quality management syste m and assessment of the technical documentation should 
apply , the threshold, bench mark s and indicators, including by supplementing those bench mark s and indicator s, in 
the rules for the classification of general-pur pose AI models with syste mic risk, the criteria for the designation of 
general-pur pose AI models with syste mic risk, the technical documentation for providers of general-pur pose AI 
models and the transparency information for providers of general-pur pose AI models. It is of particular imp ortance 
that the Commission carry out appropr iate consultations during its preparatory work, including at exper t level, and 
that those consultations be conducte d in accordance with the principles laid down in the Inter institutional 
Agreement of 13 Apr il 2016 on Better Law -Making (55). In particular , to ensure equal participation in the 
preparation of delegat ed acts, the European Parliament and the Council receive all documents at the same time as 
Member States’ exper ts, and their exper ts syste matically have access to meetings of Commission exper t groups 
dealing with the preparation of delegat ed acts.
(174) Given the rapid technological developments and the technical exper tise required to effectively apply this Regulation, 
the Commission should evaluate and review this Regulation by 2 Augu st 2029 and ever y four years thereaf ter and 
repor t to the European Parliament and the Council. In addition, taking into account the implications for the scope of 
this Regulation, the Commission should carry out an assessment of the need to amend the list of high-r isk AI 
syste ms and the list of prohibited practices once a year. Moreove r, by 2 August 2028 and ever y four years thereaf ter, 
the Commission should evaluate and repor t to the European Parliament and to the Council on the need to amend 
the list of high-r isk areas headings in the annex to this Regulation, the AI systems within the scope of the 
transparency obligations, the effectiveness of the super vision and gover nance syste m and the progress on the 
development of standardisation deliverables on energy efficient development of general-pur pose AI models, 
including the need for further measures or actions. Finally , by 2 August 2028 and ever y three years thereaf ter, the 
Commission should evaluate the impact and effectiveness of voluntar y codes of conduct to foste r the application of 
the requirements provided for high-r isk AI systems in the case of AI systems other than high-r isk AI syste ms and 
possibly other additional requirements for such AI syste ms.
(175) In order to ensure unif orm conditions for the imp lementation of this Regulation, impl ementing powers should be 
conf erred on the Commission. Those powers should be exercised in accordance with Regulation (EU) No 182/2011 
of the European Parliament and of the Council (56).
(176) Since the objective of this Regulation, namely to imp rove the functioning of the internal mark et and to promot e the 
uptake of human centr ic and trustw orthy AI, while ensur ing a high level of protection of health, safety, fundamental 
rights enshr ined in the Char ter, including democracy , the rule of law and environmental prot ection against harmful 
effects of AI syste ms in the Union and suppor ting inno vation, cannot be sufficiently achieved by the Member States 
and can rather , by reason of the scale or effects of the action, be better achieved at Union level, the Union may adop t OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 43/144(54) Directive (EU) 2019/1937 of the European Parliament and of the Council of 23 October 2019 on the protection of persons who 
repor t breaches of Union law (OJ L 305, 26.11.2019, p. 17).
(55) OJ L 123, 12.5.2016, p. 1.
(56) Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 Febr uary 2011 laying down the rules and 
general principles concer ning mechanisms for control by Member States of the Commission’s exerci se of implementing powers (OJ 
L 55, 28.2.2011, p. 13).measures in accordance with the principle of subsidiar ity as set out in Article 5 TEU. In accordance with the 
principle of propor tionality as set out in that Article, this Regulation does not go beyond what is necessar y in order 
to achieve that objective.
(177) In order to ensure lega l certainty , ensure an appropr iate adaptation period for operat ors and avoid disruption to the 
mark et, including by ensur ing continuity of the use of AI syste ms, it is appropr iate that this Regulation applies to the 
high-r isk AI syste ms that have been placed on the marke t or put into service before the general date of application 
thereof, only if, from that date, those systems are subject to signifi cant chang es in their design or intende d purpose. 
It is appropr iate to clarify that, in this respect, the concept of significant chang e should be understood as equivalent 
in substance to the notion of substantial modif ication, which is used with regard only to high-r isk AI syste ms 
pursuant to this Regulation. On an exceptiona l basis and in light of public accountability , operat ors of AI syste ms 
which are comp onents of the large-scale IT systems established by the lega l acts listed in an annex to this Regulation 
and operators of high-r isk AI syste ms that are intended to be used by public author ities should, respectively , take the 
necessar y steps to comp ly with the requirements of this Regulation by end of 2030 and by 2 August 2030.
(178) Provi ders of high-r isk AI systems are encourage d to start to comply , on a voluntar y basis, with the relevant 
oblig ations of this Regulation already during the transitional period.
(179) This Regulation should apply from 2 August 2026. How ever , taking into account the unaccepta ble risk associated 
with the use of AI in certain ways, the prohibitions as well as the general provis ions of this Regulation should already 
apply from 2 Febr uary 2025. While the full effect of those prohibitions follows with the establishment of the 
gove rnance and enforcement of this Regulation, anticipating the application of the prohibitions is imp ortant to take 
account of unaccepta ble risks and to have an effect on other procedures, such as in civil law. Moreove r, the 
infrastr ucture related to the gove rnance and the conf ormity assessment syste m should be operational before 
2 August 2026, theref ore the provisions on notified bodies and governance structure should apply from 2 August 
2025. Given the rapid pace of technological advancements and adop tion of general-pur pose AI models, obligations 
for provid ers of general-pur pose AI models should apply from 2 August 2025. Codes of practice should be ready by 
2 May 2025 in view of enabling provid ers to demonstrate com pliance on time. The AI Office should ensure that 
classification rules and procedures are up to date in light of technologi cal developments. In addition, Member States 
should lay down and notify to the Commission the rules on penalties, including administrative fines, and ensure that 
they are properly and effectively implemented by the date of application of this Regulation. Theref ore the provis ions 
on penalties should apply from 2 August 2025.
(180) The European Data Protection Super visor and the European Data Prot ection Board were consult ed in accordance 
with Article 42(1) and (2) of Regulation (EU) 2018/1725 and delivered their joint opinion on 18 June 2021,
HAVE ADOPTED THIS REGUL ATION:
CHAPTER I
GENERAL PROVISIONS
Article 1
Subject matter`
1. The purpose of this Regulation is to impro ve the functioning of the inter nal marke t and promote the uptake of 
human-centr ic and trustw orthy artificial intellig ence (AI), while ensur ing a high level of prot ection of health, safety, 
fundamental rights enshr ined in the Char ter, including democracy , the rule of law and environmental protection, against 
the harmful effects of AI systems in the Union and suppor ting inno vation.
2. This Regulation lays down:
(a)harmonised rules for the placing on the marke t, the putting into service, and the use of AI systems in the Union;EN OJ L, 12.7.2024
44/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b)prohibitions of certain AI practices;
(c)specific requirements for high-r isk AI systems and oblig ations for operators of such systems;
(d)harmonised transparency rules for certain AI systems;
(e)harmonised rules for the placing on the marke t of general-pur pose AI models;
(f)rules on mark et monitoring, marke t surveillance, gove rnance and enforcement ;
(g)measures to suppor t innovation, with a particular focus on SMEs, including start-ups.
Article 2
Scope
1. This Regulation applies to:
(a)provid ers placing on the marke t or putting into service AI syste ms or placing on the market general-pur pose AI models 
in the Union, irrespective of whether those providers are established or locat ed within the Union or in a third countr y;
(b)deplo yers of AI syste ms that have their place of establishment or are located within the Union;
(c)provid ers and deplo yers of AI syste ms that have their place of establishment or are locate d in a third countr y, where the 
output produced by the AI system is used in the Union;
(d)imp orters and distr ibut ors of AI systems;
(e)product manufacturers placing on the marke t or putting into service an AI system together with their product and 
under their own name or trademark;
(f)author ised representatives of provider s, which are not established in the Union;
(g)affected persons that are located in the Union.
2. For AI syste ms classif ied as high-r isk AI systems in accordance with Article 6(1) related to products covered by the 
Union harmonisation legislation listed in Section B of Annex I, only Article 6(1), Articles 102 to 109 and Article 112 apply . 
Article 57 applies only in so far as the requirements for high-r isk AI systems under this Regulation have been integrat ed in 
that Union harmonisation legislation.
3. This Regulation does not apply to areas outside the scope of Union law, and shall not, in any event, affect the 
compet ences of the Member States concer ning national secur ity, regardless of the type of entity entr uste d by the Member 
States with carrying out tasks in relation to those comp etences.
This Regulation does not apply to AI syste ms where and in so far they are placed on the marke t, put into service, or used 
with or without modif ication exclusively for militar y, defe nce or national secur ity purposes, rega rdless of the type of entity 
carrying out those activities.
This Regulation does not apply to AI systems which are not placed on the market or put into service in the Union, where 
the output is used in the Union exclusively for militar y, defe nce or national secur ity purposes, regardless of the type of 
entity carrying out those activities.
4. This Regulation applies neither to public author ities in a third countr y nor to international organisations falling 
within the scope of this Regulation pursuant to paragraph 1, where those author ities or organisations use AI syste ms in the 
framew ork of international cooperation or agreements for law enforcement and judicial cooperation with the Union or 
with one or more Member States, provided that such a third countr y or intern ational organisation provides adequate 
safeguards with respect to the prot ection of fundamental rights and freedoms of individuals.
5. This Regulation shall not affect the application of the provisions on the liability of provider s of interm ediar y services 
as set out in Chapt er II of Regulation (EU) 2022/2065.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 45/1446. This Regulation does not apply to AI systems or AI models, including their output, specif ically developed and put into 
service for the sole purpose of scientifi c researc h and development.
7. Union law on the prot ection of personal data, privacy and the conf identiality of communications applies to personal 
data processed in connection with the rights and obliga tions laid down in this Regulation. This Regulation shall not affect 
Regulation (EU) 2016/679 or (EU) 2018/1725, or Directive 2002/58/EC or (EU) 2016/680, without prejudice to Article 
10(5) and Article 59 of this Regulation.
8. This Regulation does not apply to any researc h, testing or development activity regard ing AI syste ms or AI models 
prior to their being placed on the marke t or put into service. Such activities shall be conducted in accordance with 
applicable Union law. Testing in real world conditions shall not be covered by that exclusion.
9. This Regulation is without prejudice to the rules laid down by other Union legal acts related to consumer prot ection 
and product safety .
10. This Regulation does not apply to obligations of deplo yers who are natural persons using AI syste ms in the course of 
a purely personal non-professional activity .
11. This Regulation does not preclude the Union or Member States from maintaining or introducing laws, regulations or 
administrative provis ions which are more favourable to workers in terms of protect ing their rights in respect of the use of 
AI systems by employers, or from encouraging or allowi ng the application of collective agreements which are more 
favourable to workers.
12. This Regulation does not apply to AI systems released under free and open-source licences, unless they are placed on 
the mark et or put into service as high-r isk AI systems or as an AI system that falls under Article 5 or 50.
Article 3
Def initions
For the purposes of this Regulation, the following definitions apply:
(1) ‘AI syste m’ means a machine-based system that is designed to operat e with varying levels of autonom y and that may 
exhibit adap tiveness after deplo yment, and that, for explicit or implicit objectives, infers, from the input it receives, 
how to generate outputs such as predictions, content, recommendations, or decisions that can influence phys ical or 
virtual environments;
(2) ‘risk’ means the combination of the probability of an occur rence of harm and the sever ity of that harm;
(3) ‘provi der’ means a natural or lega l person, public author ity, agency or other body that develops an AI system or 
a general-pur pose AI model or that has an AI syste m or a general-pur pose AI model developed and places it on the 
mark et or puts the AI syste m into service under its own name or trademark, whether for payment or free of charg e;
(4) ‘deplo yer’ means a natural or lega l person, public author ity, agency or other body using an AI system under its 
author ity except where the AI system is used in the course of a personal non-professional activity ;
(5) ‘author ised representative ’ means a natural or legal person located or established in the Union who has received and 
accep ted a written mandate from a provider of an AI system or a general-pur pose AI model to, respectively , perf orm 
and carry out on its behalf the obliga tions and procedures established by this Regulation;
(6) ‘imp orter’ means a natural or lega l person locate d or established in the Union that places on the market an AI system 
that bears the name or trademark of a natural or lega l person established in a third countr y;
(7) ‘distr ibutor ’ means a natural or legal person in the supply chain, other than the provider or the importer, that mak es 
an AI syste m available on the Union marke t;
(8) ‘operat or’ means a provid er, product manufa cturer , deplo yer, author ised representative, imp orter or distr ibut or;EN OJ L, 12.7.2024
46/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(9) ‘placing on the marke t’ means the first making available of an AI system or a general-purpo se AI model on the Union 
mark et;
(10) ‘making available on the market’ means the supply of an AI syste m or a general-pur pose AI model for distr ibution or 
use on the Union market in the course of a commercial activity , whether in retur n for payment or free of charge;
(11) ‘putting into service ’ means the supply of an AI system for first use directly to the deplo yer or for own use in the Union 
for its intended purpose;
(12) ‘intended purpose’ means the use for which an AI syste m is intended by the provider , including the specif ic cont ext 
and conditions of use, as specif ied in the information supplied by the provid er in the instr uctions for use, promotional 
or sales materials and statements, as well as in the technical documentation;
(13) ‘reasonably foreseeable misuse’ means the use of an AI syste m in a way that is not in accordance with its intended 
purpose, but which may result from reasonably foreseeable human behavi our or interaction with other systems, 
including other AI systems;
(14) ‘safety component’ means a component of a product or of an AI system which fulfils a safety function for that product 
or AI system, or the failure or malfunctioning of which endang ers the health and safety of persons or proper ty;
(15) ‘instr uctions for use’ means the information provided by the provider to inform the deplo yer of, in particular , an AI 
syste m’s intended purpose and proper use;
(16) ‘recall of an AI syste m’ means any measure aiming to achieve the retur n to the provider or taking out of service or 
disabling the use of an AI system made available to deplo yers;
(17) ‘withdra wal of an AI system’ means any measure aiming to prevent an AI system in the supply chain being made 
available on the mark et;
(18) ‘perf ormance of an AI syste m’ means the ability of an AI syste m to achieve its intended purpose;
(19) ‘notifying author ity’ means the national author ity responsible for setting up and carrying out the necessar y procedures 
for the assessment, designation and notification of conf ormity assessment bodies and for their monito ring;
(20) ‘conf ormity assessment’ means the process of demonstrating whether the requirements set out in Chapt er III, Section 2 
relating to a high-r isk AI system have been fulfilled;
(21) ‘conf ormity assessment body’ means a body that perfo rms third-par ty conf ormity assessment activities, including 
testing, certification and inspection;
(22) ‘notified body’ means a conf ormity assessment body notif ied in accordance with this Regulation and other relevant 
Union harmonisation legislation;
(23) ‘substantial modifi cation’ means a change to an AI syste m after its placing on the marke t or putting into service which 
is not foreseen or planned in the initial conf ormity assessment carried out by the provid er and as a result of which the 
com pliance of the AI system with the requirements set out in Chap ter III, Section 2 is affected or results in 
a modif ication to the intended purpose for which the AI syste m has been assessed;
(24) ‘CE marking’ means a marking by which a provid er indicates that an AI system is in conf ormity with the requirements 
set out in Chapt er III, Section 2 and other applicable Union harmonisation legislation providing for its affixing;
(25) ‘post-mark et monitoring syste m’ means all activities carried out by provid ers of AI systems to collect and review 
exper ience gained from the use of AI systems they place on the marke t or put into service for the purpose of 
identifying any need to immediate ly apply any necessar y corrective or preventive actions;
(26) ‘marke t surveillance author ity’ means the national author ity carrying out the activities and taking the measures 
pursuant to Regulation (EU) 2019/1020;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 47/144(27) ‘harmonised standard’ means a harmonised standard as defined in Article 2(1), point (c), of Regulation (EU) 
No 1025/2012;
(28) ‘common specif ication’ means a set of technical specif ications as defined in Article 2, point (4) of Regulation (EU) 
No 1025/2012, providing means to compl y with certain requirements established under this Regulation;
(29) ‘training data’ means data used for training an AI system through fitting its learnable paramet ers;
(30) ‘validation data’ means data used for providing an evaluation of the trained AI system and for tuning its non-lear nable 
paramet ers and its learning process in order , inter alia, to prevent under fitting or overfitting;
(31) ‘validation data set’ means a separate data set or part of the training data set, either as a fixed or variable split ;
(32) ‘testing data’ means data used for providing an independent evaluation of the AI system in order to conf irm the 
expect ed perform ance of that syste m before its placing on the market or putting into service;
(33) ‘input data’ means data provided to or directly acquired by an AI syste m on the basis of which the syste m produces an 
output ;
(34) ‘biometr ic data’ means personal data resulting from specific technical processing relating to the physical , physiolo gical 
or behavi oural character istics of a natural person, such as facial images or dactyloscopic data;
(35) ‘biometr ic identifica tion’ means the automat ed recognition of physical, physiolo gical, behavioural, or psychological 
human features for the purpose of establishing the identity of a natural person by comp aring biometr ic data of that 
individual to biometr ic data of individuals stored in a database;
(36) ‘biometr ic verificati on’ means the automa ted, one-to-one verification, including authentication, of the identity of 
natural persons by compari ng their biometr ic data to previously provided biometr ic data;
(37) ‘special categori es of personal data’ means the categori es of personal data refer red to in Article 9(1) of Regulation (EU) 
2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725;
(38) ‘sensitive operational data’ means operational data related to activities of prevention, detection, investig ation or 
prosecution of criminal offences, the disclosure of which could jeopardise the integrity of criminal proceedings;
(39) ‘emotion recognition syste m’ means an AI syste m for the purpose of identifying or inferr ing emotions or intentions of 
natural persons on the basis of their biometr ic data;
(40) ‘biometr ic categor isation syste m’ means an AI system for the purpose of assigning natural persons to specif ic 
cate gories on the basis of their biometr ic data, unless it is ancillar y to another commercial service and strictly 
necessar y for objective technical reasons;
(41) ‘remot e biometr ic identification system’ means an AI system for the purpose of identifying natural persons, without 
their active involvement, typically at a distance through the comp arison of a person’s biometr ic data with the 
biometr ic data contained in a refere nce database;
(42) ‘real-time remote biometr ic identifica tion syste m’ means a remot e biometr ic identification system, whereby the 
capt uring of biometr ic data, the comp arison and the identification all occur without a signifi cant dela y, compr ising 
not only instant identifica tion, but also limited shor t dela ys in order to avoid circum vention;
(43) ‘post-remot e biometr ic identifica tion syste m’ means a remot e biometr ic identification system other than a real-time 
remot e biometr ic identification system;
(44) ‘publicly accessible space ’ means any publicly or privately owned physica l place accessible to an undet ermined number 
of natural persons, regardless of whether certain conditions for access may apply , and regard less of the pote ntial 
capacity restr ictions;EN OJ L, 12.7.2024
48/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(45) ‘law enforcement author ity’ means:
(a)any public author ity comp etent for the prevention, invest igation, detection or prosecution of criminal offences or 
the execution of criminal penalties, including the safeguarding against and the prevention of threats to public 
secur ity; or
(b)any other body or entity entr uste d by Member State law to exercise public author ity and public powers for the 
purposes of the prevention, investig ation, detect ion or prosecution of criminal offences or the execution of 
criminal penalties, including the safeguarding against and the prevention of threats to public secur ity;
(46) ‘law enforcement’ means activities carried out by law enforcement author ities or on their behalf for the prevention, 
investig ation, detection or prosecution of criminal offences or the execution of criminal penalties, including 
safeguarding against and preventing threats to public secur ity;
(47) ‘AI Office’ means the Commission’s function of contr ibuting to the implementation, monitorin g and super vision of AI 
syste ms and general-pur pose AI models, and AI gover nance, provid ed for in Commission Decision of 24 Januar y 
2024; references in this Regulation to the AI Office shall be constr ued as references to the Commission;
(48) ‘national comp etent author ity’ means a notifying author ity or a marke t surveillance author ity; as regard s AI syste ms 
put into service or used by Union institutions, agencies, offices and bodies, references to national compet ent 
author ities or market surveillance author ities in this Regulation shall be constr ued as references to the European Data 
Prot ection Super visor ;
(49) ‘serious incident ’ means an incident or malfunctioning of an AI syste m that directly or indirectly leads to any of the 
following:
(a)the death of a person, or serious harm to a person’s health;
(b)a serious and irreversible disruption of the managem ent or operation of critical infrastr ucture;
(c)the infringement of obliga tions under Union law intended to protect fundamental rights;
(d)serious harm to proper ty or the environment ;
(50) ‘personal data’ means personal data as defined in Article 4, point (1), of Regulation (EU) 2016/679;
(51) ‘non-personal data’ means data other than personal data as defined in Article 4, point (1), of Regulation (EU) 
2016/679;
(52) ‘profiling’ means prof iling as defined in Article 4, point (4), of Regulation (EU) 2016/679;
(53) ‘real-w orld testing plan’ means a document that descr ibes the objectives, methodology , geographical, population and 
temporal scope, monitorin g, organisation and conduct of testing in real-world conditions;
(54) ‘sandbo x plan’ means a document agreed between the participating provid er and the comp etent author ity descr ibing 
the objectives, conditions, timeframe, methodology and requirements for the activities carried out within the sandbo x;
(55) ‘AI regulato ry sandbo x’ means a controlled framework set up by a comp etent author ity which offers provid ers or 
prospective provider s of AI syste ms the possibility to develop, train, validate and test, where appropr iate in real-wo rld 
conditions, an innovative AI system, pursuant to a sandbo x plan for a limited time under regulator y super vision;
(56) ‘AI literacy’ means skills, knowledg e and understanding that allow provider s, deplo yers and affect ed persons, taking 
into account their respective rights and obliga tions in the context of this Regulation, to make an informed deplo yment 
of AI systems, as well as to gain awareness about the oppor tunities and risks of AI and possible harm it can cause;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 49/144(57) ‘testing in real-world conditions’ means the temporar y testing of an AI syste m for its intended purpose in real-wo rld 
conditions outside a laborato ry or other wise simulated envir onment, with a view to gather ing reliable and robust data 
and to assessing and verifying the conf ormity of the AI system with the requirements of this Regulation and it does 
not qualify as placing the AI system on the marke t or putting it into service within the meaning of this Regulation, 
provided that all the conditions laid down in Article 57 or 60 are fulfilled;
(58) ‘subject’, for the purpose of real-world testing, means a natural person who participate s in testing in real-wo rld 
conditions;
(59) ‘informed consent’ means a subject’s freely given, specific, unambiguous and voluntar y expression of his or her 
willingness to participate in a particular testing in real-world conditions, after having been informed of all aspects of 
the testing that are relevant to the subject ’s decision to participate ;
(60) ‘deep fake’ means AI-g enerated or manipulate d imag e, audio or video cont ent that resembles existing persons, objects, 
places, entities or events and would falsely appear to a person to be authentic or truthful;
(61) ‘widespread infringement’ means any act or omission contrar y to Union law prot ecting the interest of individuals, 
which :
(a)has harmed or is likely to harm the collective interests of individuals residing in at least two Member States other 
than the Member State in which:
(i)the act or omission originated or took place;
(ii)the provider concer ned, or, where applicable, its author ised representative is located or established; or
(iii) the deplo yer is established, when the infringement is committed by the deplo yer;
(b)has caused, causes or is likel y to cause harm to the collective intere sts of individuals and has common features, 
including the same unlawful practice or the same interest being infringed, and is occur ring concur rently , 
committ ed by the same operat or, in at least three Member States;
(62) ‘critical infrastr ucture’ means critical infrastr ucture as defined in Article 2, point (4), of Directive (EU) 2022/2557;
(63) ‘general-pur pose AI model’ means an AI model, including where such an AI model is trained with a large amount of 
data using self-super vision at scale, that displa ys significant generality and is capable of compet ently perfo rming 
a wide rang e of distinct tasks regardless of the way the model is placed on the market and that can be integrat ed into 
a variety of downstream systems or applications, except AI models that are used for research, development or 
prot otyping activities before they are placed on the mark et;
(64) ‘high-imp act capabilities ’ means capabilities that matc h or exceed the capabilities recorded in the most advanced 
general-pur pose AI models;
(65) ‘syst emic risk’ means a risk that is specif ic to the high-im pact capabilities of general-pur pose AI models, having 
a significant impact on the Union market due to their reac h, or due to actual or reasonably foreseeable negative effects 
on public health, safety , public secur ity, fundamental rights, or the society as a whole, that can be propagat ed at scale 
across the value chain;
(66) ‘general-pur pose AI syste m’ means an AI syste m which is based on a general-pur pose AI model and which has the 
capability to serve a variety of purposes, both for direct use as well as for integrat ion in other AI systems;
(67) ‘floating-point operation’ means any mathematical operation or assignment involving floating-point numbers, which 
are a subset of the real numbers typically represente d on comput ers by an integ er of fixed precision scaled by an 
integer exponent of a fixed base;
(68) ‘downstream provider ’ means a provid er of an AI system, including a general-pur pose AI system, which integrat es an 
AI model, rega rdless of whether the AI model is provided by themselves and vertically integrat ed or provid ed by 
another entity based on contractual relations.EN OJ L, 12.7.2024
50/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojArticle 4
AI literacy
Provi ders and deplo yers of AI syste ms shall take measures to ensure, to their best exte nt, a suffi cient level of AI literacy of 
their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their 
technical kno wledge, exper ience, education and training and the context the AI systems are to be used in, and consider ing 
the persons or groups of persons on whom the AI syste ms are to be used.
CHAPTER II
PROHIBITED AI PRA CTICES
Article 5
Prohibited AI practices
1. The followi ng AI practices shall be prohibite d:
(a)the placing on the market, the putting into service or the use of an AI syste m that deplo ys subliminal techniques beyond 
a person’s consciousness or purposefully manipulative or decep tive techniques, with the objective, or the effect of 
materially distor ting the behaviour of a person or a group of persons by appreciably imp airing their ability to make an 
informed decision, thereby causing them to take a decision that they would not have other wise take n in a manner that 
causes or is reasonably likely to cause that person, another person or group of persons significant harm;
(b)the placing on the marke t, the putting into service or the use of an AI system that exploits any of the vulnerabilities of 
a natural person or a specif ic group of persons due to their age, disability or a specif ic social or economic situation, with 
the objective, or the effect, of materi ally distor ting the behavio ur of that person or a person belonging to that group in 
a manner that causes or is reasonably likely to cause that person or another person significant harm;
(c)the placing on the market, the putting into service or the use of AI syste ms for the evaluation or classif ication of natural 
persons or groups of persons over a certain period of time based on their social behavio ur or known, inferr ed or 
predicted personal or personality characteristics , with the social score leading to either or both of the follo wing:
(i)detr imental or unfa vourable treatment of certain natural persons or groups of persons in social contexts that are 
unrelat ed to the cont exts in which the data was originally generat ed or collect ed;
(ii)detr imental or unfa vourable treatment of certain natural persons or groups of persons that is unjustifie d or 
dispropor tionate to their social behavio ur or its gravity;
(d)the placing on the marke t, the putting into service for this specif ic purpose, or the use of an AI system for making risk 
assessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, 
based solely on the profiling of a natural person or on assessing their personality traits and character istics; this 
prohibition shall not apply to AI systems used to suppor t the human assessment of the involvement of a person in 
a criminal activity , which is already based on objective and verifiable facts directly linke d to a criminal activity ;
(e)the placing on the mark et, the putting into service for this specif ic purpose, or the use of AI syste ms that create or 
expand facial recognition databases through the untarget ed scraping of facial images from the inter net or CCT V footage;
(f)the placing on the marke t, the putting into service for this specific purpose, or the use of AI syste ms to infer emotions 
of a natural person in the areas of workplace and education institutions, except where the use of the AI system is 
intende d to be put in place or into the mark et for medical or safety reasons;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 51/144(g)the placing on the marke t, the putting into service for this specific purpose, or the use of biometr ic categor isation 
syste ms that catego rise individually natural persons based on their biometr ic data to deduce or infer their race, political 
opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does 
not cover any labelling or filter ing of lawfully acquired biometr ic datasets, such as imag es, based on biometr ic data or 
categori zing of biometr ic data in the area of law enforcement ;
(h)the use of ‘real-time’ remote biometr ic identification syste ms in publicly accessible spaces for the purposes of law 
enforcement, unless and in so far as such use is strictly necessar y for one of the follo wing objectives:
(i)the targe ted search for specif ic victims of abduction, trafficking in human beings or sexual exploitation of human 
beings, as well as the search for missing persons;
(ii)the prevention of a specif ic, substantial and imminent threat to the life or physica l safety of natural persons or 
a genuine and present or genuine and foreseeable threat of a terrorist attack;
(iii) the localisation or identification of a person suspect ed of having committed a criminal offence, for the purpose of 
conducting a criminal invest igation or prosecution or executing a criminal penalty for offences referred to in 
Annex II and punishable in the Member State concer ned by a custodial sente nce or a detention order for 
a maximum period of at least four years.
Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 2016/679 for the processing of 
biometr ic data for purposes other than law enforcement.
2. The use of ‘real-time’ remote biometr ic identifi cation syste ms in publicly accessible spaces for the purposes of law 
enforcement for any of the objectives refer red to in paragraph 1, first subparagraph, point (h), shall be deplo yed for the 
purposes set out in that point only to confir m the identity of the specif ically targe ted individual, and it shall take into 
account the follo wing elements:
(a)the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm 
that would be caused if the system were not used;
(b)the consequences of the use of the system for the rights and freedoms of all persons concer ned, in particular the 
seriousness, probability and scale of those consequences.
In addition, the use of ‘real-time’ remot e biometr ic identification systems in publicly accessible spaces for the purposes of 
law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), of this Article shall 
comply with necessar y and propor tionate safegua rds and conditions in relation to the use in accordance with the national 
law author ising the use thereof, in particular as regard s the temporal, geographic and personal limitations. The use of the 
‘real-time’ remot e biometr ic identifica tion syste m in publicly accessible spaces shall be author ised only if the law 
enforcement author ity has complet ed a fundamental rights impact assessment as provid ed for in Article 27 and has 
register ed the system in the EU database according to Article 49. However , in duly justified cases of urgency , the use of such 
syste ms may be commenced without the registration in the EU database, provid ed that such registration is complet ed 
without undue dela y.
3. For the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each use for the purposes of law 
enforcement of a ‘real-time’ remot e biometr ic identifica tion syste m in publicly accessible spaces shall be subject to a prior 
author isation granted by a judicial author ity or an independent administrative author ity whose decision is binding of the 
Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of 
national law referred to in paragraph 5. How ever , in a duly justified situation of urgency , the use of such system may be 
commenced without an author isation provided that such author isation is request ed without undue dela y, at the latest 
within 24 hours. If such author isation is rejected, the use shall be stopped with immediate effect and all the data, as well as 
the results and outputs of that use shall be immediately discarded and deleted.
The comp etent judicial author ity or an independent administrative author ity whose decision is binding shall grant the 
author isation only where it is satisfied, on the basis of objective evidence or clear indications presented to it, that the use of 
the ‘real-time’ remote biometr ic identifi cation system concer ned is necessar y for, and propor tionate to, achieving one of the EN OJ L, 12.7.2024
52/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojobjectives specified in paragraph 1, first subparagraph, point (h), as identified in the request and, in particular , remains 
limited to what is strictly necessar y concer ning the period of time as well as the geographic and personal scope. In deciding 
on the request, that author ity shall take into account the elements referred to in paragraph 2. No decision that produces an 
adverse legal effect on a person may be taken based solely on the output of the ‘real-time’ remote biometr ic identification 
syste m.
4. Without prejudice to paragraph 3, each use of a ‘real-time’ remot e biometr ic identifica tion system in publicly 
accessible spaces for law enforcement purposes shall be notif ied to the relevant marke t surveillance author ity and the 
national data prot ection author ity in accordance with the national rules refer red to in paragraph 5. The notif ication shall, as 
a minimum, contain the information specified under paragraph 6 and shall not include sensitive operational data.
5. A Member State may decide to provide for the possibility to fully or partially author ise the use of ‘real-time’ remote 
biometr ic identifica tion systems in publicly accessible spaces for the purposes of law enforcement within the limits and 
under the conditions listed in paragraph 1, first subparagraph, point (h), and paragraphs 2 and 3. Member States concer ned 
shall lay down in their national law the necessar y detailed rules for the request, issuance and exercise of, as well as 
super vision and repor ting relating to, the author isations referred to in paragraph 3. Those rules shall also specify in respect 
of which of the objectives listed in paragraph 1, first subparagraph, point (h), including which of the criminal offences 
referred to in point (h)(iii) thereof, the compet ent author ities may be author ised to use those syste ms for the purposes of 
law enforcement. Member States shall notify those rules to the Commission at the latest 30 days follo wing the adop tion 
thereof. Member States may introduce, in accordance with Union law, more restr ictive laws on the use of remote biometr ic 
identifica tion systems.
6. National market surveillance author ities and the national data prot ection author ities of Member States that have been 
notif ied of the use of ‘real-time’ remote biometr ic identifi cation syste ms in publicly accessible spaces for law enforcement 
purposes pursuant to paragraph 4 shall submit to the Commission annual repor ts on such use. For that purpose, the 
Commission shall provide Member States and national market surveillance and data protection author ities with a template , 
including information on the number of the decisions taken by compet ent judicial author ities or an independent 
administrative author ity whose decision is binding upon requests for author isations in accordance with paragraph 3 and 
their result.
7. The Commission shall publish annual repor ts on the use of real-time remot e biometr ic identification systems in 
publicly accessible spaces for law enforcement purposes, based on aggregat ed data in Member States on the basis of the 
annual repor ts refer red to in paragraph 6. Those annual repor ts shall not include sensitive operational data of the related 
law enforcement activities.
8. This Article shall not affect the prohibitions that apply where an AI practice infringes other Union law.
CHAPTER III
HIGH-RISK AI SYSTEMS
SECTION 1
Classification of AI systems as high-r isk
Article 6
Classif ication rules for high-r isk AI systems
1. Irrespective of whether an AI syste m is placed on the marke t or put into service independently of the products 
referred to in points (a) and (b), that AI system shall be considered to be high-r isk where both of the follo wing conditions 
are fulfilled:
(a)the AI syste m is intended to be used as a safety compo nent of a product, or the AI syste m is itself a product, cover ed by 
the Union harmonisation legislation listed in Annex I;
(b)the product whose safety compo nent pursuant to point (a) is the AI syste m, or the AI system itself as a product, is 
required to undergo a third-par ty conf ormity assessment, with a view to the placing on the marke t or the putting into 
service of that product pursuant to the Union harmonisation legislation listed in Annex I.
2. In addition to the high-r isk AI syste ms referred to in paragraph 1, AI syste ms referred to in Annex III shall be 
considered to be high-r isk.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 53/1443. By derogat ion from paragraph 2, an AI system referred to in Annex III shall not be considered to be high-r isk where it 
does not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by not 
materially influencing the outcome of decision making.
The first subparagraph shall apply where any of the follo wing conditions is fulfilled:
(a)the AI syste m is intende d to perform a narrow procedural task;
(b)the AI syste m is intende d to impro ve the result of a previously comp leted human activity ;
(c)the AI system is intende d to detect decision-making patt erns or deviations from prior decision-making patterns and is 
not meant to replace or influence the previously comp leted human assessment, without proper human review ; or
(d)the AI system is intended to perfor m a preparat ory task to an assessment relevant for the purposes of the use cases 
listed in Annex III.
Notwithstanding the first subparagraph, an AI system refer red to in Annex III shall alwa ys be considered to be high-r isk 
where the AI system perfor ms profiling of natural persons.
4. A provid er who considers that an AI syste m referred to in Annex III is not high-r isk shall document its assessment 
before that system is placed on the market or put into service. Such provid er shall be subject to the registration obliga tion 
set out in Article 49(2). Upon request of national comp etent author ities, the provider shall provide the documentation of 
the assessment.
5. The Commission shall, after consulting the European Artificial Intelligence Board (the ‘Board’), and no later than 
2 Febr uary 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 together 
with a comp rehensive list of practical exam ples of use cases of AI systems that are high-r isk and not high-r isk.
6. The Commission is empo wered to adopt deleg ated acts in accordance with Article 97 in order to amend paragraph 3, 
second subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, where 
there is concrete and reliable evidence of the existe nce of AI syste ms that fall under the scope of Annex III, but do not pose 
a significant risk of harm to the health, safety or fundamental rights of natural persons.
7. The Commission shall adop t delegat ed acts in accordance with Article 97 in order to amend paragraph 3, second 
subparagraph, of this Article by deleting any of the conditions laid down therein, where there is concrete and reliable 
evidence that this is necessar y to maintain the level of protection of health, safety and fundamental rights provided for by 
this Regulation.
8. Any amendment to the conditions laid down in paragraph 3, second subparagraph, adopt ed in accordance with 
paragraphs 6 and 7 of this Article shall not decrease the overall level of protect ion of health, safety and fundamental rights 
provid ed for by this Regulation and shall ensure consiste ncy with the deleg ated acts adop ted pursuant to Article 7(1), and 
take account of marke t and technological developments.
Article 7
Amendments to Annex III
1. The Commission is empowered to adop t deleg ated acts in accordance with Article 97 to amend Annex III by adding 
or modifying use-cases of high-r isk AI systems where both of the following conditions are fulfilled:
(a)the AI syste ms are intende d to be used in any of the areas listed in Annex III;
(b)the AI systems pose a risk of harm to health and safety , or an adverse impact on fundamental rights, and that risk is 
equivalent to, or great er than, the risk of harm or of adverse imp act posed by the high-r isk AI syste ms already referred 
to in Annex III.EN OJ L, 12.7.2024
54/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. When assessing the condition under paragraph 1, point (b), the Commission shall take into account the following 
criteria:
(a)the intended purpose of the AI system;
(b)the exte nt to which an AI syste m has been used or is likely to be used;
(c)the nature and amount of the data processed and used by the AI system, in particular whether special categori es of 
personal data are processed;
(d)the extent to which the AI system acts autonomously and the possibility for a human to override a decision or 
recommendations that may lead to pote ntial harm;
(e)the exte nt to which the use of an AI syste m has already caused harm to health and safety , has had an adverse impact on 
fundamental rights or has given rise to significant concer ns in relation to the likelihood of such harm or adverse impact, 
as demonstrated, for exam ple, by repor ts or documente d alleg ations submitted to national compet ent author ities or by 
other repor ts, as appropr iate;
(f)the pote ntial exte nt of such harm or such adverse impact, in particular in terms of its intens ity and its ability to affect 
multiple persons or to dispropor tionately affect a particular group of persons;
(g)the extent to which persons who are potent ially harmed or suffer an adverse impact are dependent on the outcome 
produced with an AI system, in particular because for practical or lega l reasons it is not reasonably possible to opt-ou t 
from that outcome;
(h)the extent to which there is an imbalance of power , or the persons who are potentially harmed or suffer an adverse 
imp act are in a vulnerable position in relation to the deplo yer of an AI syste m, in particular due to status, author ity, 
kno wledge, economic or social circumstances, or age;
(i)the exte nt to which the outcome produced involving an AI system is easily corrigible or reversible, taking into account 
the technical solutions available to correct or reverse it, whereby outcomes having an adverse imp act on health, safety or 
fundamental rights, shall not be considered to be easily corrigible or reversible;
(j)the magnitude and likelihood of benefit of the deplo yment of the AI system for individuals, groups, or society at large, 
including possible improvements in product safety;
(k)the exte nt to which existing Union law provid es for:
(i)effective measures of redress in relation to the risks posed by an AI syste m, with the exclusion of claims for 
damage s;
(ii)effective measures to prevent or substantially minimise those risks.
3. The Commission is empo wered to adopt deleg ated acts in accordance with Article 97 to amend the list in Annex III 
by removi ng high-r isk AI systems where both of the following conditions are fulfilled:
(a)the high-r isk AI syste m concer ned no longer poses any signifi cant risks to fundamental rights, health or safety , taking 
into account the criteria listed in paragraph 2;
(b)the deletion does not decrease the overall level of prot ection of health, safety and fundamental rights under Union law.
SECTION 2
Requir ements for high-r isk AI systems
Article 8
Compliance with the requirements
1. High-r isk AI systems shall comp ly with the requirements laid down in this Section, taking into account their intended 
purpose as well as the generally ackno wledged state of the art on AI and AI-related technologies. The risk management 
syste m referred to in Article 9 shall be taken into account when ensur ing compliance with those requirements.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 55/1442. Where a product contains an AI syste m, to which the requirements of this Regulation as well as requirements of the 
Union harmonisation legislation listed in Section A of Annex I apply , provid ers shall be responsible for ensur ing that their 
product is fully comp liant with all applicable requirements under applicable Union harmonisation legislation. In ensur ing 
the comp liance of high-r isk AI systems referred to in paragraph 1 with the requirements set out in this Section, and in order 
to ensure consistency , avoid duplication and minimise additional burdens, provid ers shall have a choice of integrat ing, as 
appropr iate, the necessar y testing and repor ting processes, information and documentation they provide with rega rd to 
their product into documentation and procedures that already exist and are required under the Union harmonisation 
legislation listed in Section A of Annex I.
Article 9
Risk management system
1. A risk managem ent system shall be established, imp lemented, documented and maintained in relation to high-r isk AI 
syste ms.
2. The risk managem ent system shall be understood as a continuous iterative process planned and run throughout the 
entire lifecycle of a high-r isk AI system, requir ing regular systematic review and updating. It shall comp rise the following 
steps:
(a)the identification and analysis of the known and the reasonably foreseeable risks that the high-r isk AI system can pose 
to health, safety or fundamental rights when the high-r isk AI syste m is used in accordance with its intende d purpose;
(b)the estimation and evaluation of the risks that may emerg e when the high-r isk AI syste m is used in accordance with its 
intende d purpose, and under conditions of reasonably foreseeable misuse;
(c)the evaluation of other risks possibly arising, based on the analysis of data gathered from the post-mark et monitoring 
syste m referred to in Article 72;
(d)the adoption of appropr iate and target ed risk management measures designed to address the risks identified pursuant to 
point (a).
3. The risks referred to in this Article shall concer n only those which may be reasonably mitiga ted or eliminated through 
the development or design of the high-r isk AI syste m, or the provision of adequat e technical information.
4. The risk managem ent measures referred to in paragraph 2, point (d), shall give due consideration to the effects and 
possible interaction resulting from the combined application of the requirements set out in this Section, with a view to 
minimising risks more effectively while achieving an appropr iate balance in implementing the measures to fulfil those 
requirements.
5. The risk managem ent measures refer red to in paragraph 2, point (d), shall be such that the relevant residual risk 
associate d with each hazard, as well as the overall residual risk of the high-r isk AI syste ms is judged to be accep table.
In identifying the most appropr iate risk managem ent measures, the following shall be ensured:
(a)elimination or reduction of risks identifie d and evaluate d pursuant to paragraph 2 in as far as technically feasible 
through adequate design and development of the high-r isk AI syste m;
(b)where appropr iate, imp lementation of adequate mitiga tion and control measures addressing risks that cannot be 
eliminated ;
(c)provision of information required pursuant to Article 13 and, where appropr iate, training to deplo yers.
With a view to eliminating or reducing risks relate d to the use of the high-r isk AI system, due consideration shall be given 
to the technical kno wledge, exper ience, education, the training to be expected by the deplo yer, and the presumable cont ext 
in which the system is intended to be used.EN OJ L, 12.7.2024
56/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj6. High-r isk AI systems shall be tested for the purpose of identifying the most appropr iate and target ed risk management 
measures. Testing shall ensure that high-r isk AI systems perfor m consist ently for their intended purpose and that they are in 
complia nce with the requirements set out in this Section.
7. Testing procedures may include testing in real-world conditions in accordance with Article 60.
8. The testing of high-r isk AI syste ms shall be perfo rmed, as appropr iate, at any time throughout the development 
process, and, in any event, prior to their being placed on the market or put into service. Testing shall be carried out agains t 
prior defined metr ics and probabilistic thresholds that are appropr iate to the intended purpose of the high-r isk AI syste m.
9. When implementing the risk management system as provid ed for in paragraphs 1 to 7, provider s shall give 
consideration to whether in view of its intende d purpose the high-r isk AI syste m is likely to have an adverse impact on 
persons under the age of 18 and, as appropr iate, other vulnerable groups.
10. For provider s of high-r isk AI syste ms that are subject to requirements regard ing inter nal risk managem ent processes 
under other relevant provisions of Union law, the aspects provided in paragraphs 1 to 9 may be part of, or combined with, 
the risk managem ent procedures established pursuant to that law.
Article 10
Data and data governance
1. High-r isk AI syste ms which mak e use of techniques involving the training of AI models with data shall be developed 
on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 
whenever such data sets are used.
2. Training, validation and testing data sets shall be subject to data gove rnance and management practices appropr iate 
for the intended purpose of the high-r isk AI syste m. Those practices shall concer n in particular:
(a)the relevant design choices ;
(b)data collection processes and the origin of data, and in the case of personal data, the original purpose of the data 
collection;
(c)relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and 
aggregati on;
(d)the formulation of assum ptions, in particular with respect to the information that the data are supposed to measure and 
represent ;
(e)an assessment of the availability , quantity and suitability of the data sets that are needed;
(f)examination in view of possible biases that are likel y to affect the health and safety of persons, have a negative impact 
on fundamental rights or lead to discr imination prohibited under Union law, especially where data outputs influence 
input s for future operations;
(g)appropr iate measures to detect, prevent and mitigat e possible biases identifie d according to point (f);
(h)the identifica tion of relevant data gaps or shor tcomings that prevent compliance with this Regulation, and how those 
gaps and shor tcomings can be addressed.
3. Training, validation and testing data sets shall be relevant, suffici ently representative, and to the best exte nt possible, 
free of errors and comp lete in view of the intended purpose. They shall have the appropr iate statistical proper ties, including, 
where applicable, as regards the persons or groups of persons in relation to whom the high-r isk AI system is intende d to be 
used. Those charact eristics of the data sets may be met at the level of individual data sets or at the level of a combination 
thereof.
4. Data sets shall take into account, to the extent required by the intended purpose, the char acter istics or elements that 
are particular to the specific geographical, cont extual, behavi oural or functional setting within which the high-r isk AI 
syste m is intende d to be used.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 57/1445. To the exte nt that it is strictly necessar y for the purpose of ensur ing bias detection and correction in relation to the 
high-r isk AI systems in accordance with paragraph (2), points (f) and (g) of this Article, the provid ers of such systems may 
exceptionally process special categor ies of personal data, subject to appropr iate safeguards for the fundamental rights and 
freedoms of natural persons. In addition to the provisions set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and 
Directive (EU) 2016/680, all the following conditions must be met in order for such processing to occur:
(a)the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic or 
anonymised data;
(b)the special cate gories of personal data are subject to technical limitations on the re-use of the personal data, and 
state-of-t he-ar t secur ity and privacy-preser ving measures, including pseudon ymisation;
(c)the special cate gories of personal data are subject to measures to ensure that the personal data processed are secured, 
protect ed, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and 
ensure that only author ised persons have access to those personal data with appropr iate conf identiality obliga tions;
(d)the special categori es of personal data are not to be transmitte d, transferre d or other wise accessed by other parties;
(e)the special categor ies of personal data are delete d once the bias has been corrected or the personal data has reac hed the 
end of its retention period, whichever comes first;
(f)the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 
2016/680 include the reasons why the processing of special categor ies of personal data was strictly necessar y to detect 
and correct biases, and why that objective could not be achi eved by processing other data.
6. For the development of high-r isk AI syste ms not using techniques involving the training of AI models, paragraphs 2 
to 5 apply only to the testing data sets.
Article 11
Technical documentation
1. The technical documentation of a high-r isk AI syste m shall be drawn up before that syste m is placed on the market or 
put into service and shall be kept up-to date.
The technical documentation shall be drawn up in such a way as to demonstrate that the high-r isk AI system complie s with 
the requirements set out in this Section and to provide national comp etent author ities and notified bodies with the 
necessar y information in a clear and compre hensive form to assess the compliance of the AI syste m with those 
requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, including start-ups, may provid e the 
elements of the technical documentation specified in Annex IV in a simplified manner . To that end, the Commission shall 
establish a simplified technical documentation form target ed at the needs of small and microent erprises. Where an SME, 
including a start-up, opts to provide the information required in Annex IV in a simplified manner , it shall use the form 
referred to in this paragraph. Notified bodies shall accep t the form for the purposes of the conf ormity assessment.
2. Where a high-r isk AI system related to a product covered by the Union harmonisation legislation listed in Section 
A of Annex I is placed on the marke t or put into service, a sing le set of technical documentation shall be drawn up 
containing all the information set out in paragraph 1, as well as the information required under those lega l acts.
3. The Commission is empo wered to adop t delegat ed acts in accordance with Article 97 in order to amend Annex IV, 
where necessar y, to ensure that, in light of technical progress, the technical documentation provides all the information 
necessar y to assess the compliance of the system with the requirements set out in this Section.EN OJ L, 12.7.2024
58/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojArticle 12
Record-ke eping
1. High-r isk AI syste ms shall technically allow for the auto matic recording of events (logs) over the lifetime of the 
syste m.
2. In order to ensure a level of traceability of the functioning of a high-r isk AI system that is appropr iate to the intended 
purpose of the syste m, logging capabilities shall enable the recording of events relevant for:
(a)identifying situations that may result in the high-r isk AI system presenting a risk within the meaning of Article 79(1) or 
in a substantial modification;
(b)facilitating the post-market monitoring refer red to in Article 72; and
(c)monitoring the operation of high-r isk AI systems referred to in Article 26(5).
3. For high-r isk AI systems referred to in point 1 (a), of Annex III, the logging capabilities shall provid e, at a minimum:
(a)recording of the period of each use of the syste m (star t date and time and end date and time of each use);
(b)the reference database against which input data has been checked by the syste m;
(c)the input data for which the search has led to a matc h;
(d)the identification of the natural persons involved in the verification of the results, as refer red to in Article 14(5).
Article 13
Transparency and provision of information to deplo yers
1. High-r isk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently 
transparent to enable deplo yers to interpret a syste m’s output and use it appropr iately. An appropr iate type and degree of 
transparency shall be ensured with a view to achieving comp liance with the relevant obliga tions of the provid er and 
deplo yer set out in Section 3.
2. High-r isk AI systems shall be accompanied by instr uctions for use in an appropr iate digital format or other wise that 
include concise, complet e, correct and clear information that is relevant, accessible and comp rehensible to deplo yers.
3. The instr uctions for use shall contain at least the follo wing information:
(a)the identity and the contact details of the provider and, where applicable, of its author ised representative;
(b)the character istics, capabilities and limitations of perf ormance of the high-r isk AI syste m, including:
(i)its intended purpose;
(ii)the level of accuracy , including its metr ics, robustness and cybersecur ity referred to in Article 15 against which the 
high-r isk AI syste m has been tested and validat ed and which can be expected, and any known and foreseeable 
circumstances that may have an imp act on that expected level of accuracy , robustness and cybersecur ity;
(iii) any kno wn or foreseeable circumstance, relate d to the use of the high-r isk AI syste m in accordance with its 
intended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health and 
safety or fundamental rights refer red to in Article 9(2);
(iv) where applicable, the technical capabilities and character istics of the high-r isk AI system to provid e information 
that is relevant to explain its output ;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 59/144(v)when appropr iate, its perform ance rega rding specific persons or groups of persons on which the system is 
intended to be used;
(vi) when appropr iate, specif ications for the input data, or any other relevant information in terms of the training, 
validation and testing data sets used, taking into account the intende d purpose of the high-r isk AI system;
(vii) where applicable, information to enable deplo yers to inter pret the output of the high-r isk AI system and use it 
appropr iately;
(c)the chang es to the high-r isk AI syste m and its perf ormance which have been pre-dete rmined by the provider at the 
moment of the initial conf ormity assessment, if any;
(d)the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate the 
inter pretation of the outputs of the high-r isk AI syste ms by the deplo yers;
(e)the computa tional and hardware resources needed, the expect ed lifetime of the high-r isk AI system and any necessar y 
mainte nance and care measures, including their frequency , to ensure the proper functioning of that AI system, including 
as regards software update s;
(f)where relevant, a descr iption of the mechanisms included within the high-r isk AI system that allows deplo yers to 
properly collect, store and interpret the logs in accordance with Article 12.
Article 14
Human oversight
1. High-r isk AI systems shall be designed and developed in such a way, including with appropr iate human-machi ne 
interfac e tools, that they can be effectively oversee n by natural persons during the period in which they are in use.
2. Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge 
when a high-r isk AI syste m is used in accordance with its intended purpose or under conditions of reasonably foreseeable 
misuse, in particular where such risks persist despite the application of other requirements set out in this Section.
3. The overs ight measures shall be commensurat e with the risks, level of auto nomy and context of use of the high-r isk 
AI system, and shall be ensured through either one or both of the following types of measures:
(a)measures identified and built, when technically feasible, into the high-r isk AI system by the provid er before it is placed 
on the mark et or put into service;
(b)measures identifie d by the provider before placing the high-r isk AI syste m on the marke t or putting it into service and 
that are appropr iate to be implement ed by the deplo yer.
4. For the purpose of imp lementing paragraphs 1, 2 and 3, the high-r isk AI system shall be provided to the deplo yer in 
such a way that natural persons to whom human oversight is assigned are enabled, as appropr iate and propor tionate :
(a)to properly understand the relevant capacities and limitations of the high-r isk AI system and be able to duly monitor its 
operation, including in view of detecting and addressing anomalies, dysfunctions and unexpect ed perfo rmance;
(b)to remain aware of the possible tendency of automat ically relying or over -relying on the output produced by a high-r isk 
AI syste m (automation bias), in particular for high-r isk AI systems used to provide information or recommendations for 
decisions to be take n by natural persons;
(c)to correctly interpret the high-r isk AI system’s output, taking into account, for exam ple, the interpretat ion tools and 
methods available;EN OJ L, 12.7.2024
60/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(d)to decide, in any particular situation, not to use the high-r isk AI syste m or to other wise disreg ard, overr ide or reverse 
the output of the high-r isk AI syste m;
(e)to inter vene in the operation of the high-r isk AI syste m or inter rupt the syste m through a ‘stop’ button or a similar 
procedure that allows the syste m to come to a halt in a safe state.
5. For high-r isk AI syste ms refer red to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article 
shall be such as to ensure that, in addition, no action or decision is take n by the deplo yer on the basis of the identification 
resulting from the system unless that identification has been separate ly verified and confi rmed by at least two natural 
persons with the necessar y compet ence, training and author ity.
The requirement for a separate verificati on by at least two natural persons shall not apply to high-r isk AI syste ms used for 
the purposes of law enforcement, migration, border control or asylum, where Union or national law considers the 
application of this requirement to be dispropor tionate .
Article 15
Accuracy , robustness and cybersecur ity
1. High-r isk AI syste ms shall be designed and developed in such a way that they achieve an appropr iate level of accuracy , 
robustness, and cybersecur ity, and that they perf orm consiste ntly in those respects throughout their lifecy cle.
2. To address the technical aspects of how to measure the appropr iate levels of accuracy and robustness set out in 
paragraph 1 and any other relevant perform ance metr ics, the Commission shall, in cooperation with relevant stakeholders 
and organisations such as metrology and bench marking author ities, encourage , as appropr iate, the development of 
bench mark s and measurement methodologies.
3. The levels of accuracy and the relevant accuracy metr ics of high-r isk AI syste ms shall be declared in the accom panying 
instr uctions of use.
4. High-r isk AI syste ms shall be as resilient as possible regard ing errors, faults or inconsiste ncies that may occur within 
the syste m or the environment in which the syste m operat es, in particular due to their interaction with natural persons or 
other syste ms. Technical and organisational measures shall be take n in this regard .
The robustness of high-r isk AI syste ms may be achieved through technical redundancy solutions, which may include 
backu p or fail-safe plans.
High-r isk AI syste ms that continue to learn after being placed on the market or put into service shall be developed in such 
a way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations 
(feedback loops), and as to ensure that any such feedback loops are duly addressed with appropr iate mitigati on measures.
5. High-r isk AI systems shall be resilient against attempts by unauthor ised third parties to alter their use, outputs or 
perfo rmance by exploiting syste m vulnerabilities.
The technical solutions aiming to ensure the cybersecur ity of high-r isk AI syste ms shall be appropr iate to the relevant 
circumstances and the risks.
The technical solutions to address AI specific vulnerabilities shall include, where appropr iate, measures to prevent, detect, 
respond to, resolve and control for attac ks trying to manipulat e the training data set (data poisoning), or pre-trained 
compo nents used in training (model poisoning), inputs designed to cause the AI model to make a mistake (adversar ial 
exam ples or model evasion), confi dentiality attac ks or model flaws.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 61/144SECTION 3
Oblig ations of providers and deploye rs of high-r isk AI syste ms and other parties
Article 16
Obligations of providers of high-r isk AI systems
Provi ders of high-r isk AI syste ms shall:
(a)ensure that their high-r isk AI systems are compliant with the requirements set out in Section 2;
(b)indicate on the high-r isk AI syste m or, where that is not possible, on its packaging or its accom panying documentation, 
as applicable, their name, regist ered trade name or register ed trade mark , the address at which they can be contacted;
(c)have a quality management syste m in place which comp lies with Article 17;
(d)keep the documentation refer red to in Article 18;
(e)when under their control, keep the logs auto matically generated by their high-r isk AI systems as referred to in 
Article 19;
(f)ensure that the high-r isk AI system undergoes the relevant conf ormity assessment procedure as refer red to in Article 43, 
prior to its being placed on the mark et or put into service;
(g)draw up an EU declaration of conf ormity in accordance with Article 47;
(h)affix the CE marking to the high-r isk AI system or, where that is not possible, on its packag ing or its accom panying 
documentation, to indicate conf ormity with this Regulation, in accordance with Article 48;
(i)comply with the registration oblig ations refer red to in Article 49(1);
(j)take the necessar y corrective actions and provide information as required in Article 20;
(k)upon a reasoned request of a national comp etent author ity, demonstrate the conf ormity of the high-r isk AI system with 
the requirements set out in Section 2;
(l)ensure that the high-r isk AI system com plies with accessibility requirements in accordance with Directives (EU) 
2016/2102 and (EU) 2019/882.
Article 17
Quality management system
1. Providers of high-r isk AI syste ms shall put a quality manag ement system in place that ensures comp liance with this 
Regulation. That system shall be documente d in a syste matic and orderly manner in the form of written policies, procedures 
and instr uctions, and shall include at least the following aspects:
(a) a strate gy for regulator y comp liance, including comp liance with conf ormity assessment procedures and procedures for 
the manag ement of modifications to the high-r isk AI system;
(b) techniques, procedures and syste matic actions to be used for the design, design control and design verification of the 
high-r isk AI syste m;
(c) techniques, procedures and systematic actions to be used for the development, quality control and quality assurance of 
the high-r isk AI system;
(d) examination, test and validation procedures to be carried out before, during and after the development of the high-r isk 
AI system, and the frequency with which they have to be carried out;EN OJ L, 12.7.2024
62/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(e) technical specifications, including standards, to be applied and, where the relevant harmonised standards are not 
applied in full or do not cover all of the relevant requirements set out in Section 2, the means to be used to ensure that 
the high-r isk AI system complies with those requirements;
(f) syste ms and procedures for data managem ent, including data acquisition, data collection, data analysis, data labelling, 
data storage, data filtration, data mining, data aggregation, data retention and any other operation regard ing the data 
that is perfo rmed before and for the purpose of the placing on the mark et or the putting into service of high-r isk AI 
syste ms;
(g) the risk managem ent syste m referred to in Article 9;
(h) the setting-up, imp lementation and maintenance of a post-mark et monitoring syste m, in accordance with Article 72;
(i) procedures related to the repor ting of a serious incident in accordance with Article 73;
(j) the handling of communication with national compet ent author ities, other relevant author ities, including those 
provid ing or suppor ting the access to data, notif ied bodies, other operators, custome rs or other intere sted parties;
(k) syste ms and procedures for record-keepi ng of all relevant documentation and information;
(l) resource manag ement, including secur ity-of-supply related measures;
(m) an accountability framework setting out the responsibilities of the management and other staff with regard to all the 
aspects listed in this paragraph.
2. The imp lementation of the aspects refer red to in paragraph 1 shall be propor tionate to the size of the provider ’s 
organisation. Providers shall, in any event, respect the degree of rigour and the level of protection required to ensure the 
complia nce of their high-r isk AI systems with this Regulation.
3. Providers of high-r isk AI systems that are subject to obliga tions regard ing quality management systems or an 
equivalent function under relevant sectoral Union law may include the aspects listed in paragraph 1 as part of the quality 
managem ent syste ms pursuant to that law.
4. For provid ers that are financial institutions subject to requirements regard ing their inter nal gove rnance, arrangements 
or processes under Union financial services law, the obliga tion to put in place a quality management syste m, with the 
exception of paragraph 1, points (g), (h) and (i) of this Article, shall be deemed to be fulfilled by comp lying with the rules on 
inter nal gover nance arrang ements or processes pursuant to the relevant Union financ ial services law. To that end, any 
harmonised standards referred to in Article 40 shall be take n into account.
Article 18
Documentation keep ing
1. The provider shall, for a period ending 10 years after the high-r isk AI syste m has been placed on the marke t or put 
into service, keep at the disposal of the national compet ent author ities:
(a)the technical documentation refer red to in Article 11;
(b)the documentation concer ning the quality management syste m referred to in Article 17;
(c)the documentation concer ning the changes approved by notif ied bodies, where applicable;
(d)the decisions and other documents issued by the notified bodies, where applicable;
(e)the EU declaration of conf ormity referred to in Article 47.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 63/1442. Each Member State shall determine conditions under which the documentation referred to in paragraph 1 remains at 
the disposal of the national compet ent author ities for the period indicated in that paragraph for the cases when a provid er 
or its author ised representative established on its territory goes bankr upt or ceases its activity prior to the end of that 
period.
3. Providers that are financial institutions subject to requirements regard ing their inter nal gover nance, arrangements or 
processes under Union financ ial services law shall maintain the technical documentation as part of the documentation kept 
under the relevant Union financial services law.
Article 19
Aut omatically generated logs
1. Providers of high-r isk AI syste ms shall keep the logs refer red to in Article 12(1), automat ically generated by their 
high-r isk AI syste ms, to the extent such logs are under their control. Without prejudice to applicable Union or national law, 
the logs shall be kept for a period appropr iate to the intended purpose of the high-r isk AI syste m, of at least six months, 
unless provided other wise in the applicable Union or national law, in particular in Union law on the protect ion of personal 
data.
2. Providers that are financial institutions subject to requirements regard ing their inter nal gover nance, arrangements or 
processes under Union financial services law shall maintain the logs automa tically generated by their high-r isk AI syste ms 
as part of the documentation kept under the relevant financial services law.
Article 20
Cor rectiv e actions and duty of infor mation
1. Providers of high-r isk AI syste ms which consider or have reason to consider that a high-r isk AI system that they have 
placed on the market or put into service is not in conf ormity with this Regulation shall immediate ly take the necessar y 
corrective actions to bring that system into conf ormity , to withdraw it, to disable it, or to recall it, as appropr iate. They shall 
inform the distr ibutors of the high-r isk AI syste m concer ned and, where applicable, the deplo yers, the author ised 
representative and imp orters according ly.
2. Where the high-r isk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of 
that risk, it shall immediate ly investig ate the causes, in collaboration with the repor ting deplo yer, where applicable, and 
inform the marke t surveillance author ities compet ent for the high-r isk AI system concer ned and, where applicable, the 
notif ied body that issued a certificate for that high-r isk AI system in accordance with Article 44, in particular , of the nature 
of the non-comp liance and of any relevant corrective action taken.
Article 21
Cooperation with competent author ities
1. Providers of high-r isk AI systems shall, upon a reasoned request by a comp etent author ity, provide that author ity all 
the information and documentation necessar y to demonstrate the conf ormity of the high-r isk AI syste m with the 
requirements set out in Section 2, in a languag e which can be easily understood by the author ity in one of the official 
languag es of the institutions of the Union as indicate d by the Member State concer ned.
2. Upon a reasoned request by a compet ent author ity, providers shall also give the requesting comp etent author ity, as 
applicable, access to the automat ically generated logs of the high-r isk AI syste m refer red to in Article 12(1), to the exte nt 
such logs are under their control.
3. Any information obtained by a comp etent author ity pursuant to this Article shall be treate d in accordance with the 
confi dentiality oblig ations set out in Article 78.EN OJ L, 12.7.2024
64/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojArticle 22
Author ised representativ es of providers of high-r isk AI systems
1. Prior to making their high-r isk AI syste ms available on the Union mark et, provider s established in third countr ies 
shall, by written mandate, appoint an author ised representative which is established in the Union.
2. The provid er shall enable its author ised representative to perf orm the tasks specified in the mandate received from the 
provid er.
3. The author ised representative shall perfo rm the tasks specified in the mandate received from the provider . It shall 
provid e a copy of the mandate to the marke t surveillance author ities upon request, in one of the official languages of the 
institutions of the Union, as indicated by the compet ent author ity. For the purposes of this Regulation, the mandate shall 
emp ower the author ised representative to carry out the following tasks :
(a)verify that the EU declaration of conf ormity refer red to in Article 47 and the technical documentation referred to in 
Article 11 have been drawn up and that an appropr iate conf ormity assessment procedure has been carried out by the 
provid er;
(b)keep at the disposal of the comp etent author ities and national author ities or bodies referred to in Article 74(10), for 
a period of 10 years after the high-r isk AI syste m has been placed on the marke t or put into service, the contact details 
of the provid er that appoint ed the author ised representative, a copy of the EU declaration of conf ormity refer red to in 
Article 47, the technical documentation and, if applicable, the certificate issued by the notif ied body ;
(c)provid e a compet ent author ity, upon a reasoned request, with all the information and documentation, including that 
referred to in point (b) of this subparagraph, necessar y to demonstrate the conf ormity of a high-r isk AI system with the 
requirements set out in Section 2, including access to the logs, as referred to in Article 12(1), auto matically generated by 
the high-r isk AI system, to the extent such logs are under the control of the provid er;
(d)cooperate with compet ent author ities, upon a reasoned request, in any action the latter take in relation to the high-r isk 
AI system, in particular to reduce and mitiga te the risks posed by the high-r isk AI system;
(e)where applicable, comply with the registration obliga tions refer red to in Article 49(1), or, if the registration is carried 
out by the provid er itself, ensure that the information refer red to in point 3 of Section A of Annex VIII is correct.
The mandate shall empo wer the author ised representative to be addressed, in addition to or instead of the provider , by the 
compet ent author ities, on all issues related to ensur ing comp liance with this Regulation.
4. The author ised representative shall terminate the mandat e if it considers or has reason to consider the provider to be 
acting contrar y to its obliga tions pursuant to this Regulation. In such a case, it shall immediately inform the relevant marke t 
surveillance author ity, as well as, where applicable, the relevant notified body , about the termination of the mandat e and the 
reasons theref or.
Article 23
Obligations of impor ters
1. Before placing a high-r isk AI syste m on the marke t, imp orters shall ensure that the syste m is in conf ormity with this 
Regulation by verifying that:
(a)the relevant conf ormity assessment procedure refer red to in Article 43 has been carried out by the provider of the 
high-r isk AI syste m;
(b)the provid er has drawn up the technical documentation in accordance with Article 11 and Annex IV;
(c)the system bears the required CE marking and is accompanied by the EU declaration of conf ormity refer red to in 
Article 47 and instr uctions for use;
(d)the provid er has appoint ed an author ised representative in accordance with Article 22(1).OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 65/1442. Where an impor ter has sufficient reason to consider that a high-r isk AI system is not in conf ormity with this 
Regulation, or is falsifi ed, or accompanie d by falsifi ed documentation, it shall not place the syste m on the market until it has 
been brought into conf ormity . Where the high-r isk AI syste m presents a risk within the meaning of Article 79(1), the 
imp orter shall inform the provid er of the syste m, the author ised representative and the market surveillance author ities to 
that effect.
3. Impor ters shall indicate their name, regist ered trade name or register ed trade mark, and the address at which they can 
be contacted on the high-r isk AI syste m and on its packaging or its accom panying documentation, where applicable.
4. Impor ters shall ensure that, while a high-r isk AI syste m is under their responsibility , storage or transpor t conditions, 
where applicable, do not jeopardise its compliance with the requirements set out in Section 2.
5. Impor ters shall keep, for a period of 10 years after the high-r isk AI system has been placed on the mark et or put into 
service, a copy of the certificate issued by the notif ied body , where applicable, of the instr uctions for use, and of the EU 
declaration of conf ormity referred to in Article 47.
6. Impor ters shall provid e the relevant comp etent author ities, upon a reasoned request, with all the necessar y 
information and documentation, including that refer red to in paragraph 5, to demonstrat e the conf ormity of a high-r isk AI 
syste m with the requirements set out in Section 2 in a languag e which can be easily understoo d by them. For this purpose, 
they shall also ensure that the technical documentation can be made available to those author ities.
7. Impor ters shall cooperate with the relevant compet ent author ities in any action those author ities take in relation to 
a high-r isk AI syste m placed on the marke t by the importers, in particular to reduce and mitig ate the risks posed by it.
Article 24
Obligations of distr ibutors
1. Before making a high-r isk AI system available on the marke t, distr ibutors shall verify that it bears the required CE 
marking, that it is accom panied by a copy of the EU declaration of conf ormity referred to in Article 47 and instr uctions for 
use, and that the provid er and the imp orter of that syste m, as applicable, have complied with their respective obliga tions as 
laid down in Article 16, points (b) and (c) and Article 23(3).
2. Where a distr ibutor considers or has reason to consider , on the basis of the information in its possession, that 
a high-r isk AI system is not in conf ormity with the requirements set out in Section 2, it shall not mak e the high-r isk AI 
syste m available on the market until the syste m has been brought into conf ormity with those requirements. Further more, 
where the high-r isk AI system presents a risk within the meaning of Article 79(1), the distr ibutor shall inform the provid er 
or the imp orter of the syste m, as applicable, to that effect.
3. Distr ibut ors shall ensure that, while a high-r isk AI syste m is under their responsibility , storage or transpor t 
conditions, where applicable, do not jeopardise the compliance of the syste m with the requirements set out in Section 2.
4. A distr ibut or that considers or has reason to consider , on the basis of the information in its possession, a high-r isk AI 
syste m which it has made available on the marke t not to be in conf ormity with the requirements set out in Section 2, shall 
take the corrective actions necessar y to bring that system into conf ormity with those requirements, to withdra w it or recall 
it, or shall ensure that the provid er, the impor ter or any relevant operat or, as appropr iate, take s those corrective actions. 
Where the high-r isk AI system presents a risk within the meaning of Article 79(1), the distr ibut or shall immediate ly inform 
the provid er or impor ter of the syste m and the author ities compet ent for the high-r isk AI system concer ned, giving details, 
in particular , of the non-com pliance and of any corrective actions taken.
5. Upon a reasoned request from a relevant comp etent author ity, distr ibut ors of a high-r isk AI syste m shall provide that 
author ity with all the information and documentation regarding their actions pursuant to paragraphs 1 to 4 necessar y to 
demonstrate the conf ormity of that system with the requirements set out in Section 2.
6. Distr ibut ors shall cooperate with the relevant compet ent author ities in any action those author ities take in relation to 
a high-r isk AI system made available on the market by the distr ibutors, in particular to reduce or mitigat e the risk posed by 
it.EN OJ L, 12.7.2024
66/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojArticle 25
Responsibilities along the AI value chain
1. Any distr ibut or, imp orter, deplo yer or other third-par ty shall be considered to be a provid er of a high-r isk AI system 
for the purposes of this Regulation and shall be subject to the obliga tions of the provider under Article 16, in any of the 
follo wing circumstances:
(a)they put their name or trademark on a high-r isk AI system already placed on the market or put into service, without 
prejudice to contractual arrangements stipulating that the obliga tions are other wise allocat ed;
(b)they mak e a substantial modification to a high-r isk AI system that has already been placed on the market or has already 
been put into service in such a way that it remains a high-r isk AI system pursuant to Article 6;
(c)they modify the intended purpose of an AI system, including a general-pur pose AI system, which has not been classif ied 
as high-r isk and has already been placed on the marke t or put into service in such a way that the AI syste m concer ned 
becomes a high-r isk AI system in accordance with Article 6.
2. Where the circumstances referred to in paragraph 1 occur , the provid er that initially placed the AI system on the 
marke t or put it into service shall no longe r be considered to be a provider of that specif ic AI syste m for the purposes of 
this Regulation. That initial provider shall closely cooperate with new provid ers and shall make available the necessar y 
information and provide the reasonably expected technical access and other assistance that are required for the fulfilment of 
the oblig ations set out in this Regulation, in particular rega rding the compliance with the conf ormity assessment of 
high-r isk AI syste ms. This paragraph shall not apply in cases where the initial provider has clearly specif ied that its AI 
syste m is not to be changed into a high-r isk AI system and theref ore does not fall under the obligation to hand over the 
documentation.
3. In the case of high-r isk AI syste ms that are safety components of products covered by the Union harmonisation 
legislation listed in Section A of Annex I, the product manufa cturer shall be considered to be the provider of the high-r isk 
AI system, and shall be subject to the obliga tions under Article 16 under either of the follo wing circumstances:
(a)the high-r isk AI syste m is placed on the marke t together with the product under the name or trademark of the product 
manufa cturer;
(b)the high-r isk AI system is put into service under the name or trademark of the product manufacturer after the product 
has been placed on the marke t.
4. The provider of a high-r isk AI system and the third party that supplies an AI syste m, tools, services, components, or 
processes that are used or integrat ed in a high-r isk AI system shall, by written agreement, specify the necessar y information, 
capabilities, technical access and other assistance based on the generally acknowledg ed state of the art, in order to enable 
the provider of the high-r isk AI syste m to fully comply with the obliga tions set out in this Regulation. This paragraph shall 
not apply to third parties making accessible to the public tools, services, processes, or comp onents, other than 
general-pur pose AI models, under a free and open-source licence.
The AI Office may develop and recommend voluntar y model terms for contracts between provid ers of high-r isk AI syste ms 
and third parties that supply tools, services, comp onents or processes that are used for or integrated into high-r isk AI 
syste ms. When developing those voluntar y model terms, the AI Office shall take into account possible contractual 
requirements applicable in specif ic sectors or business cases. The voluntar y model terms shall be published and be available 
free of charg e in an easily usable electronic format.
5. Paragraphs 2 and 3 are without prejudice to the need to obser ve and prot ect intellect ual proper ty rights, confi dential 
business information and trade secrets in accordance with Union and national law.
Article 26
Obligations of deplo yers of high-r isk AI systems
1. Deplo yers of high-r isk AI systems shall take appropr iate technical and organisational measures to ensure they use 
such syste ms in accordance with the instr uctions for use accompan ying the systems, pursuant to paragraphs 3 and 6.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 67/1442. Deplo yers shall assign human oversight to natural persons who have the necessar y compet ence, training and 
author ity, as well as the necessar y suppor t.
3. The obligations set out in paragraphs 1 and 2, are without prejudice to other deplo yer obligations under Union or 
national law and to the deplo yer’s freedom to organise its own resources and activities for the purpose of imp lementing the 
human overs ight measures indicated by the provid er.
4. Without prejudice to paragraphs 1 and 2, to the extent the deplo yer exercises control over the input data, that 
deplo yer shall ensure that input data is relevant and suffi ciently representative in view of the intende d purpose of the 
high-r isk AI syste m.
5. Deplo yers shall monitor the operation of the high-r isk AI syste m on the basis of the instr uctions for use and, where 
relevant, inform provid ers in accordance with Article 72. Where deplo yers have reason to consider that the use of the 
high-r isk AI system in accordance with the instr uctions may result in that AI syste m presenting a risk within the meaning of 
Article 79(1), they shall, without undue dela y, inform the provider or distr ibut or and the relevant market surveillance 
author ity, and shall suspend the use of that system. Where deplo yers have identifie d a serious incident, they shall also 
immediately inform first the provid er, and then the impor ter or distr ibut or and the relevant market surveillance author ities 
of that incident. If the deplo yer is not able to reac h the provider , Article 73 shall apply mutatis mutandis. This obliga tion 
shall not cover sensitive operational data of deplo yers of AI systems which are law enforcement author ities.
For deplo yers that are financ ial institutions subject to requirements regard ing their inter nal gover nance, arrangements or 
processes under Union financial services law, the monitorin g obligation set out in the first subparagraph shall be deemed to 
be fulfilled by complying with the rules on inter nal govern ance arrangements, processes and mechanisms pursuant to the 
relevant financial service law.
6. Deplo yers of high-r isk AI syste ms shall keep the logs auto matically generated by that high-r isk AI system to the exte nt 
such logs are under their control, for a period appropr iate to the intended purpose of the high-r isk AI system, of at least six 
months, unless provid ed other wise in applicable Union or national law, in particular in Union law on the protect ion of 
personal data.
Deplo yers that are financial institutions subject to requirements regard ing their inter nal gove rnance, arrang ements or 
processes under Union financial services law shall maintain the logs as part of the documentation kept pursuant to the 
relevant Union financ ial service law.
7. Before putting into service or using a high-r isk AI syste m at the workplace, deplo yers who are emplo yers shall inform 
workers’ representatives and the affect ed work ers that they will be subject to the use of the high-r isk AI system. This 
information shall be provided, where applicable, in accordance with the rules and procedures laid down in Union and 
national law and practice on information of work ers and their representatives.
8. Deplo yers of high-r isk AI systems that are public author ities, or Union institutions, bodies, offices or agencies shall 
comply with the registration obliga tions referred to in Article 49. When such deplo yers find that the high-r isk AI system 
that they envisage using has not been registered in the EU database refer red to in Article 71, they shall not use that system 
and shall inform the provider or the distr ibutor .
9. Where applicable, deplo yers of high-r isk AI systems shall use the information provid ed under Article 13 of this 
Regulation to comp ly with their obligation to carry out a data protection imp act assessment under Article 35 of Regulation 
(EU) 2016/679 or Article 27 of Directive (EU) 2016/680.
10. Without prejudice to Directive (EU) 2016/680, in the framew ork of an investigation for the targe ted search of 
a person suspect ed or convict ed of having committed a criminal offence, the deplo yer of a high-r isk AI syste m for 
post-remote biometr ic identifica tion shall request an author isation, ex ante, or without undue dela y and no later than 48 
hours, by a judicial author ity or an administrative author ity whose decision is binding and subject to judicial review , for the 
use of that syste m, excep t when it is used for the initial identifica tion of a potential suspect based on objective and verifiable 
facts directly linked to the offence. Each use shall be limited to what is strictly necessar y for the invest igation of a specific 
criminal offence.
If the author isation requested pursuant to the first subparagraph is rejected, the use of the post-remote biometr ic 
identifica tion syste m linked to that requested author isation shall be stopped with immediate effect and the personal data 
linked to the use of the high-r isk AI system for which the author isation was request ed shall be delete d.EN OJ L, 12.7.2024
68/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojIn no case shall such high-r isk AI syste m for post-remote biometr ic identifica tion be used for law enforcement purposes in 
an untarget ed way, without any link to a criminal offence, a criminal proceeding, a genuine and present or genuine and 
foreseeable threat of a criminal offenc e, or the searc h for a specif ic missing person. It shall be ensured that no decision that 
produces an adverse legal effect on a person may be taken by the law enforcement author ities based solely on the output of 
such post-remote biometr ic identifi cation syste ms.
This paragraph is without prejudice to Article 9 of Regulation (EU) 2016/679 and Article 10 of Directive (EU) 2016/680 
for the processing of biometr ic data.
Regardless of the purpose or deplo yer, each use of such high-r isk AI syste ms shall be documented in the relevant police file 
and shall be made available to the relevant marke t surveillance author ity and the national data protection author ity upon 
request, excluding the disclosure of sensitive operational data related to law enforcement. This subparagraph shall be 
without prejudice to the powers confe rred by Directive (EU) 2016/680 on super visor y author ities.
Deplo yers shall submit annual repor ts to the relevant marke t surveillance and national data prot ection author ities on their 
use of post-remote biometr ic identifica tion systems, excluding the disclosure of sensitive operational data related to law 
enforcement. The repor ts may be aggrega ted to cover more than one deplo yment.
Member States may introduce, in accordance with Union law, more restr ictive laws on the use of post-remote biometr ic 
identifica tion systems.
11. Without prejudice to Article 50 of this Regulation, deplo yers of high-r isk AI systems referred to in Annex III that 
make decisions or assist in making decisions relate d to natural persons shall inform the natural persons that they are subject 
to the use of the high-r isk AI syste m. For high-r isk AI systems used for law enforcement purposes Article 13 of Directive 
(EU) 2016/680 shall apply .
12. Deplo yers shall cooperate with the relevant compet ent author ities in any action those author ities take in relation to 
the high-r isk AI system in order to imp lement this Regulation.
Article 27
Fundament al rights impact assessment for high-r isk AI systems
1. Prior to deplo ying a high-r isk AI syste m referred to in Article 6(2), with the excep tion of high-r isk AI syste ms 
intende d to be used in the area listed in point 2 of Annex III, deplo yers that are bodies gove rned by public law, or are private 
entities provid ing public services, and deplo yers of high-r isk AI syste ms refer red to in points 5 (b) and (c) of Annex III, shall 
perfo rm an assessment of the impact on fundamental rights that the use of such syste m may produce. For that purpose, 
deplo yers shall perfo rm an assessment consisting of:
(a)a descr iption of the deplo yer’s processes in which the high-r isk AI system will be used in line with its intende d purpose;
(b)a descr iption of the period of time within which, and the frequency with which, each high-r isk AI system is intended to 
be used;
(c)the cate gories of natural persons and groups likely to be affected by its use in the specif ic cont ext;
(d)the specif ic risks of harm likely to have an imp act on the categori es of natural persons or groups of persons identifie d 
pursuant to point (c) of this paragraph, taking into account the information given by the provider pursuant to 
Article 13;
(e)a descr iption of the imp lementation of human oversight measures, according to the instr uctions for use;
(f)the measures to be taken in the case of the materialis ation of those risks, including the arrang ements for internal 
governance and comp laint mec hanisms.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 69/1442. The obligation laid down in paragraph 1 applies to the first use of the high-r isk AI system. The deplo yer may, in 
similar cases, rely on previously conducted fundamental rights imp act assessments or existing impact assessments carried 
out by provid er. If, during the use of the high-r isk AI system, the deplo yer considers that any of the elements listed in 
paragraph 1 has changed or is no longe r up to date, the deplo yer shall take the necessar y steps to update the information.
3. Once the assessment refer red to in paragraph 1 of this Article has been perf ormed, the deplo yer shall notify the 
marke t surveillance author ity of its results, submitting the filled- out template refer red to in paragraph 5 of this Article as 
part of the notification. In the case refer red to in Article 46(1), deplo yers may be exem pt from that obligation to notify .
4. If any of the obliga tions laid down in this Article is already met through the data protection impact assessment 
conducted pursuant to Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamental 
rights imp act assessment referred to in paragraph 1 of this Article shall comp lement that data protection impact 
assessment.
5. The AI Office shall develop a templat e for a questionnaire, including through an auto mated tool, to facilitat e deplo yers 
in com plying with their obliga tions under this Article in a simplified manner .
SECTION 4
Notifying author ities and notified bodies
Article 28
Notifying author ities
1. Each Member State shall designate or establish at least one notifying author ity responsible for setting up and carrying 
out the necessar y procedures for the assessment, designation and notif ication of conf ormity assessment bodies and for their 
monitoring. Those procedures shall be developed in cooperation between the notifying author ities of all Member States.
2. Member States may decide that the assessment and monitoring referred to in paragraph 1 is to be carried out by 
a national accreditation body within the meaning of, and in accordance with, Regulation (EC) No 765/2008.
3. Notifying author ities shall be established, organised and operated in such a way that no conf lict of interest arises with 
conf ormity assessment bodies, and that the objectivity and impar tiality of their activities are safeguarded.
4. Notifying author ities shall be organi sed in such a way that decisions relating to the notification of conf ormity 
assessment bodies are taken by compet ent persons diffe rent from those who carried out the assessment of those bodies.
5. Notifying author ities shall offer or provide neither any activities that conf ormity assessment bodies perform , nor any 
consultancy services on a commercial or competitive basis.
6. Notifying author ities shall safeguard the confidentiality of the information that they obtain, in accordance with 
Article 78.
7. Notifying author ities shall have an adequate number of com petent personnel at their disposal for the proper 
perfo rmance of their tasks. Comp etent personnel shall have the necessar y exper tise, where applicable, for their function, in 
fields such as information technologi es, AI and law, including the super vision of fundamental rights.
Article 29
Application of a confor mity assessment body for notif ication
1. Conf ormity assessment bodies shall submit an application for notification to the notifying author ity of the Member 
State in which they are established.EN OJ L, 12.7.2024
70/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The application for notif ication shall be accompanied by a descr iption of the conf ormity assessment activities, the 
conf ormity assessment module or modules and the types of AI syste ms for which the conf ormity assessment body claims 
to be compet ent, as well as by an accreditation certificate, where one exists, issued by a national accreditation body attesting 
that the conf ormity assessment body fulfils the requirements laid down in Article 31.
Any valid document related to existing designations of the applicant notified body under any other Union harmonisation 
legislation shall be added.
3. Where the conf ormity assessment body concer ned cannot provide an accreditation certificate, it shall provid e the 
notifying author ity with all the documentar y evidence necessar y for the verification, recognition and regular monitoring of 
its comp liance with the requirements laid down in Article 31.
4. For notif ied bodies which are designate d under any other Union harmonisation legislation, all documents and 
certificates linked to those designations may be used to suppor t their designation procedure under this Regulation, as 
appropr iate. The notified body shall update the documentation refer red to in paragraphs 2 and 3 of this Article whenever 
relevant changes occur , in order to enable the author ity responsible for notified bodies to monitor and verify continuous 
complia nce with all the requirements laid down in Article 31.
Article 30
Notif ication procedure
1. Notifying author ities may notify only conf ormity assessment bodies which have satisfied the requirements laid down 
in Article 31.
2. Notifying author ities shall notify the Commission and the other Member States, using the electronic notification tool 
developed and managed by the Commission, of each conf ormity assessment body refer red to in paragraph 1.
3. The notification refer red to in paragraph 2 of this Article shall include full details of the conf ormity assessment 
activities, the conf ormity assessment module or modules, the types of AI syste ms concer ned, and the relevant attestation of 
compet ence. Where a notification is not based on an accreditation certificate as refer red to in Article 29(2), the notifying 
author ity shall provide the Commission and the other Member States with documentar y evidence which attests to the 
compet ence of the conf ormity assessment body and to the arrang ements in place to ensure that that body will be 
monitor ed regularly and will continue to satisfy the requirements laid down in Article 31.
4. The conf ormity assessment body concer ned may perform the activities of a notified body only where no objections 
are raised by the Commission or the other Member States within two weeks of a notification by a notifying author ity where 
it includes an accreditation certificate refer red to in Article 29(2), or within two months of a notification by the notifying 
author ity where it includes documentar y evidence refer red to in Article 29(3).
5. Where objections are raised, the Commission shall, without dela y, enter into consultations with the relevant Member 
States and the conf ormity assessment body . In view thereof, the Commission shall decide whether the author isation is 
justified. The Commission shall address its decision to the Member State concer ned and to the relevant conf ormity 
assessment body .
Article 31
Requirements relating to notif ied bodies
1. A notif ied body shall be established under the national law of a Member State and shall have legal personality .
2. Notifi ed bodies shall satisfy the organisational, quality managem ent, resources and process requirements that are 
necessar y to fulfil their tasks, as well as suitable cybersecur ity requirements.
3. The organisational structure, allocation of responsibilities, repor ting lines and operation of notif ied bodies shall 
ensure confi dence in their perf ormance, and in the results of the conf ormity assessment activities that the notif ied bodies 
conduct.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 71/1444. Notifi ed bodies shall be independent of the provid er of a high-r isk AI syste m in relation to which they perform 
conf ormity assessment activities. Notifi ed bodies shall also be independent of any other operat or having an economic 
intere st in high-r isk AI syste ms assessed, as well as of any competit ors of the provider . This shall not preclude the use of 
assessed high-r isk AI syste ms that are necessar y for the operations of the conf ormity assessment body , or the use of such 
high-r isk AI syste ms for personal purposes.
5. Neither a conf ormity assessment body , its top-level managem ent nor the personnel responsible for carrying out its 
conf ormity assessment tasks shall be directly involved in the design, development, marke ting or use of high-r isk AI systems, 
nor shall they represent the parties engag ed in those activities. They shall not enga ge in any activity that might conf lict with 
their independence of judgement or integrity in relation to conf ormity assessment activities for which they are notified. 
This shall, in particular , apply to consultancy services.
6. Notifi ed bodies shall be organised and operat ed so as to safeguard the independence, objectivity and imp artiality of 
their activities. Notifi ed bodies shall document and implement a structure and procedures to safeguard impartiality and to 
promot e and apply the principles of imp artiality throughout their organisation, personnel and assessment activities.
7. Notifi ed bodies shall have documented procedures in place ensur ing that their personnel, committ ees, subsidiar ies, 
subcontractors and any associated body or personnel of exte rnal bodies maintain, in accordance with Article 78, the 
confi dentiality of the information which comes into their possession during the perf ormance of conf ormity assessment 
activities, except when its disclosure is required by law. The staff of notified bodies shall be bound to obser ve profe ssional 
secrecy with regard to all information obtained in carrying out their tasks under this Regulation, excep t in relation to the 
notifying author ities of the Member State in which their activities are carried out.
8. Notifi ed bodies shall have procedures for the perf ormance of activities which take due account of the size of 
a provid er, the sector in which it operat es, its structure, and the degree of comp lexity of the AI system concer ned.
9. Notifi ed bodies shall take out appropr iate liability insurance for their conf ormity assessment activities, unless liability 
is assumed by the Member State in which they are established in accordance with national law or that Member State is itself 
directly responsible for the conf ormity assessment.
10. Notifi ed bodies shall be capable of carrying out all their tasks under this Regulation with the highest degree of 
profe ssional integrity and the requisite comp etence in the specif ic field, whether those tasks are carried out by notif ied 
bodies themselves or on their behalf and under their responsibility .
11. Notifi ed bodies shall have sufficient internal compet ences to be able effectively to evaluate the tasks conducted by 
external parties on their behalf. The notif ied body shall have permanent availability of sufficient administrative, technical, 
lega l and scientific personnel who possess exper ience and knowledg e relating to the relevant types of AI syste ms, data and 
data computing , and relating to the requirements set out in Section 2.
12. Notifi ed bodies shall participate in coordination activities as refer red to in Article 38. They shall also take part 
directly , or be represente d in, European standardisation organi sations, or ensure that they are aware and up to date in 
respect of relevant standards.
Article 32
Presumption of confor mity with requirements relating to notif ied bodies
Where a conf ormity assessment body demonstrates its conf ormity with the criteria laid down in the relevant harmonised 
standards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall 
be presumed to comply with the requirements set out in Article 31 in so far as the applicable harmonised standards cover 
those requirements.EN OJ L, 12.7.2024
72/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojArticle 33
Subsidiar ies of notif ied bodies and subcontracting
1. Where a notif ied body subcontracts specif ic tasks connect ed with the conf ormity assessment or has recourse to 
a subsidiar y, it shall ensure that the subcontractor or the subsidiar y meets the requirements laid down in Article 31, and 
shall inform the notifying author ity according ly.
2. Notifi ed bodies shall take full responsibility for the tasks perf ormed by any subcontract ors or subsidiar ies.
3. Activities may be subcontract ed or carried out by a subsidiar y only with the agreement of the provider . Notified 
bodies shall mak e a list of their subsidiar ies publicly available.
4. The relevant documents concer ning the assessment of the qualific ations of the subcontract or or the subsidiar y and 
the work carried out by them under this Regulation shall be kept at the disposal of the notifying author ity for a period of 
five years from the termination date of the subcontracting.
Article 34
Operational obligations of notif ied bodies
1. Notifi ed bodies shall verify the conf ormity of high-r isk AI systems in accordance with the conf ormity assessment 
procedures set out in Article 43.
2. Notifi ed bodies shall avoid unnecessar y burdens for providers when perf orming their activities, and take due account 
of the size of the provid er, the secto r in which it operat es, its structure and the degree of complexity of the high-r isk AI 
syste m concer ned, in particular in view of minimising administrative burdens and compliance costs for micro- and small 
enterprises within the meaning of Recommendation 2003/361/EC. The notif ied body shall, never theless, respect the degree 
of rigour and the level of prot ection required for the comp liance of the high-r isk AI system with the requirements of this 
Regulation.
3. Notifi ed bodies shall make available and submit upon request all relevant documentation, including the provider s’ 
documentation, to the notifying author ity referred to in Article 28 to allow that author ity to conduct its assessment, 
designation, notification and monitoring activities, and to facilitate the assessment outlined in this Section.
Article 35
Identif ication numbers and lists of notif ied bodies
1. The Commission shall assign a sing le identification number to each notif ied body , even where a body is notified under 
more than one Union act.
2. The Commission shall mak e publicly available the list of the bodies notif ied under this Regulation, including their 
identifica tion numbers and the activities for which they have been notif ied. The Commission shall ensure that the list is kept 
up to date.
Article 36
Changes to notif ications
1. The notifying author ity shall notify the Commission and the other Member States of any relevant changes to the 
notif ication of a notif ied body via the electronic notification tool refer red to in Article 30(2).
2. The procedures laid down in Articles 29 and 30 shall apply to extensi ons of the scope of the notif ication.
For changes to the notif ication other than extensi ons of its scope, the procedures laid down in paragraphs (3) to (9) shall 
apply .OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 73/1443. Where a notif ied body decides to cease its conf ormity assessment activities, it shall inform the notifying author ity and 
the provid ers concer ned as soon as possible and, in the case of a planned cessation, at least one year before ceasing its 
activities. The certificates of the notified body may remain valid for a period of nine months after cessation of the notified 
body’s activities, on condition that another notified body has confi rmed in writing that it will assume responsibilities for the 
high-r isk AI syste ms covered by those certificates. The latte r notified body shall complet e a full assessment of the high-r isk 
AI syste ms affect ed by the end of that nine-month-per iod before issuing new certificates for those systems. Where the 
notif ied body has ceased its activity , the notifying author ity shall withdra w the designation.
4. Where a notifying author ity has suffici ent reason to consider that a notified body no longer meets the requirements 
laid down in Article 31, or that it is failing to fulfil its obliga tions, the notifying author ity shall without dela y investig ate the 
matt er with the utmost diligence. In that context, it shall inform the notif ied body concer ned about the objections raised 
and give it the possibility to mak e its views known. If the notifying author ity comes to the conclusion that the notified body 
no longer meets the requirements laid down in Article 31 or that it is failing to fulfil its obligations, it shall restr ict, suspend 
or withdra w the designation as appropr iate, depending on the seriousness of the failure to meet those requirements or fulfil 
those obliga tions. It shall immediately inform the Commission and the other Member States according ly.
5. Where its designation has been suspended, restr icted, or fully or partially withdra wn, the notif ied body shall inform 
the provid ers concer ned within 10 days.
6. In the event of the restr iction, suspension or withdrawal of a designation, the notifying author ity shall take 
appropr iate steps to ensure that the files of the notif ied body concer ned are kept, and to make them available to notifying 
author ities in other Member States and to mark et surveillance author ities at their request.
7. In the event of the restr iction, suspension or withdra wal of a designation, the notifying author ity shall:
(a)assess the imp act on the certificates issued by the notif ied body ;
(b)submit a repor t on its findings to the Commission and the other Member States within three months of having notified 
the change s to the designation;
(c)require the notif ied body to suspend or withdra w, within a reasonable period of time determined by the author ity, any 
certificates which were unduly issued, in order to ensure the continuing conf ormity of high-r isk AI systems on the 
marke t;
(d)inform the Commission and the Member States about certificates the suspension or withdra wal of which it has required;
(e)provid e the national compet ent author ities of the Member State in which the provider has its regist ered place of 
business with all relevant information about the certificates of which it has required the suspension or withdra wal; that 
author ity shall take the appropr iate measures, where necessar y, to avoid a pote ntial risk to health, safety or fundamental 
rights.
8. With the exception of certificates unduly issued, and where a designation has been suspended or restr icted, the 
certificates shall remain valid in one of the following circumstances:
(a)the notifying author ity has conf irmed, within one month of the suspension or restr iction, that there is no risk to health, 
safety or fundamental rights in relation to certificates affect ed by the suspension or restr iction, and the notifying 
author ity has outlined a timeline for actions to remedy the suspension or restr iction; or
(b)the notifying author ity has confir med that no certificates relevant to the suspension will be issued, amended or re-issued 
during the course of the suspension or restr iction, and states whether the notif ied body has the capability of continuing 
to monitor and remain responsible for existing certificates issued for the period of the suspension or restr iction; in the 
event that the notifying author ity determ ines that the notif ied body does not have the capability to suppor t existing 
certificates issued, the provider of the system cover ed by the certificate shall conf irm in writing to the national 
compet ent author ities of the Member State in which it has its registered place of business, within three months of the 
suspension or restr iction, that another qualif ied notif ied body is temporar ily assuming the functions of the notif ied 
body to monitor and remain responsible for the certificates during the period of suspension or restr iction.EN OJ L, 12.7.2024
74/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj9. With the excep tion of certificates unduly issued, and where a designation has been withdra wn, the certificates shall 
remain valid for a period of nine months under the follo wing circumstances:
(a)the national compet ent author ity of the Member State in which the provider of the high-r isk AI syste m cover ed by the 
certificate has its registered place of business has confi rmed that there is no risk to health, safety or fundamental rights 
associate d with the high-r isk AI systems concer ned; and
(b)another notified body has confir med in writing that it will assume immediate responsibility for those AI syste ms and 
complet es its assessment within 12 months of the withdra wal of the designation.
In the circumstances referred to in the first subparagraph, the national comp etent author ity of the Member State in which 
the provid er of the syste m covered by the certificate has its place of business may exte nd the provisional validity of the 
certificates for additional periods of three months, which shall not exceed 12 months in total.
The national compet ent author ity or the notif ied body assuming the functions of the notified body affect ed by the chang e 
of designation shall immediate ly inform the Commission, the other Member States and the other notif ied bodies thereof.
Article 37
Challenge to the competence of notif ied bodies
1. The Commission shall, where necessar y, investigat e all cases where there are reasons to doubt the comp etence of 
a notified body or the continued fulfilment by a notif ied body of the requirements laid down in Article 31 and of its 
applicable responsibilities.
2. The notifying author ity shall provid e the Commission, on request, with all relevant information relating to the 
notif ication or the maintenance of the comp etence of the notified body concer ned.
3. The Commission shall ensure that all sensitive information obtained in the course of its investigations pursuant to this 
Article is treate d confidentially in accordance with Article 78.
4. Where the Commission ascer tains that a notif ied body does not meet or no longer meets the requirements for its 
notif ication, it shall inform the notifying Member State according ly and request it to take the necessar y corrective measures, 
including the suspension or withdra wal of the notif ication if necessar y. Where the Member State fails to take the necessar y 
corrective measures, the Commission may, by means of an imp lementing act, suspend, restr ict or withdraw the designation. 
That implementing act shall be adop ted in accordance with the examination procedure refer red to in Article 98(2).
Article 38
Coordination of notif ied bodies
1. The Commission shall ensure that, with regard to high-r isk AI systems, appropr iate coordination and cooperation 
between notified bodies active in the conf ormity assessment procedures pursuant to this Regulation are put in place and 
properly operat ed in the form of a sectoral group of notified bodies.
2. Each notifying author ity shall ensure that the bodies notified by it participate in the work of a group refer red to in 
paragraph 1, directly or through designated representatives.
3. The Commission shall provid e for the exchange of kno wledge and best practices between notifying author ities.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 75/144Article 39
Conf ormity assessment bodies of third countr ies
Conf ormity assessment bodies established under the law of a third countr y with which the Union has concluded an 
agreement may be author ised to carry out the activities of notified bodies under this Regulation, provided that they meet 
the requirements laid down in Article 31 or they ensure an equivalent level of compliance.
SECTION 5
Standar ds, conf ormity assessment, certificates, registr ation
Article 40
Har monised standards and standardisation deliv erables
1. High-r isk AI syste ms or general-pur pose AI models which are in conf ormity with harmonised standards or parts 
thereof the references of which have been published in the Official Journal of the European Union in accordance with 
Regulation (EU) No 1025/2012 shall be presumed to be in conf ormity with the requirements set out in Section 2 of this 
Chapt er or, as applicable, with the obliga tions set out in of Chapt er V, Sections 2 and 3, of this Regulation, to the extent that 
those standards cover those requirements or obliga tions.
2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue dela y, 
standardisation requests coveri ng all requirements set out in Section 2 of this Chap ter and, as applicable, standardisation 
requests coverin g obliga tions set out in Chapt er V, Sections 2 and 3, of this Regulation. The standardisation request shall 
also ask for deliverables on repor ting and documentation processes to impro ve AI systems’ resource perf ormance, such as 
reducing the high-r isk AI system’s consump tion of energy and of other resources during its lifecy cle, and on the 
energy-eff icient development of general-pur pose AI models. When prepar ing a standardisation request, the Commission 
shall consult the Board and relevant stakeh olders, including the advisor y forum.
When issuing a standardisation request to European standardisation organisations, the Commission shall specify that 
standards have to be clear , consiste nt, including with the standards developed in the various sectors for products covered by 
the existing Union harmonisation legislation listed in Annex I, and aiming to ensure that high-r isk AI syste ms or 
general-pur pose AI models placed on the marke t or put into service in the Union meet the relevant requirements or 
obliga tions laid down in this Regulation.
The Commission shall request the European standardisation organisations to provide evidence of their best efforts to fulfil 
the objectives refer red to in the first and the second subparagraph of this paragraph in accordance with Article 24 of 
Regulation (EU) No 1025/2012.
3. The participants in the standardisation process shall seek to promot e invest ment and inno vation in AI, including 
through increasing legal certainty , as well as the comp etitiveness and growth of the Union marke t, to contr ibut e to 
strengthening global cooperation on standardisation and taking into account existing intern ational standards in the field of 
AI that are consiste nt with Union values, fundamental rights and interests, and to enhance multi-stake holder gove rnance 
ensur ing a balanced representation of interests and the effective participation of all relevant stak eholders in accordance with 
Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.
Article 41
Common specif ications
1. The Commission may adopt, imp lementing acts establishing common specifications for the requirements set out in 
Section 2 of this Chapt er or, as applicable, for the obligations set out in Sections 2 and 3 of Chapt er V where the following 
conditions have been fulfilled:
(a)the Commission has requested, pursuant to Article 10(1) of Regulation (EU) No 1025/2012, one or more European 
standardisation organisations to draf t a harmonised standard for the requirements set out in Section 2 of this Chapt er, 
or, as applicable, for the obliga tions set out in Sections 2 and 3 of Chap ter V, and:
(i)the request has not been accept ed by any of the European standardisation organisations; orEN OJ L, 12.7.2024
76/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(ii)the harmonised standards addressing that request are not delivered within the deadline set in accordance with 
Article 10(1) of Regulation (EU) No 1025/2012; or
(iii) the relevant harmonised standards insuff iciently address fundamental rights concer ns; or
(iv) the harmonised standards do not com ply with the request ; and
(b)no refere nce to harmonised standards covering the requirements refer red to in Section 2 of this Chapt er or, as 
applicable, the oblig ations referred to in Sections 2 and 3 of Chap ter V has been published in the Official Journal of the 
Europe an Union in accordance with Regulation (EU) No 1025/2012, and no such reference is expect ed to be published 
within a reasonable period.
When draf ting the common specif ications, the Commission shall consult the advisor y forum refer red to in Article 67.
The implementing acts referred to in the first subparagraph of this paragraph shall be adop ted in accordance with the 
examination procedure referred to in Article 98(2).
2. Before prepar ing a draf t implementing act, the Commission shall inform the committee referred to in Article 22 of 
Regulation (EU) No 1025/2012 that it considers the conditions laid down in paragraph 1 of this Article to be fulfilled.
3. High-r isk AI syste ms or general-pur pose AI models which are in conf ormity with the common specific ations referred 
to in paragraph 1, or parts of those specifications, shall be presumed to be in conf ormity with the requirements set out in 
Section 2 of this Chapt er or, as applicable, to comp ly with the oblig ations refer red to in Sections 2 and 3 of Chapt er V, to 
the exte nt those common specifications cover those requirements or those obligations.
4. Where a harmonised standard is adop ted by a European standardisation organisation and proposed to the 
Commission for the publication of its reference in the Official Journal of the European Union, the Commission shall assess the 
harmonised standard in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised standard is 
published in the Official Journal of the European Union, the Commission shall repeal the imp lementing acts refer red to in 
paragraph 1, or parts thereof which cover the same requirements set out in Section 2 of this Chap ter or, as applicable, the 
same obliga tions set out in Sections 2 and 3 of Chap ter V.
5. Where provider s of high-r isk AI syste ms or general-pur pose AI models do not comply with the common 
specific ations referred to in paragraph 1, they shall duly justify that they have adopt ed technical solutions that meet the 
requirements refer red to in Section 2 of this Chapt er or, as applicable, comply with the obligations set out in Sections 2 and 
3 of Chapt er V to a level at least equivalent thereto.
6. Where a Member State considers that a common specification does not entirely meet the requirements set out in 
Section 2 or, as applicable, com ply with obliga tions set out in Sections 2 and 3 of Chapt er V, it shall inform the 
Commission thereof with a detailed explanation. The Commission shall assess that information and, if appropr iate, amend 
the implementing act establishing the common specif ication concer ned.
Article 42
Presumption of confor mity with certain requirements
1. High-r isk AI systems that have been trained and tested on data reflecting the specif ic geographical, behavi oural, 
cont extual or functional setting within which they are intended to be used shall be presumed to comply with the relevant 
requirements laid down in Article 10(4).
2. High-r isk AI syste ms that have been certified or for which a statement of conf ormity has been issued under 
a cybersecur ity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the 
Official Journal of the European Union shall be presumed to comply with the cybersecur ity requirements set out in Article 15 
of this Regulation in so far as the cybersecur ity certificate or statem ent of conf ormity or parts thereof cover those 
requirements.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 77/144Article 43
Conformit y assessment
1. For high-r isk AI systems listed in point 1 of Annex III, where, in demonstrating the complia nce of a high-r isk AI 
syste m with the requirements set out in Section 2, the provid er has applied harmonised standards refer red to in Article 40, 
or, where applicable, common specifications refer red to in Article 41, the provider shall opt for one of the following 
conf ormity assessment procedures based on:
(a)the internal control refer red to in Annex VI; or
(b)the assessment of the quality management syste m and the assessment of the technical documentation, with the 
involvement of a notified body , referred to in Annex VII.
In demonstrating the compliance of a high-r isk AI system with the requirements set out in Section 2, the provid er shall 
follo w the conf ormity assessment procedure set out in Annex VII where:
(a)harmonised standards refer red to in Article 40 do not exist, and common specifications referred to in Article 41 are not 
available;
(b)the provid er has not applied, or has applied only part of, the harmonised standard;
(c)the common specifications refer red to in point (a) exist, but the provider has not applied them;
(d)one or more of the harmonised standards referred to in point (a) has been published with a restr iction, and only on the 
part of the standard that was restr icted.
For the purposes of the conf ormity assessment procedure refer red to in Annex VII, the provider may choose any of the 
notif ied bodies. How ever , where the high-r isk AI syste m is intended to be put into service by law enforcement, immigration 
or asylum author ities or by Union institutions, bodies, offices or agencies, the mark et surveillance author ity refer red to in 
Article 74(8) or (9), as applicable, shall act as a notified body .
2. For high-r isk AI systems refer red to in points 2 to 8 of Annex III, provider s shall follow the conf ormity assessment 
procedure based on inter nal control as referred to in Annex VI, which does not provide for the involvement of a notif ied 
body .
3. For high-r isk AI systems cover ed by the Union harmonisation legislation listed in Section A of Annex I, the provid er 
shall follo w the relevant conf ormity assessment procedure as required under those legal acts. The requirements set out in 
Section 2 of this Chapt er shall apply to those high-r isk AI syste ms and shall be part of that assessment. Points 4.3., 4.4., 4.5. 
and the fifth paragraph of point 4.6 of Annex VII shall also apply .
For the purposes of that assessment, notif ied bodies which have been notif ied under those legal acts shall be entitled to 
control the conf ormity of the high-r isk AI syste ms with the requirements set out in Section 2, provided that the compliance 
of those notif ied bodies with requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the context of 
the notification procedure under those lega l acts.
Where a legal act listed in Section A of Annex I enables the product manufa cturer to opt out from a third-par ty conf ormity 
assessment, provided that that manufacture r has applied all harmonised standards coveri ng all the relevant requirements, 
that manufa cturer may use that option only if it has also applied harmonised standards or, where applicable, common 
specific ations referred to in Article 41, covering all requirements set out in Section 2 of this Chapt er.
4. High-r isk AI systems that have already been subject to a conf ormity assessment procedure shall undergo a new 
conf ormity assessment procedure in the event of a substantial modification, regardless of whether the modified syste m is 
intende d to be further distr ibut ed or continues to be used by the current deplo yer.
For high-r isk AI systems that continue to learn after being placed on the marke t or put into service, chang es to the high-r isk 
AI syste m and its perfo rmance that have been pre-dete rmined by the provid er at the moment of the initial conf ormity 
assessment and are part of the information contained in the technical documentation refer red to in point 2(f) of Annex IV, 
shall not constitute a substantial modifi cation.
5. The Commission is empo wered to adop t deleg ated acts in accordance with Article 97 in order to amend Annex es VI 
and VII by updating them in light of technical progress.EN OJ L, 12.7.2024
78/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj6. The Commission is empo wered to adop t deleg ated acts in accordance with Article 97 in order to amend paragraphs 1 
and 2 of this Article in order to subject high-r isk AI syste ms referred to in points 2 to 8 of Annex III to the conf ormity 
assessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegat ed acts taking into 
account the effectiveness of the conf ormity assessment procedure based on internal control referred to in Annex VI in 
preventing or minimising the risks to health and safety and prot ection of fundamental rights posed by such syste ms, as well 
as the availability of adequat e capacities and resources among notified bodies.
Article 44
Cer tificates
1. Certificates issued by notif ied bodies in accordance with Annex VII shall be drawn-up in a languag e which can be 
easily understood by the relevant author ities in the Member State in which the notified body is established.
2. Certificates shall be valid for the period they indicate , which shall not exceed five years for AI systems covered by 
Annex I, and four years for AI syste ms covered by Annex III. At the request of the provider , the validity of a certificate may 
be exte nded for further periods, each not exceeding five years for AI systems covered by Annex I, and four years for AI 
syste ms cover ed by Annex III, based on a re-assessment in accordance with the applicable conf ormity assessment 
procedures. Any supplement to a certificate shall remain valid, provided that the certificate which it supplements is valid.
3. Where a notified body finds that an AI syste m no longer meets the requirements set out in Section 2, it shall, taking 
account of the principle of propor tionality , suspend or withdra w the certificate issued or impose restr ictions on it, unless 
complia nce with those requirements is ensured by appropr iate corrective action take n by the provider of the system within 
an appropr iate deadline set by the notif ied body . The notif ied body shall give reasons for its decision.
An appeal procedure against decisions of the notified bodies, including on conf ormity certificates issued, shall be available .
Article 45
Infor mation obligations of notif ied bodies
1. Notifi ed bodies shall inform the notifying author ity of the follo wing:
(a)any Union technical documentation assessment certificates, any supplements to those certificates, and any quality 
managem ent syste m approva ls issued in accordance with the requirements of Annex VII;
(b)any refusal, restr iction, suspension or withdra wal of a Union technical documentation assessment certificate or a quality 
managem ent syste m approva l issued in accordance with the requirements of Annex VII;
(c)any circumstances affecting the scope of or conditions for notif ication;
(d)any request for information which they have received from marke t surveillance author ities regarding conf ormity 
assessment activities;
(e)on request, conf ormity assessment activities perf ormed within the scope of their notif ication and any other activity 
perfo rmed, including cross-border activities and subcontracting.
2. Each notif ied body shall inform the other notif ied bodies of:
(a)quality managem ent system approva ls which it has refused, suspended or withdra wn, and, upon request, of quality 
syste m approva ls which it has issued;
(b)Union technical documentation assessment certificates or any supplements thereto which it has refused, withdra wn, 
suspended or other wise restr icted, and, upon request, of the certificates and/or supplements thereto which it has issued.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 79/1443. Each notif ied body shall provide the other notified bodies carrying out similar conf ormity assessment activities 
covering the same types of AI systems with relevant information on issues relating to negative and, on request, positive 
conf ormity assessment results.
4. Notifi ed bodies shall safegua rd the confi dentiality of the information that they obtain, in accordance with Article 78.
Article 46
Derogation from confor mity assessment procedure
1. By way of derogat ion from Article 43 and upon a duly justified request, any market surveillance author ity may 
author ise the placing on the marke t or the putting into service of specific high-r isk AI systems within the territory of the 
Member State concer ned, for exceptiona l reasons of public secur ity or the prot ection of life and health of persons, 
envir onmental protection or the prot ection of key industr ial and infrastr uctural assets. That author isation shall be for 
a limited period while the necessar y conf ormity assessment procedures are being carried out, taking into account the 
exceptional reasons justifying the deroga tion. The completion of those procedures shall be under taken without undue dela y.
2. In a duly justifi ed situation of urgency for excep tional reasons of public secur ity or in the case of specif ic, substantial 
and imminent threat to the life or physica l safety of natural persons, law-enf orcement author ities or civil prot ection 
author ities may put a specif ic high-r isk AI system into service without the author isation referred to in paragraph 1, 
provid ed that such author isation is request ed during or after the use without undue dela y. If the author isation referred to in 
paragraph 1 is refused, the use of the high-r isk AI system shall be stopp ed with immediate effect and all the results and 
outputs of such use shall be immediately discarded.
3. The author isation referred to in paragraph 1 shall be issued only if the marke t surveillance author ity concludes that 
the high-r isk AI syste m comp lies with the requirements of Section 2. The marke t surveillance author ity shall inform the 
Commission and the other Member States of any author isation issued pursuant to paragraphs 1 and 2. This obligation shall 
not cover sensitive operational data in relation to the activities of law-enf orcement author ities.
4. Where, within 15 calendar days of receipt of the information refer red to in paragraph 3, no objection has been raised 
by either a Member State or the Commission in respect of an author isation issued by a marke t surveillance author ity of 
a Member State in accordance with paragraph 1, that author isation shall be deemed justified.
5. Where, within 15 calendar days of receipt of the notification referred to in paragraph 3, objections are raised by 
a Member State against an author isation issued by a mark et surveillance author ity of another Member State, or where the 
Commission considers the author isation to be contrar y to Union law, or the conclusion of the Member States rega rding the 
complia nce of the syste m as referred to in paragraph 3 to be unfounded, the Commission shall, without dela y, enter into 
consultations with the relevant Member State. The operat ors concer ned shall be consulted and have the possibility to 
present their views. Having regard thereto, the Commission shall decide whether the author isation is justified. The 
Commission shall address its decision to the Member State concer ned and to the relevant operato rs.
6. Where the Commission considers the author isation unjustified, it shall be withdra wn by the marke t surveillance 
author ity of the Member State concer ned.
7. For high-r isk AI syste ms related to products covered by Union harmonisation legislation listed in Section A of 
Annex I, only the derogat ions from the conf ormity assessment established in that Union harmonisation legislation shall 
apply .
Article 47
EU declaration of confor mity
1. The provider shall draw up a written machine readable, physical or electronically signed EU declaration of conf ormity 
for each high-r isk AI system, and keep it at the disposal of the national comp etent author ities for 10 years after the 
high-r isk AI syste m has been placed on the mark et or put into service. The EU declaration of conf ormity shall identify the 
high-r isk AI system for which it has been drawn up. A copy of the EU declaration of conf ormity shall be submitt ed to the 
relevant national comp etent author ities upon request.EN OJ L, 12.7.2024
80/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The EU declaration of conf ormity shall state that the high-r isk AI system concer ned meets the requirements set out in 
Section 2. The EU declaration of conf ormity shall contain the information set out in Annex V, and shall be translated into 
a language that can be easily understoo d by the national compet ent author ities of the Member States in which the high-r isk 
AI system is placed on the mark et or made available.
3. Where high-r isk AI syste ms are subject to other Union harmonisation legislation which also requires an EU 
declaration of conf ormity , a sing le EU declaration of conf ormity shall be drawn up in respect of all Union law applicable to 
the high-r isk AI system. The declaration shall contain all the information required to identify the Union harmonisation 
legislation to which the declaration relates .
4. By drawing up the EU declaration of conf ormity , the provid er shall assume responsibility for comp liance with the 
requirements set out in Section 2. The provider shall keep the EU declaration of conf ormity up-to-dat e as appropr iate.
5. The Commission is emp owered to adopt delegat ed acts in accordance with Article 97 in order to amend Annex V by 
updating the cont ent of the EU declaration of conf ormity set out in that Annex, in order to introduce elements that become 
necessar y in light of technical progress.
Article 48
CE marking
1. The CE marking shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.
2. For high-r isk AI syste ms provided digitally , a digital CE marking shall be used, only if it can easily be accessed via the 
interfac e from which that system is accessed or via an easily accessible machi ne-readable code or other electronic means.
3. The CE marking shall be affixed visibly , legibly and indelibly for high-r isk AI syste ms. Where that is not possible or 
not warrant ed on account of the nature of the high-r isk AI syste m, it shall be affixed to the packag ing or to the 
accom panying documentation, as appropr iate.
4. Where applicable, the CE marking shall be followed by the identifica tion number of the notified body responsible for 
the conf ormity assessment procedures set out in Article 43. The identifica tion number of the notif ied body shall be affixed 
by the body itself or, under its instr uctions, by the provid er or by the provid er’s author ised representative. The identification 
number shall also be indicated in any promotional material which mentions that the high-r isk AI system fulfils the 
requirements for CE marking.
5. Where high-r isk AI systems are subject to other Union law which also provid es for the affixing of the CE marking, the 
CE marking shall indicate that the high-r isk AI system also fulfil the requirements of that other law.
Article 49
Regis tration
1. Before placing on the marke t or putting into service a high-r isk AI system listed in Annex III, with the excep tion of 
high-r isk AI syste ms referred to in point 2 of Annex III, the provid er or, where applicable, the author ised representative 
shall regist er themselves and their syste m in the EU database referred to in Article 71.
2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is not 
high-r isk according to Article 6(3), that provider or, where applicable, the author ised representative shall regist er 
themselves and that system in the EU database refer red to in Article 71.
3. Before putting into service or using a high-r isk AI syste m listed in Annex III, with the excep tion of high-r isk AI 
syste ms listed in point 2 of Annex III, deplo yers that are public author ities, Union institutions, bodies, offices or agencies or 
persons acting on their behalf shall regist er themselves, select the syste m and register its use in the EU database referred to 
in Article 71.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 81/1444. For high-r isk AI systems refer red to in points 1, 6 and 7 of Annex III, in the areas of law enforcement, migration, 
asylum and border control management, the registration referred to in paragraphs 1, 2 and 3 of this Article shall be in 
a secure non-public section of the EU database refer red to in Article 71 and shall include only the follo wing information, as 
applicable, referred to in:
(a)Section A, points 1 to 10, of Annex VIII, with the excep tion of points 6, 8 and 9;
(b)Section B, points 1 to 5, and points 8 and 9 of Annex VIII;
(c)Section C, points 1 to 3, of Annex VIII;
(d)points 1, 2, 3 and 5, of Annex IX.
Only the Commission and national author ities referred to in Article 74(8) shall have access to the respective restr icted 
sections of the EU database listed in the first subparagraph of this paragraph.
5. High-r isk AI syste ms refer red to in point 2 of Annex III shall be regist ered at national level.
CHAPTER IV
TRANSP ARENCY OBLIGA TIONS FOR PROVIDERS AND DEPLO YERS OF CERT AIN AI SYSTEMS
Article 50
Transparency obligations for providers and deplo yers of certain AI systems
1. Providers shall ensure that AI syste ms intended to interac t directly with natural persons are designed and developed in 
such a way that the natural persons concer ned are informed that they are interacting with an AI system, unless this is 
obvious from the point of view of a natural person who is reasonably well-inf ormed, obser vant and circumspect, taking 
into account the circumstances and the cont ext of use. This obligation shall not apply to AI systems author ised by law to 
detect , prevent, invest igate or prosecute criminal offences, subject to appropr iate safeguards for the rights and freedoms of 
third parties, unless those syste ms are available for the public to repor t a criminal offence.
2. Providers of AI syste ms, including general-pur pose AI systems, generating synthetic audio, image, video or text 
cont ent, shall ensure that the outputs of the AI system are marke d in a machine-readable format and detectable as 
artificially generat ed or manipulate d. Providers shall ensure their technical solutions are effective, interoperable, robust and 
reliable as far as this is technically feasible, taking into account the specif icities and limitations of various types of cont ent, 
the costs of imp lementation and the generally ackno wledged state of the art, as may be reflected in relevant technical 
standards. This obligation shall not apply to the extent the AI systems perfo rm an assistive function for standard editing or 
do not substantially alter the input data provid ed by the deplo yer or the semantics thereof, or where author ised by law to 
detect , prevent, investig ate or prosecute criminal offences.
3. Deplo yers of an emotion recognition syste m or a biometr ic catego risation syste m shall inform the natural persons 
exposed thereto of the operation of the syste m, and shall process the personal data in accordance with Regulations (EU) 
2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obliga tion shall not apply to AI syste ms 
used for biometr ic categor isation and emotion recognition, which are permitted by law to detect , prevent or invest igate 
criminal offences, subject to appropr iate safeguards for the rights and freedoms of third parties, and in accordance with 
Union law.
4. Deplo yers of an AI syste m that gene rates or manipulat es image, audio or video cont ent constituting a deep fake, shall 
disclose that the content has been artificially generated or manipulated. This obliga tion shall not apply where the use is 
author ised by law to detect, prevent, invest igate or prosecute criminal offence. Where the cont ent forms part of an evidently 
artistic, creative, satir ical, fictional or analogous work or programme, the transparency obligations set out in this paragraph 
are limit ed to disclosure of the existe nce of such generated or manipulate d cont ent in an appropr iate manner that does not 
ham per the displa y or enjo yment of the work.
Deplo yers of an AI system that generat es or manipulate s text which is published with the purpose of informing the public 
on matt ers of public intere st shall disclose that the text has been artificially generated or manipulate d. This obligation shall 
not apply where the use is author ised by law to detect, prevent, investig ate or prosecute criminal offences or where the 
AI-g enerated cont ent has undergone a process of human review or edito rial control and where a natural or legal person 
holds editor ial responsibility for the publication of the content.EN OJ L, 12.7.2024
82/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj5. The information referred to in paragraphs 1 to 4 shall be provided to the natural persons concer ned in a clear and 
distinguishable manner at the latest at the time of the first interaction or exposure. The information shall conf orm to the 
applicable accessibility requirements.
6. Paragraphs 1 to 4 shall not affect the requirements and obligations set out in Chapt er III, and shall be without 
prejudice to other transparency oblig ations laid down in Union or national law for deplo yers of AI systems.
7. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective 
imp lementation of the obligations regard ing the detection and labelling of artificially generated or manipulat ed cont ent. 
The Commission may adop t imp lementing acts to approve those codes of practice in accordance with the procedure laid 
down in Article 56 (6). If it deems the code is not adequat e, the Commission may adopt an imp lementing act specifying 
common rules for the implementation of those obliga tions in accordance with the examination procedure laid down in 
Article 98(2).
CHAPTER V
GENERAL -PURPOSE AI MODELS
SECTION 1
Classification rules
Article 51
Classif ication of general-pur pose AI models as general-pur pose AI models with systemic risk
1. A general-pur pose AI model shall be classified as a general-pur pose AI model with systemic risk if it meets any of the 
follo wing conditions:
(a)it has high imp act capabilities evaluated on the basis of appropr iate technical tools and methodologies, including 
indicator s and benchmarks;
(b)based on a decision of the Commission, ex officio or followi ng a qualif ied alert from the scientific panel, it has 
capabilities or an imp act equivalent to those set out in point (a) having rega rd to the criteria set out in Annex XIII.
2. A general-pur pose AI model shall be presumed to have high impact capabilities pursuant to paragraph 1, point (a), 
when the cumulative amount of computation used for its training measured in floating point operations is great er than 
1025.
3. The Commission shall adopt delegat ed acts in accordance with Article 97 to amend the thresholds listed in 
paragraphs 1 and 2 of this Article, as well as to supplement benchmarks and indicator s in light of evolving technological 
developments, such as algor ithmic improvements or increased hardware efficiency , when necessar y, for these thresholds to 
reflect the state of the art.
Article 52
Procedure
1. Where a gene ral-pur pose AI model meets the condition referred to in Article 51(1), point (a), the relevant provid er 
shall notify the Commission without dela y and in any event within two week s after that requirement is met or it becomes 
kno wn that it will be met. That notif ication shall include the information necessar y to demonstrate that the relevant 
requirement has been met. If the Commission becomes aware of a general-pur pose AI model presenting syste mic risks of 
which it has not been notified, it may decide to designate it as a model with syste mic risk.
2. The provid er of a general-pur pose AI model that meets the condition refer red to in Article 51(1), point (a), may 
present, with its notif ication, sufficiently substantiate d arguments to demonstrate that, exceptiona lly, although it meets that 
requirement, the general-pur pose AI model does not present, due to its specif ic characteristics , syste mic risks and theref ore 
should not be classif ied as a general-pur pose AI model with systemic risk.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 83/1443. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 are not suffi ciently 
substantiate d and the relevant provid er was not able to demonstrate that the general-purpo se AI model does not present, 
due to its specif ic character istics, syste mic risks, it shall reject those arguments, and the general-pur pose AI model shall be 
considered to be a general-pur pose AI model with syste mic risk.
4. The Commission may designate a general-pur pose AI model as presenting systemic risks, ex officio or following 
a qualif ied alert from the scientific panel pursuant to Article 90(1), point (a), on the basis of criteria set out in Annex XIII.
The Commission is emp owered to adop t delegat ed acts in accordance with Article 97 in order to amend Annex XIII by 
specifying and updating the criteria set out in that Annex.
5. Upon a reasoned request of a provid er whose model has been designate d as a general-pur pose AI model with systemic 
risk pursuant to paragraph 4, the Commission shall take the request into account and may decide to reassess whether the 
general-pur pose AI model can still be considered to present systemic risks on the basis of the criteria set out in Annex XIII. 
Such a request shall contain objective, detailed and new reasons that have arisen since the designation decision. Provi ders 
may request reassessment at the earliest six months after the designation decision. Where the Commission, following its 
reassessment, decides to maintain the designation as a general-pur pose AI model with syste mic risk, providers may request 
reassessment at the earliest six months after that decision.
6. The Commission shall ensure that a list of general-pur pose AI models with systemic risk is published and shall keep 
that list up to date, without prejudice to the need to obser ve and prot ect intellectual proper ty rights and confi dential 
business information or trade secrets in accordance with Union and national law.
SECTION 2
Oblig ations for providers of gener al-pur pose AI models
Article 53
Obligations for providers of general-pur pose AI models
1. Providers of general-pur pose AI models shall:
(a)draw up and keep up-to-da te the technical documentation of the model, including its training and testing process and 
the results of its evaluation, which shall contain, at a minimum, the information set out in Annex XI for the purpose of 
provid ing it, upon request, to the AI Office and the national compet ent author ities;
(b)draw up, keep up-to-da te and mak e available information and documentation to provid ers of AI syste ms who intend to 
integrat e the general-pur pose AI model into their AI systems. Without prejudice to the need to obser ve and prot ect 
intellect ual proper ty rights and confidential business information or trade secrets in accordance with Union and 
national law, the information and documentation shall:
(i)enable provider s of AI syste ms to have a good understanding of the capabilities and limitations of the 
general-pur pose AI model and to comply with their obligations pursuant to this Regulation; and
(ii)contain, at a minimum, the elements set out in Annex XII;
(c)put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply 
with, including through state-of-t he-ar t technologi es, a reser vation of rights expressed pursuant to Article 4(3) of 
Directive (EU) 2019/790;
(d)draw up and make publicly availa ble a sufficiently detailed summar y about the content used for training of the 
general-pur pose AI model, according to a templat e provided by the AI Office.EN OJ L, 12.7.2024
84/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The obliga tions set out in paragraph 1, points (a) and (b), shall not apply to providers of AI models that are released 
under a free and open-source licence that allows for the access, usage , modif ication, and distr ibution of the model, and 
whose paramet ers, including the weights, the information on the model archit ecture, and the information on model usage, 
are made publicly available . This exception shall not apply to general-pur pose AI models with systemic risks.
3. Providers of general-pur pose AI models shall cooperate as necessar y with the Commission and the national 
compet ent author ities in the exercise of their comp etences and powers pursuant to this Regulation.
4. Providers of general-pur pose AI models may rely on codes of practice within the meaning of Article 56 to 
demonstrate complia nce with the obliga tions set out in paragraph 1 of this Article, until a harmonised standard is 
published. Compliance with European harmonised standards grants providers the presump tion of conf ormity to the exte nt 
that those standards cover those obliga tions. Providers of general-pur pose AI models who do not adhere to an approved 
code of practice or do not comply with a European harmonised standard shall demonstrate alternative adequat e means of 
complia nce for assessment by the Commission.
5. For the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and (e) thereof, the Commission is 
emp owered to adop t deleg ated acts in accordance with Article 97 to detail measurement and calculation methodologies 
with a view to allowing for comp arable and verifiable documentation.
6. The Commission is empo wered to adop t deleg ated acts in accordance with Article 97(2) to amend Annexes XI and XII 
in light of evolving technological developments.
7. Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treate d in 
accordance with the conf identiality obliga tions set out in Article 78.
Article 54
Author ised represent ativ es of providers of general-pur pose AI models
1. Prior to placing a general-pur pose AI model on the Union market, provider s established in third countr ies shall, by 
written mandat e, appoint an author ised representative which is established in the Union.
2. The provid er shall enable its author ised representative to perf orm the tasks specified in the mandate received from the 
provid er.
3. The author ised representative shall perfo rm the tasks specified in the mandate received from the provider . It shall 
provid e a copy of the mandate to the AI Offi ce upon request, in one of the official languag es of the institutions of the 
Union. For the purposes of this Regulation, the mandate shall empo wer the author ised representative to carry out the 
follo wing tasks :
(a)verify that the technical documentation specif ied in Annex XI has been drawn up and all obliga tions referred to in 
Article 53 and, where applicable, Article 55 have been fulfilled by the provider;
(b)keep a copy of the technical documentation specified in Annex XI at the disposal of the AI Offi ce and national 
compet ent author ities, for a period of 10 years after the general-pur pose AI model has been placed on the mark et, and 
the contact details of the provid er that appointed the author ised representative;
(c)provid e the AI Office, upon a reasoned request, with all the information and documentation, including that referred to 
in point (b), necessar y to demonstrat e complia nce with the oblig ations in this Chapt er;
(d)cooperate with the AI Office and compet ent author ities, upon a reasoned request, in any action they take in relation to 
the general-pur pose AI model, including when the model is integrat ed into AI systems placed on the marke t or put into 
service in the Union.
4. The mandate shall empower the author ised representative to be addressed, in addition to or instead of the provider , 
by the AI Offi ce or the compet ent author ities, on all issues related to ensur ing complia nce with this Regulation.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 85/1445. The author ised representative shall terminate the mandat e if it considers or has reason to consider the provider to be 
acting contrar y to its obliga tions pursuant to this Regulation. In such a case, it shall also immediate ly inform the AI Office 
about the termination of the mandate and the reasons theref or.
6. The obliga tion set out in this Article shall not apply to providers of general-pur pose AI models that are released under 
a free and open-source licence that allows for the access, usage , modifi cation, and distr ibution of the model, and whose 
parameter s, including the weights, the information on the model arch itecture, and the information on model usage , are 
made publicly available, unless the general-pur pose AI models present systemic risks.
SECTION 3
Oblig ations of providers of gener al-pur pose AI models with syste mic risk
Article 55
Obligations of providers of general-pur pose AI models with systemic risk
1. In addition to the obligations listed in Articles 53 and 54, provid ers of general-pur pose AI models with systemic risk 
shall:
(a)perfo rm model evaluation in accordance with standardised prot ocols and tools reflecting the state of the art, including 
conducting and documenting adversar ial testing of the model with a view to identifying and mitiga ting syste mic risks;
(b)assess and mitig ate possible systemic risks at Union level, including their sources, that may stem from the development, 
the placing on the marke t, or the use of general-pur pose AI models with syste mic risk;
(c)keep track of, document, and repor t, without undue dela y, to the AI Office and, as appropr iate, to national compet ent 
author ities, relevant information about serious incidents and possible corrective measures to address them;
(d)ensure an adequate level of cybersecur ity protect ion for the gene ral-pur pose AI model with systemic risk and the 
physica l infrastr ucture of the model.
2. Providers of general-pur pose AI models with systemic risk may rely on codes of practice within the meaning of 
Article 56 to demonstrate comp liance with the obliga tions set out in paragraph 1 of this Article, until a harmonised 
standard is published. Compliance with European harmonised standards grants provid ers the presump tion of conf ormity to 
the extent that those standards cover those obligations. Providers of general-pur pose AI models with syste mic risks who do 
not adhere to an approved code of practice or do not comply with a European harmonised standard shall demonstrate 
alternative adequate means of compliance for assessment by the Commission.
3. Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treate d in 
accordance with the conf identiality obliga tions set out in Article 78.
SECTION 4
Codes of practice
Article 56
Codes of practice
1. The AI Offi ce shall encourage and facilitate the drawing up of codes of practice at Union level in order to contr ibute 
to the proper application of this Regulation, taking into account international approaches.
2. The AI Office and the Board shall aim to ensure that the codes of practice cover at least the obliga tions provid ed for in 
Articles 53 and 55, including the follo wing issues:EN OJ L, 12.7.2024
86/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(a)the means to ensure that the information referred to in Article 53(1), points (a) and (b), is kept up to date in light of 
marke t and technological developments;
(b)the adequat e level of detail for the summar y about the cont ent used for training;
(c)the identifica tion of the type and nature of the systemic risks at Union level, including their sources, where appropr iate;
(d)the measures, procedures and modalities for the assessment and manag ement of the syste mic risks at Union level, 
including the documentation thereof, which shall be propor tionate to the risks, take into consideration their sever ity 
and probability and take into account the specif ic challeng es of tack ling those risks in light of the possible ways in 
which such risks may emerge and materi alise along the AI value chain.
3. The AI Offi ce may invite all provider s of general-pur pose AI models, as well as relevant national comp etent 
author ities, to participate in the drawing-up of codes of practice. Civil society organisations, industr y, academia and other 
relevant stak eholders, such as downstream provider s and independent exper ts, may suppor t the process.
4. The AI Offi ce and the Board shall aim to ensure that the codes of practice clearly set out their specif ic objectives and 
contain commitments or measures, including key perfo rmance indicator s as appropr iate, to ensure the achievement of 
those objectives, and that they take due account of the needs and interests of all interest ed parties, including affect ed 
persons, at Union level.
5. The AI Office shall aim to ensure that participants to the codes of practice repor t regularly to the AI Office on the 
imp lementation of the commitments and the measures taken and their outcomes, including as measured agains t the key 
perfo rmance indicator s as appropr iate. Key perfo rmance indicator s and repor ting commitments shall reflect diffe rences in 
size and capacity between various participants.
6. The AI Offi ce and the Board shall regularly monitor and evaluate the achi evement of the objectives of the codes of 
practice by the participants and their contr ibution to the proper application of this Regulation. The AI Office and the Board 
shall assess whether the codes of practice cover the obliga tions provided for in Articles 53 and 55, and shall regularly 
monitor and evaluate the achievement of their objectives. They shall publish their assessment of the adequacy of the codes 
of practice.
The Commission may, by way of an implementing act, approve a code of practice and give it a general validity within the 
Union. That implementing act shall be adop ted in accordance with the examination procedure referred to in Article 98(2).
7. The AI Office may invit e all providers of general-pur pose AI models to adhere to the codes of practice. For provider s 
of general-pur pose AI models not presenting syste mic risks this adherence may be limit ed to the obligations provided for in 
Article 53, unless they declare explicitly their interest to join the full code.
8. The AI Office shall, as appropr iate, also encourage and facilitate the review and adaptation of the codes of practice, in 
particular in light of emerging standards. The AI Office shall assist in the assessment of available standards.
9. Codes of practice shall be ready at the latest by 2 May 2025. The AI Office shall take the necessar y steps, including 
inviting provider s pursuant to paragraph 7.
If, by 2 August 2025, a code of practice cannot be final ised, or if the AI Office deems it is not adequat e following its 
assessment under paragraph 6 of this Article, the Commission may provid e, by means of imp lementing acts, common rules 
for the implementation of the obliga tions provided for in Articles 53 and 55, including the issues set out in paragraph 2 of 
this Article. Those imp lementing acts shall be adop ted in accordance with the examination procedure refer red to in Article 
98(2).OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 87/144CHAPTER VI
MEASURES IN SUPPOR T OF INNO VATION
Article 57
AI regulator y sandbo xes
1. Member States shall ensure that their compet ent author ities establish at least one AI regulato ry sandbo x at national 
level, which shall be operational by 2 August 2026. That sandbo x may also be established jointly with the comp etent 
author ities of other Member States. The Commission may provide technical suppor t, advice and tools for the establishment 
and operation of AI regulatory sandbo xes.
The obligation under the first subparagraph may also be fulfilled by participating in an existing sandbo x in so far as that 
participation provides an equivalent level of national coverag e for the participating Member States.
2. Additional AI regulator y sandbo xes at regional or local level, or established jointly with the compet ent author ities of 
other Member States may also be established.
3. The European Data Prot ection Super visor may also establish an AI regulator y sandbo x for Union institutions, bodies, 
offices and agencies, and may exercise the roles and the tasks of national compet ent author ities in accordance with this 
Chapt er.
4. Member States shall ensure that the comp etent author ities referred to in paragraphs 1 and 2 allocate suffi cient 
resources to comply with this Article effectively and in a timely manner . Where appropr iate, national compet ent author ities 
shall cooperate with other relevant author ities, and may allow for the involvement of other actors within the AI ecosyste m. 
This Article shall not affect other regulato ry sandbo xes established under Union or national law. Member States shall ensure 
an appropr iate level of cooperation between the author ities super vising those other sandbo xes and the national compet ent 
author ities.
5. AI regulatory sandbo xes established under paragraph 1 shall provid e for a controlled environment that fosters 
inno vation and facilitates the development, training, testing and validation of inno vative AI syste ms for a limited time 
before their being placed on the marke t or put into service pursuant to a specif ic sandbo x plan agreed between the 
provid ers or prospective providers and the compet ent author ity. Such sandbo xes may include testing in real world 
conditions super vised therein.
6. Compet ent author ities shall provid e, as appropr iate, guidance, super vision and suppor t within the AI regulator y 
sandbo x with a view to identifying risks, in particular to fundamental rights, health and safety, testing, mitigation measures, 
and their effectiveness in relation to the obligations and requirements of this Regulation and, where relevant, other Union 
and national law super vised within the sandbo x.
7. Compet ent author ities shall provide provider s and prospective provid ers participating in the AI regulator y sandbo x 
with guidance on regulator y expectations and how to fulfil the requirements and obligations set out in this Regulation.
Upon request of the provider or prospective provid er of the AI system, the compet ent author ity shall provid e a written 
proof of the activities successfully carried out in the sandbo x. The compet ent author ity shall also provide an exit repor t 
detailing the activities carried out in the sandbo x and the related results and learning outcomes. Provi ders may use such 
documentation to demonstrate their compliance with this Regulation through the conf ormity assessment process or 
relevant marke t surveillance activities. In this regard , the exit repor ts and the written proof provid ed by the national 
compet ent author ity shall be taken positively into account by market surveillance author ities and notif ied bodies, with 
a view to accelerating conf ormity assessment procedures to a reasonable exte nt.
8. Subject to the confidentiality provisions in Article 78, and with the agreement of the provider or prospective provider , 
the Commission and the Board shall be author ised to access the exit repor ts and shall take them into account, as 
appropr iate, when exercising their tasks under this Regulation. If both the provid er or prospective provid er and the national 
compet ent author ity explicitly agree, the exit repor t may be made publicly available through the sing le information 
platf orm refer red to in this Article.
9. The establishment of AI regulator y sandbo xes shall aim to contr ibute to the following objectives:
(a)imp roving legal certainty to achi eve regulator y complia nce with this Regulation or, where relevant, other applicable 
Union and national law;EN OJ L, 12.7.2024
88/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b)suppor ting the shar ing of best practices through cooperation with the author ities involved in the AI regulator y 
sandbo x;
(c)foste ring inno vation and comp etitiveness and facilitating the development of an AI ecosyste m;
(d)contr ibuting to evidence-based regulatory learning;
(e)facilitating and accelerating access to the Union marke t for AI syste ms, in particular when provided by SMEs, including 
start-ups.
10. National comp etent author ities shall ensure that, to the exte nt the innovative AI syste ms involve the processing of 
personal data or other wise fall under the super visor y remit of other national author ities or comp etent author ities providing 
or suppor ting access to data, the national data protect ion author ities and those other national or compet ent author ities are 
associate d with the operation of the AI regulator y sandbo x and involved in the super vision of those aspects to the extent of 
their respective tasks and powers.
11. The AI regulato ry sandbo xes shall not affect the super visor y or corrective powers of the compet ent author ities 
super vising the sandbo xes, including at regional or local level. Any significant risks to health and safety and fundamental 
rights identifi ed during the development and testing of such AI systems shall result in an adequat e mitigation. National 
compet ent author ities shall have the power to temporar ily or permanently suspend the testing process, or the participation 
in the sandbo x if no effective mitig ation is possible, and shall inform the AI Offi ce of such decision. National compet ent 
author ities shall exer cise their super visor y powe rs within the limits of the relevant law, using their discretionar y powers 
when imp lementing legal provisions in respect of a specific AI regulator y sandbo x project, with the objective of suppor ting 
inno vation in AI in the Union.
12. Providers and prospective provid ers participating in the AI regulator y sandbo x shall remain liable under applicable 
Union and national liability law for any damage inflicted on third parties as a result of the exper imentation taking place in 
the sandbo x. How ever , provid ed that the prospective provider s obser ve the specif ic plan and the terms and conditions for 
their participation and follow in good faith the guidance given by the national compet ent author ity, no administrative fines 
shall be imposed by the author ities for infringements of this Regulation. Where other com petent author ities responsible for 
other Union and national law were actively involved in the super vision of the AI system in the sandbo x and provid ed 
guidance for compliance, no administrative fines shall be imp osed rega rding that law.
13. The AI regulato ry sandbo xes shall be designed and imp lemented in such a way that, where relevant, they facilitat e 
cross-border cooperation between national compet ent author ities.
14. National comp etent author ities shall coordinate their activities and cooperate within the framew ork of the Board.
15. National comp etent author ities shall inform the AI Office and the Board of the establishment of a sandbo x, and may 
ask them for suppor t and guidance. The AI Office shall make publicly available a list of planned and existing sandbo xes and 
keep it up to date in order to encourage more interaction in the AI regulator y sandbo xes and cross-border cooperation.
16. National comp etent author ities shall submit annual repor ts to the AI Office and to the Board, from one year after 
the establishment of the AI regulatory sandbo x and ever y year thereaf ter until its term ination, and a final repor t. Those 
repor ts shall provid e information on the progress and results of the imp lementation of those sandbo xes, including best 
practices, incidents, lessons learnt and recommendations on their setup and, where relevant, on the application and possible 
revision of this Regulation, including its deleg ated and impl ementing acts, and on the application of other Union law 
super vised by the compet ent author ities within the sandbo x. The national compet ent author ities shall make those annual 
repor ts or abstracts thereof available to the public, online. The Commission shall, where appropr iate, take the annual 
repor ts into account when exer cising its tasks under this Regulation.
17. The Commission shall develop a sing le and dedicated interface containing all relevant information relate d to AI 
regulator y sandbo xes to allow stak eholders to interac t with AI regulato ry sandbo xes and to raise enquir ies with compet ent 
author ities, and to seek non-binding guidance on the conf ormity of inno vative products, services, business models 
embedding AI technologies, in accordance with Article 62(1), point (c). The Commission shall proactively coordinate with 
national compet ent author ities, where relevant.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 89/144Article 58
Detail ed arrangements for, and functioning of, AI regulat ory sandbo xes
1. In order to avoid fragmentation across the Union, the Commission shall adop t imp lementing acts specifying the 
detailed arrangements for the establishment, development, implementation, operation and super vision of the AI regulator y 
sandbo xes. The imp lementing acts shall include common principles on the follo wing issues:
(a)eligibility and selection criteria for participation in the AI regulator y sandbo x;
(b)procedures for the application, participation, monitoring, exiting from and termination of the AI regulator y sandbo x, 
including the sandbo x plan and the exit repor t;
(c)the terms and conditions applicable to the participants.
Those implementing acts shall be adop ted in accordance with the examination procedure refer red to in Article 98(2).
2. The implementing acts referred to in paragraph 1 shall ensure:
(a)that AI regulator y sandbo xes are open to any applying provider or prospective provid er of an AI system who fulfils 
eligibility and selection criteria, which shall be transparent and fair, and that national com petent author ities inform 
applicants of their decision within three months of the application;
(b)that AI regulatory sandbo xes allow broad and equal access and keep up with demand for participation; providers and 
prospective providers may also submit applications in partnerships with deplo yers and other relevant third parties;
(c)that the detailed arrang ements for, and conditions concer ning AI regulator y sandbo xes suppor t, to the best exte nt 
possible, flexibility for national comp etent author ities to establish and operate their AI regulatory sandbo xes;
(d)that access to the AI regulator y sandbo xes is free of charge for SMEs, including start-ups, without prejudice to 
exceptional costs that national comp etent author ities may reco ver in a fair and propor tionate manner ;
(e)that they facilitat e providers and prospective providers, by means of the learning outcomes of the AI regulator y 
sandbo xes, in comp lying with conf ormity assessment obligations under this Regulation and the voluntar y application 
of the codes of conduct referred to in Article 95;
(f)that AI regulator y sandbo xes facilitate the involvement of other relevant actor s within the AI ecosyste m, such as notif ied 
bodies and standardisation organisations, SMEs, including start-ups, enterprises, innovat ors, testing and exper imenta -
tion facilities, research and exper imentation labs and European Digital Inno vation Hubs, centres of excellence, 
individual researchers , in order to allow and facilitat e cooperation with the public and private secto rs;
(g)that procedures, processes and administrative requirements for application, selection, participation and exiting the AI 
regulator y sandbo x are simple, easily intelligible, and clearly communicated in order to facilitat e the participation of 
SMEs, including start-ups, with limited legal and administrative capacities and are streamlined across the Union, in order 
to avoid fragmentation and that participation in an AI regulatory sandbo x established by a Member State, or by the 
European Data Protection Super visor is mutually and unif ormly recognised and carries the same legal effects across the 
Union;
(h)that participation in the AI regulato ry sandbo x is limited to a period that is appropr iate to the complexity and scale of 
the project and that may be exte nded by the national comp etent author ity;
(i)that AI regulator y sandbo xes facilitate the development of tools and infrastr ucture for testing, bench marking, assessing 
and explaining dimensions of AI syste ms relevant for regulator y learning, such as accuracy , robustness and 
cybersecur ity, as well as measures to mitiga te risks to fundamental rights and society at large.
3. Prospective provider s in the AI regulatory sandbo xes, in particular SMEs and start-ups, shall be directed, where 
relevant, to pre-deplo yment services such as guidance on the implementation of this Regulation, to other value-adding 
services such as help with standardisation documents and certification, testing and exper imentation facilities, European 
Digital Inno vation Hubs and centres of excellence.EN OJ L, 12.7.2024
90/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj4. Where national comp etent author ities consider author ising testing in real world conditions super vised within the 
framew ork of an AI regulator y sandbo x to be established under this Article, they shall specifically agree the terms and 
conditions of such testing and, in particular , the appropr iate safeguards with the participants, with a view to protecting 
fundamental rights, health and safety. Where appropr iate, they shall cooperate with other national comp etent author ities 
with a view to ensur ing consistent practices across the Union.
Article 59
Further processing of personal data for dev eloping certain AI systems in the public interest in the AI regulator y 
sandbo x
1. In the AI regulato ry sandbo x, personal data lawfully collect ed for other purposes may be processed solely for the 
purpose of developing, training and testing certain AI systems in the sandbo x when all of the following conditions are met:
(a)AI syste ms shall be developed for safeguarding substantial public intere st by a public author ity or another natural or 
lega l person and in one or more of the follo wing areas:
(i)public safety and public health, including disease detection, diagnosis prevention, control and treatment and 
improvement of health care systems;
(ii)a high level of protect ion and improvement of the quality of the envir onment, protection of biodiversity , prot ection 
against pollution, green transition measures, climat e change mitiga tion and adap tation measures;
(iii) energy sustainability ;
(iv) safety and resilience of transpor t systems and mobility , critical infrastr ucture and networks;
(v)efficiency and quality of public administration and public services;
(b)the data processed are necessar y for comp lying with one or more of the requirements refer red to in Chap ter III, 
Section 2 where those requirements cannot effectively be fulfilled by processing anonymised , synthetic or other 
non-personal data;
(c)there are effective monitoring mechanisms to identify if any high risks to the rights and freedoms of the data subjects, as 
referred to in Article 35 of Regulation (EU) 2016/679 and in Article 39 of Regulation (EU) 2018/1725, may arise 
during the sandbo x exper imentation, as well as response mec hanisms to promp tly mitiga te those risks and, where 
necessar y, stop the processing;
(d)any personal data to be processed in the cont ext of the sandbo x are in a functionally separate , isolated and prot ected 
data processing environment under the control of the prospective provider and only author ised persons have access to 
those data;
(e)provid ers can further share the originally collect ed data only in accordance with Union data protection law; any 
personal data create d in the sandbo x cannot be shared outside the sandbo x;
(f)any processing of personal data in the context of the sandbo x neither leads to measures or decisions affecting the data 
subjects nor does it affect the application of their rights laid down in Union law on the prot ection of personal data;
(g)any personal data processed in the cont ext of the sandbo x are protect ed by means of appropr iate technical and 
organisational measures and deleted once the participation in the sandbo x has terminated or the personal data has 
reac hed the end of its retention period;
(h)the logs of the processing of personal data in the cont ext of the sandbo x are kept for the duration of the participation in 
the sandbo x, unless provid ed other wise by Union or national law;
(i)a comp lete and detailed descr iption of the process and rationale behind the training, testing and validation of the AI 
syste m is kept together with the testing results as part of the technical documentation refer red to in Annex IV;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 91/144(j)a shor t summar y of the AI project developed in the sandbo x, its objectives and expecte d results is published on the 
website of the compet ent author ities; this obligati on shall not cover sensitive operational data in relation to the activities 
of law enforcement, border control, immigration or asylum author ities.
2. For the purposes of the prevention, invest igation, detection or prosecution of criminal offences or the execution of 
criminal penalties, including safeguarding against and preventing threats to public secur ity, under the control and 
responsibility of law enforcement author ities, the processing of personal data in AI regulato ry sandbo xes shall be based on 
a specific Union or national law and subject to the same cumulative conditions as referred to in paragraph 1.
3. Paragraph 1 is without prejudice to Union or national law which excludes processing of personal data for other 
purposes than those explicitly mentioned in that law, as well as to Union or national law laying down the basis for the 
processing of personal data which is necessar y for the purpose of developing, testing or training of inno vative AI systems or 
any other legal basis, in comp liance with Union law on the prot ection of personal data.
Article 60
Testing of high-r isk AI systems in real world conditions outside AI regulator y sandbo xes
1. Testing of high-r isk AI systems in real world conditions outside AI regulator y sandbo xes may be conducted by 
provid ers or prospective provider s of high-r isk AI systems listed in Annex III, in accordance with this Article and the 
real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.
The Commission shall, by means of implementing acts, specify the detailed elements of the real-wo rld testing plan. Those 
imp lementing acts shall be adopt ed in accordance with the examination procedure refer red to in Article 98(2).
This paragraph shall be without prejudice to Union or national law on the testing in real world conditions of high-r isk AI 
syste ms related to products covered by Union harmonisation legislation listed in Annex I.
2. Providers or prospective provid ers may conduct testing of high-r isk AI syste ms referred to in Annex III in real world 
conditions at any time before the placing on the mark et or the putting into service of the AI syste m on their own or in 
partnership with one or more deplo yers or prospective deplo yers.
3. The testing of high-r isk AI syste ms in real world conditions under this Article shall be without prejudice to any ethical 
review that is required by Union or national law.
4. Providers or prospective provider s may conduct the testing in real world conditions only where all of the following 
conditions are met:
(a)the provid er or prospective provider has drawn up a real-world testing plan and submitted it to the market surveillance 
author ity in the Member State where the testing in real world conditions is to be conducte d;
(b)the marke t surveillance author ity in the Member State where the testing in real world conditions is to be conducte d has 
approved the testing in real world conditions and the real-world testing plan; where the marke t surveillance author ity 
has not provided an answer within 30 days, the testing in real world conditions and the real-world testing plan shall be 
understood to have been approved; where national law does not provid e for a tacit approva l, the testing in real world 
conditions shall remain subject to an author isation;
(c)the provid er or prospective provider , with the excep tion of provider s or prospective provider s of high-r isk AI syste ms 
referred to in points 1, 6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control 
managem ent, and high-r isk AI systems referred to in point 2 of Annex III has registered the testing in real world 
conditions in accordance with Article 71(4) with a Union-wide unique sing le identification number and with the 
information specified in Annex IX; the provider or prospective provider of high-r isk AI systems referred to in points 1, 
6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control management, has register ed 
the testing in real-world conditions in the secure non-public section of the EU database according to Article 49(4), point 
(d), with a Union-wide unique sing le identifica tion number and with the information specified therein; the provid er or 
prospective provid er of high-r isk AI systems referred to in point 2 of Annex III has regist ered the testing in real-wo rld 
conditions in accordance with Article 49(5);EN OJ L, 12.7.2024
92/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(d)the provider or prospective provider conducting the testing in real world conditions is established in the Union or has 
appointed a legal representative who is established in the Union;
(e)data collected and processed for the purpose of the testing in real world conditions shall be transf erred to third 
countr ies only provided that appropr iate and applicable safeguards under Union law are imp lemented;
(f)the testing in real world conditions does not last longer than necessar y to achieve its objectives and in any case not 
longer than six months, which may be exte nded for an additional period of six months, subject to prior notif ication by 
the provider or prospective provider to the marke t surveillance author ity, accom panied by an explanation of the need 
for such an extension;
(g)the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or 
disability , are appropr iately prot ected;
(h)where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more 
deplo yers or prospective deplo yers, the latter have been informed of all aspects of the testing that are relevant to their 
decision to participate, and given the relevant instr uctions for use of the AI system refer red to in Article 13; the 
provid er or prospective provider and the deplo yer or prospective deplo yer shall conclude an agreement specifying their 
roles and responsibilities with a view to ensur ing comp liance with the provisions for testing in real world conditions 
under this Regulation and under other applicable Union and national law;
(i)the subjects of the testing in real world conditions have given informed consent in accordance with Article 61, or in the 
case of law enforcement, where the seeking of informed consent would prevent the AI system from being tested, the 
testing itself and the outcome of the testing in the real world conditions shall not have any negative effect on the 
subjects, and their personal data shall be deleted after the test is perfo rmed;
(j)the testing in real world conditions is effectively overseen by the provider or prospective provider , as well as by 
deplo yers or prospective deplo yers through persons who are suitably qualified in the relevant field and have the 
necessar y capacity , training and author ity to perf orm their tasks ;
(k)the predictions, recommendations or decisions of the AI syste m can be effectively reversed and disreg arded.
5. Any subjects of the testing in real world conditions, or their legally designated representative, as appropr iate, may, 
without any resulting detr iment and without having to provide any justifi cation, withdra w from the testing at any time by 
revok ing their informed consent and may request the immediate and permanent deletion of their personal data. The 
withdra wal of the informed consent shall not affect the activities already carried out.
6. In accordance with Article 75, Member States shall confer on their market surveillance author ities the powers of 
requir ing providers and prospective provid ers to provide information, of carrying out unannounced remot e or on-site 
inspections, and of perfor ming checks on the conduct of the testing in real world conditions and the related high-r isk AI 
syste ms. Marke t surveillance author ities shall use those powers to ensure the safe development of testing in real world 
conditions.
7. Any serious incident identifi ed in the course of the testing in real world conditions shall be repor ted to the national 
marke t surveillance author ity in accordance with Article 73. The provid er or prospective provid er shall adopt immediate 
mitiga tion measures or, failing that, shall suspend the testing in real world conditions until such mitigation take s place, or 
other wise terminate it. The provider or prospective provider shall establish a procedure for the promp t recall of the AI 
syste m upon such termination of the testing in real world conditions.
8. Providers or prospective providers shall notify the national market surveillance author ity in the Member State where 
the testing in real world conditions is to be conducte d of the suspension or term ination of the testing in real world 
conditions and of the final outcomes.
9. The provid er or prospective provid er shall be liable under applicable Union and national liability law for any damage 
caused in the course of their testing in real world conditions.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 93/144Article 61
Infor med consent to participate in testing in real world conditions outside AI regulat ory sandbo xes
1. For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtained 
from the subjects of testing prior to their participation in such testing and after their having been duly informed with 
concise, clear , relevant, and understandable information regarding:
(a)the nature and objectives of the testing in real world conditions and the possible inconve nience that may be linke d to 
their participation;
(b)the conditions under which the testing in real world conditions is to be conducted, including the expecte d duration of 
the subject or subjects’ participation;
(c)their rights, and the guarantees regard ing their participation, in particular their right to refuse to participate in, and the 
right to withdraw from, testing in real world conditions at any time without any resulting detr iment and without having 
to provid e any justificati on;
(d)the arrang ements for requesting the reversal or the disregarding of the predictions, recommendations or decisions of 
the AI syste m;
(e)the Union-wide unique sing le identification number of the testing in real world conditions in accordance with Article 
60(4) point (c), and the contact details of the provid er or its lega l representative from whom further information can be 
obtained.
2. The informed consent shall be dated and documented and a copy shall be given to the subjects of testing or their legal 
representative.
Article 62
Measures for providers and deplo yers, in particular SMEs, including start-ups
1. Member States shall under take the following actions:
(a)provid e SMEs, including start-ups, having a registered office or a branc h in the Union, with priority access to the AI 
regulator y sandbo xes, to the exte nt that they fulfil the eligibility conditions and selection criteria; the priority access 
shall not preclude other SMEs, including start-ups, other than those refer red to in this paragraph from access to the AI 
regulator y sandbo x, provided that they also fulfil the eligibility conditions and selection criteria;
(b)organise specif ic awareness raising and training activities on the application of this Regulation tailored to the needs of 
SMEs including start-ups, deplo yers and, as appropr iate, local public author ities;
(c)utilise existing dedicat ed channels and where appropr iate, establish new ones for communication with SMEs including 
start-ups, deplo yers, other innovat ors and, as appropr iate, local public author ities to provide advice and respond to 
quer ies about the implementation of this Regulation, including as rega rds participation in AI regulator y sandbo xes;
(d)facilitate the participation of SMEs and other relevant stak eholders in the standardisation development process.
2. The specific interests and needs of the SME provid ers, including start-ups, shall be take n into account when setting the 
fees for conf ormity assessment under Article 43, reducing those fees propor tionately to their size, marke t size and other 
relevant indicator s.
3. The AI Office shall under take the following actions:
(a)provid e standardised template s for areas covered by this Regulation, as specif ied by the Board in its request ;
(b)develop and maintain a sing le information platf orm provid ing easy to use information in relation to this Regulation for 
all operat ors across the Union;EN OJ L, 12.7.2024
94/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(c)organise appropr iate communication campaigns to raise awareness about the obligati ons arising from this Regulation;
(d)evaluate and promot e the convergence of best practices in public procurement procedures in relation to AI systems.
Article 63
Derogations for specif ic operat ors
1. Microenter prises within the meaning of Recommendation 2003/361/EC may comply with certain elements of the 
quality management system required by Article 17 of this Regulation in a simplified manner , provid ed that they do not 
have partner enterprises or linked enter prises within the meaning of that Recommendation. For that purpose, the 
Commission shall develop guidelines on the elements of the quality manag ement system which may be comp lied with in 
a simplified manner consider ing the needs of microenterp rises, without affecting the level of protection or the need for 
complia nce with the requirements in respect of high-r isk AI syste ms.
2. Paragraph 1 of this Article shall not be inter preted as exemp ting those operat ors from fulfilling any other 
requirements or oblig ations laid down in this Regulation, including those established in Articles 9, 10, 11, 12, 13, 14, 15, 
72 and 73.
CHAPTER VII
GOVERNANCE
SECTION 1
Gove rnance at Union level
Article 64
AI Office
1. The Commission shall develop Union exper tise and capabilities in the field of AI through the AI Office.
2. Member States shall facilitat e the tasks entr uste d to the AI Office, as reflecte d in this Regulation.
Article 65
Establishment and structure of the European Artificial Intelligence Board
1. A European Artificial Intellig ence Board (the ‘Board’) is hereb y established.
2. The Board shall be comp osed of one representative per Member State. The European Data Protection Super visor shall 
participate as obser ver. The AI Office shall also attend the Board’s meetings, without taking part in the votes. Other national 
and Union author ities, bodies or exper ts may be invit ed to the meetings by the Board on a case by case basis, where the 
issues discussed are of relevance for them.
3. Each representative shall be designated by their Member State for a period of three years, renewable once.
4. Member States shall ensure that their representatives on the Board:
(a)have the relevant compet ences and powers in their Member State so as to contr ibute actively to the achievement of the 
Board’s tasks refer red to in Article 66;
(b)are designate d as a sing le contact point vis-à-vis the Board and, where appropr iate, taking into account Member States’ 
needs, as a sing le contact point for stakeholders;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 95/144(c)are emp ower ed to facilitate consistency and coordination between national compet ent author ities in their Member State 
as rega rds the imp lementation of this Regulation, including through the collection of relevant data and information for 
the purpose of fulfilling their tasks on the Board.
5. The designated representatives of the Member States shall adop t the Board’s rules of procedure by a two-thirds 
majorit y. The rules of procedure shall, in particular , lay down procedures for the selection process, the duration of the 
mandate of, and specif ications of the tasks of, the Chair , detailed arrang ements for voting, and the organi sation of the 
Board’s activities and those of its sub-groups.
6. The Board shall establish two standing sub-groups to provide a platf orm for cooperation and exchang e among marke t 
surveillance author ities and notifying author ities about issues related to market surveillance and notified bodies respectively .
The standing sub-group for marke t surveillance should act as the administrative cooperation group (ADCO) for this 
Regulation within the meaning of Article 30 of Regulation (EU) 2019/1020.
The Board may establish other standing or temporar y sub-groups as appropr iate for the purpose of examining specif ic 
issues. Where appropr iate, representatives of the advisor y forum refer red to in Article 67 may be invited to such sub-groups 
or to specif ic meetings of those subgroups as obser vers.
7. The Board shall be organised and operat ed so as to safeguard the objectivity and imp artiality of its activities.
8. The Board shall be chaired by one of the representatives of the Member States. The AI Offi ce shall provid e the 
secretar iat for the Board, convene the meetings upon request of the Chair , and prepare the agenda in accordance with the 
tasks of the Board pursuant to this Regulation and its rules of procedure.
Article 66
Tasks of the Board
The Board shall advise and assist the Commission and the Member States in order to facilitat e the consistent and effective 
application of this Regulation. To that end, the Board may in particular:
(a) contr ibut e to the coordination among national comp etent author ities responsible for the application of this Regulation 
and, in cooperation with and subject to the agreement of the marke t surveillance author ities concer ned, suppor t joint 
activities of marke t surveillance author ities referred to in Article 74(11);
(b) collect and share technical and regulato ry exper tise and best practices among Member States;
(c) provid e advice on the imp lementation of this Regulation, in particular as regards the enforcement of rules on 
general-pur pose AI models;
(d) contr ibut e to the harmonisation of administrative practices in the Member States, including in relation to the 
derogat ion from the conf ormity assessment procedures refer red to in Article 46, the functioning of AI regulator y 
sandbo xes, and testing in real world conditions referred to in Articles 57, 59 and 60;
(e) at the request of the Commission or on its own initiative, issue recommendations and written opinions on any relevant 
matt ers related to the implementation of this Regulation and to its consiste nt and effective application, including:
(i)on the development and application of codes of conduct and codes of practice pursuant to this Regulation, as well 
as of the Commission’s guidelines;
(ii)the evaluation and review of this Regulation pursuant to Article 112, including as rega rds the serious incident 
repor ts referred to in Article 73, and the functioning of the EU database referred to in Article 71, the preparation 
of the deleg ated or implementing acts, and as regard s possible alignments of this Regulation with the Union 
harmonisation legislation listed in Annex I;
(iii) on technical specif ications or existing standards regarding the requirements set out in Chapt er III, Section 2;EN OJ L, 12.7.2024
96/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(iv) on the use of harmonised standards or common specifications refer red to in Articles 40 and 41;
(v)trends, such as European global comp etitiveness in AI, the uptak e of AI in the Union, and the development of 
digital skills;
(vi) trends on the evolving typology of AI value chains, in particular on the resulting implications in terms of 
accountability ;
(vii) on the potential need for amendment to Annex III in accordance with Article 7, and on the potential need for 
possible revision of Article 5 pursuant to Article 112, taking into account relevant available evidence and the 
latest developments in technology ;
(f) suppor t the Commission in promoting AI literac y, public awareness and understanding of the benefi ts, risks, 
safeguards and rights and obliga tions in relation to the use of AI syste ms;
(g) facilitate the development of common criteria and a shared understanding among market operat ors and compet ent 
author ities of the relevant concep ts provid ed for in this Regulation, including by contr ibuting to the development of 
bench mark s;
(h) cooperate, as appropr iate, with other Union institutions, bodies, offices and agencies, as well as relevant Union exper t 
groups and networks, in particular in the fields of product safety , cybersecur ity, comp etition, digital and media services, 
financ ial services, consumer prot ection, data and fundamental rights prot ection;
(i) contr ibut e to effective cooperation with the compet ent author ities of third countr ies and with inter national 
organisations;
(j) assist national comp etent author ities and the Commission in developing the organi sational and technical exper tise 
required for the imp lementation of this Regulation, including by contr ibuting to the assessment of training needs for 
staff of Member States involved in implementing this Regulation;
(k) assist the AI Offi ce in suppor ting national comp etent author ities in the establishment and development of AI 
regulator y sandbo xes, and facilitate cooperation and information-shar ing among AI regulator y sandbo xes;
(l) contr ibut e to, and provide relevant advice on, the development of guidance documents;
(m) advise the Commission in relation to inter national matt ers on AI;
(n) provid e opinions to the Commission on the qualif ied alerts regard ing general-pur pose AI models;
(o) receive opinions by the Member States on qualified alerts regard ing general-pur pose AI models, and on national 
exper iences and practices on the monitoring and enforcement of AI syste ms, in particular systems integrating the 
general-pur pose AI models.
Article 67
Advisor y forum
1. An advisor y forum shall be established to provid e technical exper tise and advise the Board and the Commission, and 
to contr ibut e to their tasks under this Regulation.
2. The membership of the advisor y forum shall represent a balanced selection of stak eholders, including industr y, 
start-ups, SMEs, civil society and academia. The membership of the advisor y forum shall be balanced with rega rd to 
commercial and non-commercial interests and, within the catego ry of commercial interests, with regard to SMEs and other 
under takings.
3. The Commission shall appoint the members of the advisor y forum, in accordance with the criteria set out in 
paragraph 2, from amongst stakeholders with recognised exper tise in the field of AI.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 97/1444. The term of offic e of the members of the advisor y forum shall be two years, which may be extended by up to no more 
than four years.
5. The Fundamental Rights Agency , ENISA, the European Committe e for Standardization (CEN), the European 
Committe e for Electrotec hnical Standardization (CENELEC), and the European Telecommunications Standards Institute 
(ETSI) shall be permanent members of the advisor y forum.
6. The advisor y forum shall draw up its rules of procedure. It shall elect two co-chairs from among its members, in 
accordance with criteria set out in paragraph 2. The term of office of the co-c hairs shall be two years, renewable once.
7. The advisor y forum shall hold meetings at least twice a year. The advisor y forum may invit e exper ts and other 
stak eholders to its meetings.
8. The advisor y forum may prepare opinions, recommendations and written contr ibutions at the request of the Board or 
the Commission.
9. The advisor y forum may establish standing or temporar y sub-groups as appropr iate for the purpose of examining 
specific questions related to the objectives of this Regulation.
10. The advisor y forum shall prepare an annual repor t on its activities. That repor t shall be made publicly available .
Article 68
Scientif ic panel of independent exper ts
1. The Commission shall, by means of an imp lementing act, mak e provis ions on the establishment of a scientific panel 
of independent exper ts (the ‘scientific panel’) intended to suppor t the enforcement activities under this Regulation. That 
imp lementing act shall be adop ted in accordance with the examination procedure referred to in Article 98(2).
2. The scientific panel shall consist of exper ts selected by the Commission on the basis of up-to -date scientific or 
technical exper tise in the field of AI necessar y for the tasks set out in paragraph 3, and shall be able to demonstrate meeting 
all of the following conditions:
(a)having particular exper tise and compet ence and scientific or technical exper tise in the field of AI;
(b)independence from any provid er of AI syste ms or general-pur pose AI models;
(c)an ability to carry out activities dilige ntly, accurately and objectively .
The Commission, in consultation with the Board, shall determine the number of exper ts on the panel in accordance with 
the required needs and shall ensure fair gender and geographical representation.
3. The scientifi c panel shall advise and suppor t the AI Office, in particular with rega rd to the following tasks:
(a)suppor ting the implementation and enforcement of this Regulation as regard s general-pur pose AI models and systems, 
in particular by:
(i)alerting the AI Office of possible systemic risks at Union level of general-pur pose AI models, in accordance with 
Article 90;
(ii)contr ibuting to the development of tools and methodologies for evaluating capabilities of general-pur pose AI 
models and syste ms, including through bench mark s;
(iii) providing advice on the classif ication of general-pur pose AI models with systemic risk;
(iv) providing advice on the classif ication of various general-pur pose AI models and systems;EN OJ L, 12.7.2024
98/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(v)contr ibuting to the development of tools and templates;
(b)suppor ting the work of market surveillance author ities, at their request ;
(c)suppor ting cross-border marke t surveillance activities as referred to in Article 74(11), without prejudice to the powers 
of marke t surveillance author ities;
(d)suppor ting the AI Offi ce in carrying out its duties in the cont ext of the Union safeguard procedure pursuant to 
Article 81.
4. The exper ts on the scientific panel shall perfor m their tasks with impar tiality and objectivity , and shall ensure the 
confi dentiality of information and data obtained in carrying out their tasks and activities. They shall neither seek nor take 
instr uctions from anyone when exercising their tasks under paragraph 3. Each exper t shall draw up a declaration of 
intere sts, which shall be made publicly available. The AI Office shall establish systems and procedures to actively manage 
and prevent pote ntial conf licts of interest.
5. The implementing act refer red to in paragraph 1 shall include provisions on the conditions, procedures and detailed 
arrangements for the scientific panel and its members to issue alerts, and to request the assistance of the AI Office for the 
perfo rmance of the tasks of the scientifi c panel.
Article 69
Access to the pool of exper ts by the Member States
1. Member States may call upon exper ts of the scientific panel to suppor t their enforcement activities under this 
Regulation.
2. The Member States may be required to pay fees for the advice and suppor t provid ed by the exper ts. The structure and 
the level of fees as well as the scale and structure of reco verable costs shall be set out in the imp lementing act refer red to in 
Article 68(1), taking into account the objectives of the adequat e imp lementation of this Regulation, cost-effectiveness and 
the necessity of ensur ing effective access to exper ts for all Member States.
3. The Commission shall facilitate timely access to the exper ts by the Member States, as needed, and ensure that the 
combination of suppor t activities carried out by Union AI testing suppor t pursuant to Article 84 and exper ts pursuant to 
this Article is efficiently organised and provides the best possible added value.
SECTION 2
National compe tent author ities
Article 70
Designation of national competent author ities and single points of cont act
1. Each Member State shall establish or designate as national compet ent author ities at least one notifying author ity and 
at least one marke t surveillance author ity for the purposes of this Regulation. Those national comp etent author ities shall 
exercise their powers independently , impar tially and without bias so as to safeguard the objectivity of their activities and 
tasks , and to ensure the application and implementation of this Regulation. The members of those author ities shall refrain 
from any action incom patible with their duties. Provided that those principles are obser ved, such activities and tasks may be 
perfo rmed by one or more designate d author ities, in accordance with the organisational needs of the Member State.
2. Member States shall communicate to the Commission the identity of the notifying author ities and the marke t 
surveillance author ities and the tasks of those author ities, as well as any subsequent change s thereto . Member States shall 
make publicly availa ble information on how compet ent author ities and sing le points of contact can be contact ed, through 
electronic communication means by 2 August 2025. Member States shall designate a market surveillance author ity to act as 
the sing le point of contact for this Regulation, and shall notify the Commission of the identity of the sing le point of contact. 
The Commission shall make a list of the sing le points of contact publicly available.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 99/1443. Member States shall ensure that their national compet ent author ities are provided with adequate technical, financial 
and human resources, and with infrastr ucture to fulfil their tasks effectively under this Regulation. In particular , the national 
compet ent author ities shall have a suffi cient number of personnel permanently available whose comp etences and exper tise 
shall include an in-depth understanding of AI technologies, data and data computing , personal data prot ection, 
cybersecur ity, fundamental rights, health and safety risks and kno wledge of existing standards and legal requirements. 
Member States shall assess and, if necessar y, update compet ence and resource requirements referred to in this paragraph on 
an annual basis.
4. National compet ent author ities shall take appropr iate measures to ensure an adequate level of cybersecur ity.
5. When perfo rming their tasks, the national com petent author ities shall act in accordance with the confi dentiality 
obliga tions set out in Article 78.
6. By 2 August 2025, and once ever y two years thereaf ter, Member States shall repor t to the Commission on the status 
of the financ ial and human resources of the national comp etent author ities, with an assessment of their adequacy . The 
Commission shall transmit that information to the Board for discussion and possible recommendations.
7. The Commission shall facilitate the exchang e of exper ience between national comp etent author ities.
8. National comp etent author ities may provid e guidance and advice on the imp lementation of this Regulation, in 
particular to SMEs including start-ups, taking into account the guidance and advice of the Board and the Commission, as 
appropr iate. Whenever national comp etent author ities intend to provide guidance and advice with rega rd to an AI system 
in areas covered by other Union law, the national comp etent author ities under that Union law shall be consult ed, as 
appropr iate.
9. Where Union institutions, bodies, offices or agencies fall within the scope of this Regulation, the European Data 
Protection Super visor shall act as the compet ent author ity for their super vision.
CHAPTER VIII
EU DATABASE FOR HIGH-RISK AI SYSTEMS
Article 71
EU database for high-r isk AI systems listed in Annex III
1. The Commission shall, in collaboration with the Member States, set up and maintain an EU database containing 
information refer red to in paragraphs 2 and 3 of this Article concer ning high-r isk AI systems referred to in Article 6(2) 
which are registered in accordance with Articles 49 and 60 and AI systems that are not considered as high-r isk pursuant to 
Article 6(3) and which are registered in accordance with Article 6(4) and Article 49. When setting the functional 
specific ations of such database, the Commission shall consult the relevant exper ts, and when updating the functional 
specific ations of such database, the Commission shall consult the Board.
2. The data listed in Sections A and B of Annex VIII shall be entered into the EU database by the provid er or, where 
applicable, by the author ised representative.
3. The data listed in Section C of Annex VIII shall be entered into the EU database by the deplo yer who is, or who acts on 
behalf of, a public author ity, agency or body , in accordance with Article 49(3) and (4).
4. With the exception of the section referred to in Article 49(4) and Article 60(4), point (c), the information contained in 
the EU database register ed in accordance with Article 49 shall be accessible and publicly available in a user -friendly manner . 
The information should be easily navig able and machi ne-readable. The information regist ered in accordance with Article 60 
shall be accessible only to marke t surveillance author ities and the Commission, unless the prospective provider or provid er 
has given consent for also making the information accessible the public.
5. The EU database shall contain personal data only in so far as necessar y for collecting and processing information in 
accordance with this Regulation. That information shall include the names and contact details of natural persons who are 
responsible for registeri ng the syste m and have the lega l author ity to represent the provid er or the deplo yer, as applicable.EN OJ L, 12.7.2024
100/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj6. The Commission shall be the controller of the EU database. It shall make available to providers, prospective provider s 
and deplo yers adequat e technical and administrative suppor t. The EU database shall comply with the applicable accessibility 
requirements.
CHAPTER IX
POST -MARKET MONITORING, INFORMA TION SHARING AND MARKET SUR VEILL ANCE
SECTION 1
Post-market monitor ing
Article 72
Post-mark et monitorin g by providers and pos t-mark et monitor ing plan for high-r isk AI systems
1. Providers shall establish and document a post-mark et monitoring syste m in a manner that is propor tionate to the 
nature of the AI technologies and the risks of the high-r isk AI system.
2. The post-mark et monito ring system shall actively and systematically collect, document and analyse relevant data 
which may be provided by deplo yers or which may be collected through other sources on the perform ance of high-r isk AI 
syste ms throughout their lifetime, and which allow the provider to evaluate the continuous complia nce of AI systems with 
the requirements set out in Chap ter III, Section 2. Where relevant, post-mark et monitoring shall include an analysis of the 
interac tion with other AI syste ms. This obligati on shall not cover sensitive operational data of deplo yers which are 
law-enf orcement author ities.
3. The post-mark et monitori ng system shall be based on a post-mark et monitoring plan. The post-mark et monitoring 
plan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt an imp lementing act 
laying down detailed provisions establishing a templat e for the post-mark et monitoring plan and the list of elements to be 
included in the plan by 2 Febr uary 2026. That imp lementing act shall be adop ted in accordance with the examination 
procedure referred to in Article 98(2).
4. For high-r isk AI syste ms covered by the Union harmonisation legislation listed in Section A of Annex I, where 
a post-market monitoring system and plan are already established under that legislation, in order to ensure consiste ncy, 
avoid duplications and minimise additional burdens, provid ers shall have a choice of integrat ing, as appropr iate, the 
necessar y elements descr ibed in paragraphs 1, 2 and 3 using the templat e refer red in paragraph 3 into syste ms and plans 
already existing under that legislation, provid ed that it achieves an equivalent level of protect ion.
The first subparagraph of this paragraph shall also apply to high-r isk AI syste ms referred to in point 5 of Annex III placed 
on the market or put into service by financ ial institutions that are subject to requirements under Union financ ial services 
law regarding their inter nal gove rnance, arrang ements or processes.
SECTION 2
Shar ing of information on serious incidents
Article 73
Repor ting of serious incidents
1. Providers of high-r isk AI syste ms placed on the Union marke t shall repor t any serious incident to the marke t 
surveillance author ities of the Member States where that incident occur red.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 101/1442. The repor t referred to in paragraph 1 shall be made immediate ly after the provider has established a causal link 
between the AI system and the serious incident or the reasonable likelihood of such a link , and, in any event, not later than 
15 days after the provid er or, where applicable, the deplo yer, becomes aware of the serious incident.
The period for the repor ting refer red to in the first subparagraph shall take account of the sever ity of the serious incident.
3. Notwithstanding paragraph 2 of this Article, in the event of a widespread infringem ent or a serious incident as 
defined in Article 3, point (49)(b), the repor t referred to in paragraph 1 of this Article shall be provid ed immediately , and 
not later than two days after the provider or, where applicable, the deplo yer becomes aware of that incident.
4. Notwithstanding paragraph 2, in the event of the death of a person, the repor t shall be provided immediately after the 
provid er or the deplo yer has established, or as soon as it suspects, a causal relationship between the high-r isk AI syste m and 
the serious incident, but not later than 10 days after the date on which the provider or, where applicable, the deplo yer 
becomes aware of the serious incident.
5. Where necessar y to ensure timely repor ting, the provider or, where applicable, the deplo yer, may submit an initial 
repor t that is incom plete, follo wed by a comp lete repor t.
6. Following the repor ting of a serious incident pursuant to paragraph 1, the provid er shall, without dela y, perf orm the 
necessar y investigations in relation to the serious incident and the AI syste m concer ned. This shall include a risk assessment 
of the incident, and corrective action.
The provider shall cooperate with the compet ent author ities, and where relevant with the notif ied body concer ned, during 
the invest igations refer red to in the first subparagraph, and shall not perfo rm any investig ation which involves altering the 
AI syste m concer ned in a way which may affect any subsequent evaluation of the causes of the incident, prior to informing 
the comp etent author ities of such action.
7. Upon receiving a notif ication related to a serious incident refer red to in Article 3, point (49)(c), the relevant marke t 
surveillance author ity shall inform the national public author ities or bodies referred to in Article 77(1). The Commission 
shall develop dedicated guidance to facilitate comp liance with the obligations set out in paragraph 1 of this Article. That 
guidance shall be issued by 2 August 2025, and shall be assessed regularly .
8. The market surveillance author ity shall take appropr iate measures, as provid ed for in Article 19 of Regulation (EU) 
2019/1020, within seven days from the date it received the notif ication refer red to in paragraph 1 of this Article, and shall 
follo w the notification procedures as provided in that Regulation.
9. For high-r isk AI syste ms referred to in Annex III that are placed on the market or put into service by provider s that are 
subject to Union legislative instr uments laying down repor ting obligati ons equivalent to those set out in this Regulation, the 
notif ication of serious incidents shall be limit ed to those referred to in Article 3, point (49)(c).
10. For high-r isk AI syste ms which are safety components of devices, or are themselves devices, covered by Regulations 
(EU) 2017/745 and (EU) 2017/746, the notification of serious incidents shall be limit ed to those refer red to in Article 3, 
point (49)(c) of this Regulation, and shall be made to the national compet ent author ity chosen for that purpose by the 
Member States where the incident occur red.
11. National compet ent author ities shall immediately notify the Commission of any serious incident, whether or not 
they have take n action on it, in accordance with Article 20 of Regulation (EU) 2019/1020.
SECTION 3
Enforcement
Article 74
Marke t surveillance and control of AI systems in the Union mark et
1. Regulation (EU) 2019/1020 shall apply to AI systems covered by this Regulation. For the purposes of the effective 
enforcement of this Regulation:EN OJ L, 12.7.2024
102/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(a)any reference to an economic operator under Regulation (EU) 2019/1020 shall be understood as including all operat ors 
identifie d in Article 2(1) of this Regulation;
(b)any reference to a product under Regulation (EU) 2019/1020 shall be understood as including all AI syste ms falling 
within the scope of this Regulation.
2. As part of their repor ting obliga tions under Article 34(4) of Regulation (EU) 2019/1020, the marke t surveillance 
author ities shall repor t annually to the Commission and relevant national competition author ities any information 
identifie d in the course of marke t surveillance activities that may be of potential interest for the application of Union law on 
competition rules. They shall also annually repor t to the Commission about the use of prohibite d practices that occur red 
during that year and about the measures take n.
3. For high-r isk AI systems related to products cover ed by the Union harmonisation legislation listed in Section A of 
Annex I, the marke t surveillance author ity for the purposes of this Regulation shall be the author ity responsible for marke t 
surveillance activities designated under those legal acts.
By derogat ion from the first subparagraph, and in appropr iate circumstances, Member States may designate another 
relevant author ity to act as a market surveillance author ity, provid ed they ensure coordination with the relevant sectoral 
marke t surveillance author ities responsible for the enforcement of the Union harmonisation legislation listed in Annex I.
4. The procedures referred to in Articles 79 to 83 of this Regulation shall not apply to AI syste ms relate d to products 
covered by the Union harmonisation legislation listed in section A of Annex I, where such lega l acts already provide for 
procedures ensur ing an equivalent level of prot ection and having the same objective. In such cases, the relevant sectoral 
procedures shall apply instead.
5. Without prejudice to the powers of market surveillance author ities under Article 14 of Regulation (EU) 2019/1020, 
for the purpose of ensur ing the effective enforcement of this Regulation, marke t surveillance author ities may exercise the 
powe rs referred to in Article 14(4), points (d) and (j), of that Regulation remotely , as appropr iate.
6. For high-r isk AI syste ms placed on the marke t, put into service, or used by financ ial institutions regulated by Union 
financ ial services law, the marke t surveillance author ity for the purposes of this Regulation shall be the relevant national 
author ity responsible for the financ ial super vision of those institutions under that legislation in so far as the placing on the 
marke t, putting into service, or the use of the AI system is in direct connection with the provision of those financial 
services.
7. By way of derogation from paragraph 6, in appropr iate circumstances, and provided that coordination is ensured, 
another relevant author ity may be identified by the Member State as marke t surveillance author ity for the purposes of this 
Regulation.
National market surveillance author ities super vising regulated credit institutions regulate d under Directive 2013/36/EU, 
which are participating in the Sing le Super visor y Mec hanism established by Regulation (EU) No 1024/2013, should repor t, 
without dela y, to the European Central Bank any information identified in the course of their marke t surveillance activities 
that may be of potent ial interest for the prudential super visor y tasks of the European Central Bank specified in that 
Regulation.
8. For high-r isk AI syste ms listed in point 1 of Annex III to this Regulation, in so far as the systems are used for law 
enforcement purposes, border manag ement and justice and democracy , and for high-r isk AI systems listed in points 6, 7 
and 8 of Annex III to this Regulation, Member States shall designate as market surveillance author ities for the purposes of 
this Regulation either the comp etent data prot ection super visor y author ities under Regulation (EU) 2016/679 or Directive 
(EU) 2016/680, or any other author ity designate d pursuant to the same conditions laid down in Articles 41 to 44 of 
Directive (EU) 2016/680. Market surveillance activities shall in no way affect the independence of judicial author ities, or 
other wise interfere with their activities when acting in their judicial capacity .
9. Where Union institutions, bodies, offices or agencies fall within the scope of this Regulation, the European Data 
Protection Super visor shall act as their market surveillance author ity, excep t in relation to the Cour t of Justice of the 
European Union acting in its judicial capacity .
10. Member States shall facilitate coordination between marke t surveillance author ities designate d under this Regulation 
and other relevant national author ities or bodies which super vise the application of Union harmonisation legislation listed 
in Annex I, or in other Union law, that might be relevant for the high-r isk AI syste ms refer red to in Annex III.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 103/14411. Market surveillance author ities and the Commission shall be able to propose joint activities, including joint 
invest igations, to be conducted by either market surveillance author ities or mark et surveillance author ities jointly with the 
Commission, that have the aim of promoting compliance, identifying non-compliance, raising awareness or providing 
guidance in relation to this Regulation with respect to specif ic cate gories of high-r isk AI systems that are found to present 
a serious risk across two or more Member States in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Office 
shall provid e coordination suppor t for joint investig ations.
12. Without prejudice to the powers provided for under Regulation (EU) 2019/1020, and where relevant and limited to 
what is necessar y to fulfil their tasks , the marke t surveillance author ities shall be granted full access by provid ers to the 
documentation as well as the training, validation and testing data sets used for the development of high-r isk AI systems, 
including, where appropr iate and subject to secur ity safeguards, through application programming interfaces (API) or other 
relevant technical means and tools enabling remot e access.
13. Market surveillance author ities shall be granted access to the source code of the high-r isk AI syste m upon a reasoned 
request and only when both of the following conditions are fulfilled:
(a)access to source code is necessar y to assess the conf ormity of a high-r isk AI syste m with the requirements set out in 
Chapt er III, Section 2; and
(b)testing or auditing procedures and verificati ons based on the data and documentation provided by the provider have 
been exhaust ed or proved insuff icient.
14. Any information or documentation obtained by marke t surveillance author ities shall be treate d in accordance with 
the confidentiality obligations set out in Article 78.
Article 75
Mutual assis tance, market surveillance and control of general-pur pose AI systems
1. Where an AI system is based on a general-pur pose AI model, and the model and the system are developed by the 
same provider , the AI Office shall have powers to monito r and super vise comp liance of that AI syste m with obligations 
under this Regulation. To carry out its monitoring and super vision tasks, the AI Offi ce shall have all the powers of a marke t 
surveillance author ity provided for in this Section and Regulation (EU) 2019/1020.
2. Where the relevant marke t surveillance author ities have suffi cient reason to consider general-pur pose AI systems that 
can be used directly by deplo yers for at least one purpose that is classif ied as high-r isk pursuant to this Regulation to be 
non-comp liant with the requirements laid down in this Regulation, they shall cooperate with the AI Office to carry out 
complia nce evaluations, and shall inform the Board and other market surveillance author ities according ly.
3. Where a marke t surveillance author ity is unable to conclude its invest igation of the high-r isk AI system because of its 
inability to access certain information related to the general-pur pose AI model despite having made all appropr iate efforts 
to obtain that information, it may submit a reasoned request to the AI Office, by which access to that information shall be 
enforced. In that case, the AI Office shall supply to the applicant author ity without dela y, and in any event within 30 days, 
any information that the AI Office considers to be relevant in order to establish whether a high-r isk AI system is 
non-comp liant. Marke t surveillance author ities shall safegua rd the confidentiality of the information that they obtain in 
accordance with Article 78 of this Regulation. The procedure provided for in Chapt er VI of Regulation (EU) 2019/1020 
shall apply mutatis mutandis.
Article 76
Super vision of testing in real world conditions by market surveillance author ities
1. Market surveillance author ities shall have compet ences and powers to ensure that testing in real world conditions is in 
accordance with this Regulation.EN OJ L, 12.7.2024
104/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. Where testing in real world conditions is conducte d for AI systems that are super vised within an AI regulator y 
sandbo x under Article 58, the marke t surveillance author ities shall verify the compliance with Article 60 as part of their 
super visor y role for the AI regulator y sandbo x. Those author ities may, as appropr iate, allow the testing in real world 
conditions to be conducte d by the provid er or prospective provid er, in deroga tion from the conditions set out in Article 
60(4), points (f) and (g).
3. Where a market surveillance author ity has been informed by the prospective provider , the provid er or any third party 
of a serious incident or has other grounds for consider ing that the conditions set out in Articles 60 and 61 are not met, it 
may take either of the following decisions on its territory , as appropr iate:
(a)to suspend or terminate the testing in real world conditions;
(b)to require the provid er or prospective provider and the deplo yer or prospective deplo yer to modify any aspect of the 
testing in real world conditions.
4. Where a marke t surveillance author ity has take n a decision referred to in paragraph 3 of this Article, or has issued an 
objection within the meaning of Article 60(4), point (b), the decision or the objection shall indicate the grounds theref or 
and how the provid er or prospective provider can challeng e the decision or objection.
5. Where applicable, where a marke t surveillance author ity has taken a decision refer red to in paragraph 3, it shall 
communicate the grounds theref or to the marke t surveillance author ities of other Member States in which the AI system 
has been tested in accordance with the testing plan.
Article 77
Powers of author ities protecting fundamental rights
1. National public author ities or bodies which super vise or enforce the respect of oblig ations under Union law 
protect ing fundamental rights, including the right to non-discr imination, in relation to the use of high-r isk AI syste ms 
referred to in Annex III shall have the power to request and access any documentation create d or maintained under this 
Regulation in accessible language and format when access to that documentation is necessar y for effectively fulfilling their 
mandates within the limits of their jurisdiction. The relevant public author ity or body shall inform the marke t surveillance 
author ity of the Member State concer ned of any such request.
2. By 2 November 2024, each Member State shall identify the public author ities or bodies referred to in paragraph 1 and 
make a list of them publicly available. Member States shall notify the list to the Commission and to the other Member 
States, and shall keep the list up to date.
3. Where the documentation refer red to in paragraph 1 is insuff icient to ascer tain whether an infringement of 
obliga tions under Union law prot ecting fundamental rights has occur red, the public author ity or body refer red to in 
paragraph 1 may make a reasoned request to the marke t surveillance author ity, to organise testing of the high-r isk AI 
syste m through technical means. The market surveillance author ity shall organise the testing with the close involvement of 
the requesting public author ity or body within a reasonable time follo wing the request.
4. Any information or documentation obtained by the national public author ities or bodies referred to in paragraph 1 of 
this Article pursuant to this Article shall be treated in accordance with the confidentiality obligations set out in Article 78.
Article 78
Conf identiality
1. The Commission, marke t surveillance author ities and notif ied bodies and any other natural or lega l person involved 
in the application of this Regulation shall, in accordance with Union or national law, respect the confi dentiality of 
information and data obtained in carrying out their tasks and activities in such a manner as to prot ect, in particular:OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 105/144(a)the intellectual proper ty rights and confidential business information or trade secrets of a natural or lega l person, 
including source code, excep t in the cases referred to in Article 5 of Directive (EU) 2016/943 of the European 
Parliame nt and of the Council (57);
(b)the effective imp lementation of this Regulation, in particular for the purposes of inspections, investig ations or audits;
(c)public and national secur ity interests;
(d)the conduct of criminal or administrative proceedings;
(e)information classif ied pursuant to Union or national law.
2. The author ities involved in the application of this Regulation pursuant to paragraph 1 shall request only data that is 
strictly necessar y for the assessment of the risk posed by AI syste ms and for the exercise of their powers in accordance with 
this Regulation and with Regulation (EU) 2019/1020. They shall put in place adequate and effective cybersecur ity measures 
to protect the secur ity and conf identiality of the information and data obtained, and shall delete the data collected as soon 
as it is no longe r needed for the purpose for which it was obtained, in accordance with applicable Union or national law.
3. Without prejudice to paragraphs 1 and 2, information exchang ed on a confidential basis between the national 
compet ent author ities or between national compet ent author ities and the Commission shall not be disclosed without prior 
consultation of the originating national compet ent author ity and the deplo yer when high-r isk AI systems referred to in 
point 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or asylum author ities and when such 
disclosure would jeopardise public and national secur ity interests. This exchange of information shall not cover sensitive 
operational data in relation to the activities of law enforcement, border control, immigration or asylum author ities.
When the law enforcement, immigration or asylum author ities are provid ers of high-r isk AI systems referred to in point 1, 
6 or 7 of Annex III, the technical documentation refer red to in Annex IV shall remain within the premises of those 
author ities. Those author ities shall ensure that the mark et surveillance author ities referred to in Article 74(8) and (9), as 
applicable, can, upon request, immediate ly access the documentation or obtain a copy thereof. Only staff of the marke t 
surveillance author ity holding the appropr iate level of secur ity clearance shall be allowed to access that documentation or 
any copy thereof.
4. Paragraphs 1, 2 and 3 shall not affect the rights or obliga tions of the Commission, Member States and their relevant 
author ities, as well as those of notif ied bodies, with regard to the exchang e of information and the dissemination of 
warnings, including in the cont ext of cross-border cooperation, nor shall they affect the oblig ations of the parties concer ned 
to provid e information under criminal law of the Member States.
5. The Commission and Member States may exchang e, where necessar y and in accordance with relevant provisions of 
inter national and trade agreements, confi dential information with regulator y author ities of third countr ies with which they 
have concluded bilat eral or multilate ral conf identiality arrang ements guarante eing an adequate level of confidentiality .
Article 79
Procedure at national level for dealing with AI systems presenting a risk
1. AI syste ms presenting a risk shall be understood as a ‘product presenting a risk’ as defined in Article 3, point 19 of 
Regulation (EU) 2019/1020, in so far as they present risks to the health or safety, or to fundamental rights, of persons.
2. Where the marke t surveillance author ity of a Member State has sufficient reason to consider an AI system to present 
a risk as refer red to in paragraph 1 of this Article, it shall carry out an evaluation of the AI syste m concer ned in respect of 
its comp liance with all the requirements and obligations laid down in this Regulation. Particular attention shall be given to 
AI syste ms presenting a risk to vulnerable groups. Where risks to fundamental rights are identified, the market surveillance 
author ity shall also inform and fully cooperate with the relevant national public author ities or bodies referred to in Article 
77(1). The relevant operat ors shall cooperate as necessar y with the marke t surveillance author ity and with the other 
national public author ities or bodies referred to in Article 77(1).EN OJ L, 12.7.2024
106/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(57) Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on the prot ection of undisclosed know-ho w 
and business information (trade secrets) against their unlaw ful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).Where, in the course of that evaluation, the market surveillance author ity or, where applicable the market surveillance 
author ity in cooperation with the national public author ity referred to in Article 77(1), finds that the AI system does not 
comply with the requirements and obligations laid down in this Regulation, it shall without undue dela y require the relevant 
operato r to take all appropr iate corrective actions to bring the AI syste m into comp liance, to withdra w the AI system from 
the mark et, or to recall it within a period the marke t surveillance author ity may prescr ibe, and in any event within the 
shor ter of 15 working days, or as provided for in the relevant Union harmonisation legislation.
The marke t surveillance author ity shall inform the relevant notif ied body according ly. Article 18 of Regulation (EU) 
2019/1020 shall apply to the measures referred to in the second subparagraph of this paragraph.
3. Where the mark et surveillance author ity considers that the non-comp liance is not restr icted to its national territory , it 
shall inform the Commission and the other Member States without undue dela y of the results of the evaluation and of the 
actions which it has required the operato r to take .
4. The operator shall ensure that all appropr iate corrective action is take n in respect of all the AI syste ms concer ned that 
it has made available on the Union marke t.
5. Where the operator of an AI system does not take adequate corrective action within the period refer red to in 
paragraph 2, the market surveillance author ity shall take all appropr iate provisional measures to prohibit or restr ict the AI 
syste m’s being made available on its national marke t or put into service, to withdra w the product or the standalone AI 
syste m from that marke t or to recall it. That author ity shall without undue dela y notify the Commission and the other 
Member States of those measures.
6. The notification refer red to in paragraph 5 shall include all available details, in particular the information necessar y 
for the identification of the non-comp liant AI syste m, the origin of the AI syste m and the supply chain, the nature of the 
non-comp liance alleged and the risk involved, the nature and duration of the national measures taken and the arguments 
put forward by the relevant operat or. In particular , the market surveillance author ities shall indicate whether the 
non-comp liance is due to one or more of the follo wing:
(a)non-comp liance with the prohibition of the AI practices referred to in Article 5;
(b)a failure of a high-r isk AI system to meet requirements set out in Chapt er III, Section 2;
(c)shor tcomings in the harmonised standards or common specifications refer red to in Articles 40 and 41 confer ring 
a presump tion of conf ormity ;
(d)non-comp liance with Article 50.
7. The marke t surveillance author ities other than the mark et surveillance author ity of the Member State initiating the 
procedure shall, without undue dela y, inform the Commission and the other Member States of any measures adopt ed and of 
any additional information at their disposal relating to the non-compliance of the AI system concer ned, and, in the event of 
disagreement with the notif ied national measure, of their objections.
8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection has 
been raised by either a marke t surveillance author ity of a Member State or by the Commission in respect of a provis ional 
measure taken by a marke t surveillance author ity of another Member State, that measure shall be deemed justifi ed. This 
shall be without prejudice to the procedural rights of the concer ned operator in accordance with Article 18 of Regulation 
(EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event of 
non-comp liance with the prohibition of the AI practices referred to in Article 5 of this Regulation.
9. The marke t surveillance author ities shall ensure that appropr iate restr ictive measures are taken in respect of the 
product or the AI syste m concer ned, such as withdra wal of the product or the AI syste m from their marke t, without undue 
dela y.
Article 80
Procedure for dealing with AI systems classif ied by the provider as non-high-r isk in application of Annex III
1. Where a mark et surveillance author ity has sufficient reason to consider that an AI system classif ied by the provider as 
non-high-r isk pursuant to Article 6(3) is indeed high-r isk, the mark et surveillance author ity shall carry out an evaluation of 
the AI system concer ned in respect of its classific ation as a high-r isk AI syste m based on the conditions set out in Article 
6(3) and the Commission guidelines.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 107/1442. Where, in the course of that evaluation, the market surveillance author ity finds that the AI syste m concer ned is 
high-r isk, it shall without undue dela y require the relevant provider to take all necessar y actions to bring the AI syste m into 
complia nce with the requirements and obligations laid down in this Regulation, as well as take appropr iate corrective action 
within a period the marke t surveillance author ity may prescr ibe.
3. Where the marke t surveillance author ity considers that the use of the AI system concer ned is not restr icted to its 
national territory , it shall inform the Commission and the other Member States without undue dela y of the results of the 
evaluation and of the actions which it has required the provider to take .
4. The provid er shall ensure that all necessar y action is taken to bring the AI system into compliance with the 
requirements and obliga tions laid down in this Regulation. Where the provider of an AI system concer ned does not bring 
the AI syste m into compliance with those requirements and obligati ons within the period referred to in paragraph 2 of this 
Article, the provid er shall be subject to fines in accordance with Article 99.
5. The provid er shall ensure that all appropr iate corrective action is take n in respect of all the AI systems concer ned that 
it has made available on the Union marke t.
6. Where the provider of the AI system concer ned does not take adequate corrective action within the period referred to 
in paragraph 2 of this Article, Article 79(5) to (9) shall apply .
7. Where, in the course of the evaluation pursuant to paragraph 1 of this Article, the mark et surveillance author ity 
establishes that the AI system was misclassified by the provider as non-high-r isk in order to circum vent the application of 
requirements in Chapt er III, Section 2, the provid er shall be subject to fines in accordance with Article 99.
8. In exercising their power to monitor the application of this Article, and in accordance with Article 11 of Regulation 
(EU) 2019/1020, marke t surveillance author ities may perform appropr iate checks, taking into account in particular 
information stored in the EU database referred to in Article 71 of this Regulation.
Article 81
Union safeguard procedure
1. Where, within three months of receipt of the notif ication referred to in Article 79(5), or within 30 days in the case of 
non-comp liance with the prohibition of the AI practices refer red to in Article 5, objections are raised by the marke t 
surveillance author ity of a Member State to a measure taken by another marke t surveillance author ity, or where the 
Commission considers the measure to be contrar y to Union law, the Commission shall without undue dela y enter into 
consultation with the marke t surveillance author ity of the relevant Member State and the operat or or operat ors, and shall 
evaluate the national measure. On the basis of the results of that evaluation, the Commission shall, within six months, or 
within 60 days in the case of non-comp liance with the prohibition of the AI practices referred to in Article 5, starting from 
the notification referred to in Article 79(5), decide whether the national measure is justified and shall notify its decision to 
the market surveillance author ity of the Member State concer ned. The Commission shall also inform all other marke t 
surveillance author ities of its decision.
2. Where the Commission considers the measure taken by the relevant Member State to be justified, all Member States 
shall ensure that they take appropr iate restr ictive measures in respect of the AI syste m concer ned, such as requir ing the 
withdra wal of the AI system from their mark et without undue dela y, and shall inform the Commission according ly. Where 
the Commission considers the national measure to be unjustified, the Member State concer ned shall withdra w the measure 
and shall inform the Commission according ly.
3. Where the national measure is considered justifi ed and the non-comp liance of the AI syste m is attributed to 
shor tcomings in the harmonised standards or common specific ations refer red to in Articles 40 and 41 of this Regulation, 
the Commission shall apply the procedure provided for in Article 11 of Regulation (EU) No 1025/2012.
Article 82
Compliant AI systems which present a risk
1. Where, having perfor med an evaluation under Article 79, after consulting the relevant national public author ity 
referred to in Article 77(1), the mark et surveillance author ity of a Member State finds that although a high-r isk AI system 
complie s with this Regulation, it never theless presents a risk to the health or safety of persons, to fundamental rights, or to 
other aspects of public interest protection, it shall require the relevant operato r to take all appropr iate measures to ensure 
that the AI system concer ned, when placed on the market or put into service, no longer presents that risk without undue 
dela y, within a period it may prescr ibe.EN OJ L, 12.7.2024
108/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj2. The provider or other relevant operato r shall ensure that corrective action is taken in respect of all the AI syste ms 
concer ned that it has made available on the Union market within the timeline prescr ibed by the market surveillance 
author ity of the Member State refer red to in paragraph 1.
3. The Member States shall immediately inform the Commission and the other Member States of a finding under 
paragraph 1. That information shall include all available details, in particular the data necessar y for the identifica tion of the 
AI syste m concer ned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature and 
duration of the national measures take n.
4. The Commission shall without undue dela y enter into consultation with the Member States concer ned and the 
relevant operators, and shall evaluate the national measures taken. On the basis of the results of that evaluation, the 
Commission shall decide whether the measure is justified and, where necessar y, propose other appropr iate measures.
5. The Commission shall immediately communicate its decision to the Member States concer ned and to the relevant 
operato rs. It shall also inform the other Member States.
Article 83
Formal non-compliance
1. Where the marke t surveillance author ity of a Member State mak es one of the follo wing findings, it shall require the 
relevant provid er to put an end to the non-compliance concer ned, within a period it may prescr ibe:
(a)the CE marking has been affixed in violation of Article 48;
(b)the CE marking has not been affixed;
(c)the EU declaration of conf ormity referred to in Article 47 has not been drawn up;
(d)the EU declaration of conf ormity referred to in Article 47 has not been drawn up correctly ;
(e)the registration in the EU database referred to in Article 71 has not been carried out;
(f)where applicable, no author ised representative has been appointed;
(g)technical documentation is not available.
2. Where the non-compliance referred to in paragraph 1 persists, the marke t surveillance author ity of the Member State 
concer ned shall take appropr iate and propor tionate measures to restr ict or prohibit the high-r isk AI system being made 
available on the marke t or to ensure that it is recalled or withdrawn from the mark et without dela y.
Article 84
Union AI testing suppor t structures
1. The Commission shall designate one or more Union AI testing suppor t structures to perfo rm the tasks listed under 
Article 21(6) of Regulation (EU) 2019/1020 in the area of AI.
2. Without prejudice to the tasks refer red to in paragraph 1, Union AI testing suppor t structures shall also provide 
independent technical or scientific advice at the request of the Board, the Commission, or of market surveillance author ities.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 109/144SECTION 4
Remedies
Article 85
Right to lodge a complaint with a market surveillance author ity
Without prejudice to other administrative or judicial remedies, any natural or legal person having grounds to consider that 
there has been an infringement of the provisions of this Regulation may submit complaints to the relevant marke t 
surveillance author ity.
In accordance with Regulation (EU) 2019/1020, such complaints shall be take n into account for the purpose of conducting 
marke t surveillance activities, and shall be handled in line with the dedicat ed procedures established theref or by the marke t 
surveillance author ities.
Article 86
Right to explanation of individual decision-making
1. Any affect ed person subject to a decision which is take n by the deplo yer on the basis of the output from a high-r isk AI 
syste m listed in Annex III, with the exception of syste ms listed under point 2 thereof, and which produces lega l effects or 
similarly signifi cantly affects that person in a way that they consider to have an adverse impact on their health, safety or 
fundamental rights shall have the right to obtain from the deplo yer clear and meaningful explanations of the role of the AI 
syste m in the decision-making procedure and the main elements of the decision take n.
2. Paragraph 1 shall not apply to the use of AI syste ms for which excep tions from, or restr ictions to, the obliga tion 
under that paragraph follow from Union or national law in compliance with Union law.
3. This Article shall apply only to the exte nt that the right refer red to in paragraph 1 is not other wise provided for under 
Union law.
Article 87
Repor ting of infr ingements and protection of repor ting persons
Directive (EU) 2019/1937 shall apply to the repor ting of infringem ents of this Regulation and the protection of persons 
repor ting such infringem ents.
SECTION 5
Super vision, investig ation, enforcement and monitor ing in respect of providers of gener al-pur pose AI models
Article 88
Enforcement of the obligations of providers of general-pur pose AI models
1. The Commission shall have exclusive powers to super vise and enforce Chapt er V, taking into account the procedural 
guarantees under Article 94. The Commission shall entr ust the imp lementation of these tasks to the AI Office, without 
prejudice to the powers of organisation of the Commission and the division of compet ences between Member States and 
the Union based on the Treaties.
2. Without prejudice to Article 75(3), marke t surveillance author ities may request the Commission to exercise the 
powe rs laid down in this Section, where that is necessar y and propor tionate to assist with the fulfilment of their tasks under 
this Regulation.EN OJ L, 12.7.2024
110/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojArticle 89
Monitor ing actions
1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessar y 
actions to monitor the effective implementation and comp liance with this Regulation by provid ers of general-pur pose AI 
models, including their adherence to approve d codes of practice.
2. Downstream provid ers shall have the right to lodg e a comp laint alleging an infringem ent of this Regulation. 
A comp laint shall be duly reasoned and indicate at least:
(a)the point of contact of the provid er of the general-pur pose AI model concer ned;
(b)a descr iption of the relevant facts, the provisions of this Regulation concer ned, and the reason why the downstream 
provid er considers that the provid er of the general-purpo se AI model concer ned infringed this Regulation;
(c)any other information that the downstream provider that sent the request considers relevant, including, where 
appropr iate, information gathered on its own initiative.
Article 90
Aler ts of systemic risks by the scientif ic panel
1. The scientifi c panel may provid e a qualified alert to the AI Office where it has reason to suspect that:
(a)a general-pur pose AI model poses concrete identifia ble risk at Union level; or
(b)a general-pur pose AI model meets the conditions refer red to in Article 51.
2. Upon such qualif ied alert, the Commission, through the AI Offi ce and after having informed the Board, may exercise 
the powers laid down in this Section for the purpose of assessing the matt er. The AI Office shall inform the Board of any 
measure according to Articles 91 to 94.
3. A qualified alert shall be duly reasoned and indicate at least:
(a)the point of contact of the provid er of the general-pur pose AI model with systemic risk concer ned;
(b)a descr iption of the relevant facts and the reasons for the alert by the scientifi c panel;
(c)any other information that the scientific panel considers to be relevant, including, where appropr iate, information 
gathered on its own initiative.
Article 91
Power to request document ation and information
1. The Commission may request the provider of the general-pur pose AI model concer ned to provide the documentation 
drawn up by the provider in accordance with Articles 53 and 55, or any additional information that is necessar y for the 
purpose of assessing comp liance of the provider with this Regulation.
2. Before sending the request for information, the AI Offi ce may initiate a structured dialogue with the provid er of the 
general-pur pose AI model.
3. Upon a duly substantiated request from the scientific panel, the Commission may issue a request for information to 
a provider of a general-pur pose AI model, where the access to information is necessar y and propor tionate for the fulfilment 
of the tasks of the scientific panel under Article 68(2).OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 111/1444. The request for information shall state the lega l basis and the purpose of the request, specify what information is 
required, set a period within which the information is to be provid ed, and indicate the fines provided for in Article 101 for 
supplying incor rect, incom plete or misleading information.
5. The provider of the general-pur pose AI model concer ned, or its representative shall supply the information requested. 
In the case of lega l persons, comp anies or firms, or where the provider has no lega l personality , the persons author ised to 
represent them by law or by their statutes, shall supply the information requested on behalf of the provider of the 
general-pur pose AI model concer ned. Lawy ers duly author ised to act may supply information on behalf of their clients. The 
clients shall never theless remain fully responsible if the information supplied is incomplet e, incor rect or misleading.
Article 92
Power to conduct evaluations
1. The AI Office, after consulting the Board, may conduct evaluations of the general-pur pose AI model concer ned:
(a)to assess comp liance of the provid er with obliga tions under this Regulation, where the information gathered pursuant 
to Article 91 is insufficient ; or
(b)to investig ate syste mic risks at Union level of general-pur pose AI models with syste mic risk, in particular following 
a qualified alert from the scientific panel in accordance with Article 90(1), point (a).
2. The Commission may decide to appoint independent exper ts to carry out evaluations on its behalf, including from the 
scientific panel established pursuant to Article 68. Independent exper ts appointed for this task shall meet the criteria 
outlined in Article 68(2).
3. For the purposes of paragraph 1, the Commission may request access to the general-pur pose AI model concer ned 
through APIs or further appropr iate technical means and tools, including source code.
4. The request for access shall state the lega l basis, the purpose and reasons of the request and set the period within 
which the access is to be provided, and the fines provided for in Article 101 for failure to provide access.
5. The providers of the general-purpo se AI model concer ned or its representative shall supply the information requested. 
In the case of lega l persons, comp anies or firms, or where the provider has no lega l personality , the persons author ised to 
represent them by law or by their statute s, shall provide the access request ed on behalf of the provider of the 
general-pur pose AI model concer ned.
6. The Commission shall adopt imp lementing acts setting out the detailed arrang ements and the conditions for the 
evaluations, including the detailed arrang ements for involving independent exper ts, and the procedure for the selection 
thereof. Those implementing acts shall be adop ted in accordance with the examination procedure referred to in Article 
98(2).
7. Prior to requesting access to the general-pur pose AI model concer ned, the AI Offi ce may initiate a structured dialogue 
with the provider of the general-pur pose AI model to gather more information on the inter nal testing of the model, intern al 
safeguards for preventing systemic risks, and other internal procedures and measures the provid er has taken to mitigat e 
such risks.
Article 93
Power to request measures
1. Where necessar y and appropr iate, the Commission may request provid ers to:
(a)take appropr iate measures to comp ly with the obligations set out in Articles 53 and 54;EN OJ L, 12.7.2024
112/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b)imp lement mitig ation measures, where the evaluation carried out in accordance with Article 92 has given rise to serious 
and substantiate d concer n of a syste mic risk at Union level;
(c)restr ict the making available on the market, withdra w or recall the model.
2. Before a measure is requested, the AI Office may initiate a structured dialogue with the provider of the 
general-pur pose AI model.
3. If, during the structured dialogue referred to in paragraph 2, the provider of the gene ral-pur pose AI model with 
syste mic risk offers commitments to implement mitigation measures to address a syste mic risk at Union level, the 
Commission may, by decision, make those commitments binding and declare that there are no further grounds for action.
Article 94
Procedural rights of economic operat ors of the general-pur pose AI model
Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to the providers of the general-pur pose AI model, 
without prejudice to more specific procedural rights provided for in this Regulation.
CHAPTER X
CODES OF CONDUCT AND GUIDELINES
Article 95
Codes of conduct for volunt ary application of specif ic requirements
1. The AI Offi ce and the Member States shall encourage and facilitat e the drawing up of codes of conduct, including 
related gover nance mechanisms, intended to foster the voluntar y application to AI systems, other than high-r isk AI systems, 
of some or all of the requirements set out in Chap ter III, Section 2 taking into account the available technical solutions and 
industr y best practices allowi ng for the application of such requirements.
2. The AI Office and the Member States shall facilitate the drawing up of codes of conduct concer ning the voluntar y 
application, including by deplo yers, of specific requirements to all AI syste ms, on the basis of clear objectives and key 
perfo rmance indicato rs to measure the achievement of those objectives, including elements such as, but not limited to:
(a)applicable elements provided for in Union ethical guidelines for trustwo rthy AI;
(b)assessing and minimising the imp act of AI syste ms on envir onmental sustainability , including as regards energy-eff icient 
programming and techniques for the efficient design, training and use of AI;
(c)promoting AI literac y, in particular that of persons dealing with the development, operation and use of AI;
(d)facilitating an inclusive and diverse design of AI syste ms, including through the establishment of inclusive and diverse 
development teams and the promotion of stak eholders’ participation in that process;
(e)assessing and preventing the nega tive imp act of AI syste ms on vulnerable persons or groups of vulnerable persons, 
including as regards accessibility for persons with a disability , as well as on gender equality .
3. Codes of conduct may be drawn up by individual provid ers or deplo yers of AI syste ms or by organisations 
representing them or by both, including with the involvement of any intere sted stakeholders and their representative 
organisations, including civil society organisations and academia. Codes of conduct may cover one or more AI syste ms 
taking into account the similar ity of the intende d purpose of the relevant systems.
4. The AI Office and the Member States shall take into account the specific interests and needs of SMEs, including 
start-ups, when encouraging and facilitating the drawing up of codes of conduct.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 113/144Article 96
Guidelines from the Commission on the implementa tion of this Regulation
1. The Commission shall develop guidelines on the practical imp lementation of this Regulation, and in particular on:
(a)the application of the requirements and obliga tions referred to in Articles 8 to 15 and in Article 25;
(b)the prohibite d practices referred to in Article 5;
(c)the practical implementation of the provisions relate d to substantial modification;
(d)the practical implementation of transparency obliga tions laid down in Article 50;
(e)detailed information on the relationship of this Regulation with the Union harmonisation legislation listed in Annex I, 
as well as with other relevant Union law, including as regards consistency in their enforcement ;
(f)the application of the definition of an AI system as set out in Article 3, point (1).
When issuing such guidelines, the Commission shall pay particular attention to the needs of SMEs including start-ups, of 
local public author ities and of the sectors most likely to be affected by this Regulation.
The guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledg ed 
state of the art on AI, as well as of relevant harmonised standards and common specif ications that are refer red to in 
Articles 40 and 41, or of those harmonised standards or technical specific ations that are set out pursuant to Union 
harmonisation law.
2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelines 
previously adop ted when deemed necessar y.
CHAPTER XI
DELEGA TION OF POWER AND COMM ITTEE PROCEDURE
Article 97
Exe rcise of the delegation
1. The power to adop t deleg ated acts is confe rred on the Commission subject to the conditions laid down in this Article.
2. The power to adop t deleg ated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5) 
and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferr ed on the Commission for 
a period of five years from 1 August 2024. The Commission shall draw up a repor t in respect of the delegation of power 
not later than nine months before the end of the five-year period. The deleg ation of power shall be tacitly extende d for 
periods of an identical duration, unless the European Parliame nt or the Council opposes such extension not later than three 
months before the end of each period.
3. The delegation of power referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5) and (6), 
Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) may be revok ed at any time by the European Parliament 
or by the Council. A decision of revoc ation shall put an end to the deleg ation of power specif ied in that decision. It shall 
take effect the day following that of its publication in the Official Journal of the European Union or at a later date specified 
therein. It shall not affect the validity of any delegat ed acts already in force.
4. Before adopting a delegat ed act, the Commission shall consult exper ts designate d by each Member State in accordance 
with the principles laid down in the Interinstitutional Agreement of 13 Apr il 2016 on Bett er Law-Making .EN OJ L, 12.7.2024
114/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj5. As soon as it adop ts a deleg ated act, the Commission shall notify it simultaneously to the European Parliament and to 
the Council.
6. Any delegat ed act adop ted pursuant to Article 6(6) or (7), Article 7(1) or (3), Article 11(3), Article 43(5) or (6), 
Article 47(5), Article 51(3), Article 52(4) or Article 53(5) or (6) shall enter into force only if no objection has been 
expressed by either the European Parliame nt or the Council within a period of three months of notification of that act to 
the European Parliament and the Council or if, before the expir y of that period, the European Parliament and the Council 
have both informed the Commission that they will not object. That period shall be extended by three months at the 
initiative of the European Parliame nt or of the Council.
Article 98
Committee procedure
1. The Commission shall be assist ed by a committee. That committee shall be a committ ee within the meaning of 
Regulation (EU) No 182/2011.
2. Where reference is made to this paragraph, Article 5 of Regulation (EU) No 182/2011 shall apply .
CHAPTER XII
PENAL TIES
Article 99
Penalties
1. In accordance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules on 
penalties and other enforcement measures, which may also include warnings and non-monetar y measures, applicable to 
infringements of this Regulation by operat ors, and shall take all measures necessar y to ensure that they are properly and 
effectively implement ed, thereby taking into account the guidelines issued by the Commission pursuant to Article 96. The 
penalties provided for shall be effective, propor tionate and dissuasive. They shall take into account the interests of SMEs, 
including start-ups, and their economic viability .
2. The Member States shall, without dela y and at the latest by the date of entr y into application, notify the Commission 
of the rules on penalties and of other enforcement measures referred to in paragraph 1, and shall notify it, without dela y, of 
any subsequent amendment to them.
3. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative 
fines of up to EUR 35 000 000 or, if the offender is an under taking, up to 7 % of its total worldwide annual turnover for the 
preceding financial year, whichever is higher .
4. Non-compliance with any of the followi ng provisions relate d to operato rs or notified bodies, other than those laid 
down in Articles 5, shall be subject to administrative fines of up to EUR 15 000 000 or, if the offender is an under taking, up 
to 3 % of its total worldwide annual turnover for the preceding financ ial year, whichever is higher:
(a)obliga tions of provid ers pursuant to Article 16;
(b)obliga tions of author ised representatives pursuant to Article 22;
(c)obliga tions of importers pursuant to Article 23;
(d)obliga tions of distr ibutors pursuant to Article 24;
(e)obliga tions of deplo yers pursuant to Article 26;
(f)requirements and obliga tions of notified bodies pursuant to Article 31, Article 33(1), (3) and (4) or Article 34;
(g)transparency obliga tions for provid ers and deplo yers pursuant to Article 50.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 115/1445. The supply of incor rect, incom plete or misleading information to notif ied bodies or national compet ent author ities in 
reply to a request shall be subject to administrative fines of up to EUR 7 500 000 or, if the offender is an under taking, up to 
1 % of its total worldwide annual turnover for the preceding financial year, whichever is higher .
6. In the case of SMEs, including start-ups, each fine referred to in this Article shall be up to the percentage s or amount 
referred to in paragraphs 3, 4 and 5, whichever thereof is lower.
7. When deciding whether to impose an administrative fine and when deciding on the amount of the administrative fine 
in each individual case, all relevant circumstances of the specif ic situation shall be take n into account and, as appropr iate, 
regard shall be given to the following:
(a)the nature, gravity and duration of the infringem ent and of its consequences, taking into account the purpose of the AI 
syste m, as well as, where appropr iate, the number of affect ed persons and the level of damage suffer ed by them;
(b)whether administrative fines have already been applied by other market surveillance author ities to the same operator for 
the same infringem ent;
(c)whether administrative fines have already been applied by other author ities to the same operato r for infringements of 
other Union or national law, when such infringements result from the same activity or omission constituting a relevant 
infringement of this Regulation;
(d)the size, the annual turnover and marke t share of the operat or committing the infringement ;
(e)any other aggravating or mitiga ting factor applicable to the circumstances of the case, such as financ ial benefits gained, 
or losses avoided, directly or indirectly , from the infringem ent;
(f)the degree of cooperation with the national compet ent author ities, in order to remedy the infringem ent and mitigat e the 
possible adverse effects of the infringement;
(g)the degree of responsibility of the operator taking into account the technical and organisational measures imp lemented 
by it;
(h)the manner in which the infringement became kno wn to the national comp etent author ities, in particular whether , and 
if so to what exte nt, the operato r notif ied the infringem ent;
(i)the intentional or negligent character of the infringement;
(j)any action taken by the operator to mitigat e the harm suffer ed by the affected persons.
8. Each Member State shall lay down rules on to what extent administrative fines may be imp osed on public author ities 
and bodies established in that Member State.
9. Depending on the legal syste m of the Member States, the rules on administrative fines may be applied in such 
a manner that the fines are imp osed by compet ent national cour ts or by other bodies, as applicable in those Member States. 
The application of such rules in those Member States shall have an equivalent effect.
10. The exercise of powers under this Article shall be subject to appropr iate procedural safeguards in accordance with 
Union and national law, including effective judicial remedies and due process.
11. Member States shall, on an annual basis, repor t to the Commission about the administrative fines they have issued 
during that year, in accordance with this Article, and about any related litigation or judicial proceedings.
Article 100
Adminis trativ e fines on Union instit utions, bodies, offices and agencies
1. The European Data Protection Super visor may imp ose administrative fines on Union institutions, bodies, offices and 
agencies falling within the scope of this Regulation. When deciding whether to impose an administrative fine and when 
deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation 
shall be take n into account and due rega rd shall be given to the followi ng:EN OJ L, 12.7.2024
116/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(a)the nature, gravity and duration of the infringem ent and of its consequences, taking into account the purpose of the AI 
syste m concer ned, as well as, where appropr iate, the number of affected persons and the level of damage suffered by 
them;
(b)the degree of responsibility of the Union institution, body , office or agency , taking into account technical and 
organisational measures imp lemented by them;
(c)any action take n by the Union institution, body , office or agency to mitigat e the damage suffered by affected persons;
(d)the degree of cooperation with the European Data Prot ection Super visor in order to remedy the infringement and 
mitiga te the possible adverse effects of the infringement, including comp liance with any of the measures previously 
ordered by the European Data Prot ection Super visor against the Union institution, body , office or agency concer ned 
with regard to the same subject matt er;
(e)any similar previous infringements by the Union institution, body , office or agency;
(f)the manner in which the infringem ent became kno wn to the European Data Prot ection Super visor , in particular 
whether , and if so to what extent, the Union institution, body , office or agency notif ied the infringem ent;
(g)the annual budg et of the Union institution, body , office or agency.
2. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrative 
fines of up to EUR 1 500 000.
3. The non-com pliance of the AI syste m with any requirements or obligati ons under this Regulation, other than those 
laid down in Article 5, shall be subject to administrative fines of up to EUR 750 000.
4. Before taking decisions pursuant to this Article, the European Data Protect ion Super visor shall give the Union 
institution, body , office or agency which is the subject of the proceedings conducted by the European Data Protection 
Super visor the oppor tunity of being heard on the matt er regard ing the possible infringem ent. The European Data 
Protection Super visor shall base his or her decisions only on elements and circumstances on which the parties concer ned 
have been able to comment. Comp lainants, if any, shall be associated closely with the proceedings.
5. The rights of defence of the parties concer ned shall be fully respected in the proceedings. They shall be entitled to 
have access to the European Data Protection Super visor ’s file, subject to the legitimat e interest of individuals or 
under takings in the protect ion of their personal data or business secrets.
6. Funds collected by imp osition of fines in this Article shall contr ibut e to the general budg et of the Union. The fines 
shall not affect the effective operation of the Union institution, body , office or agency fined.
7. The European Data Prot ection Super visor shall, on an annual basis, notify the Commission of the administrative fines 
it has imp osed pursuant to this Article and of any litigation or judicial proceedings it has initiated.
Article 101
Fines for providers of general-pur pose AI models
1. The Commission may impose on provid ers of general-pur pose AI models fines not exceeding 3 % of their annual total 
worldwide turnove r in the preceding financial year or EUR 15 000 000, whichever is higher ., when the Commission finds 
that the provider intent ionally or negligently:
(a)infringed the relevant provisions of this Regulation;
(b)failed to comply with a request for a document or for information pursuant to Article 91, or supplied incor rect, 
incom plete or misleading information;
(c)failed to comp ly with a measure request ed under Article 93;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 117/144(d)failed to make available to the Commission access to the general-pur pose AI model or general-pur pose AI model with 
syste mic risk with a view to conducting an evaluation pursuant to Article 92.
In fixing the amount of the fine or periodic penalty payment, regard shall be had to the nature, gravity and duration of the 
infringement, taking due account of the principles of propor tionality and appropr iateness. The Commission shall also into 
account commitments made in accordance with Article 93(3) or made in relevant codes of practice in accordance with 
Article 56.
2. Before adop ting the decision pursuant to paragraph 1, the Commission shall communicate its preliminar y findings to 
the provid er of the gene ral-pur pose AI model and give it an oppor tunity to be heard.
3. Fines imposed in accordance with this Article shall be effective, propor tionate and dissuasive.
4. Information on fines imp osed under this Article shall also be communicated to the Board as appropr iate.
5. The Cour t of Justice of the European Union shall have unlimit ed jurisdiction to review decisions of the Commission 
fixing a fine under this Article. It may cancel, reduce or increase the fine imposed.
6. The Commission shall adop t imp lementing acts containing detailed arrang ements and procedural safeguards for 
proceedings in view of the possible adop tion of decisions pursuant to paragraph 1 of this Article. Those imp lementing acts 
shall be adop ted in accordance with the examination procedure refer red to in Article 98(2).
CHAPTER XIII
FINAL PROVISIONS
Article 102
Amendment to Regulation (EC) No 300/2008
In Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added:
‘When adopti ng detailed measures related to technical specif ications and procedures for approva l and use of secur ity 
equipment concer ning Artificial Intellig ence syste ms within the meaning of Regulation (EU) 2024/1689 of the European 
Parliame nt and of the Council (*), the requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into 
account. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’ .
Article 103
Amendment to Regulation (EU) No 167/2013
In Article 17(5) of Regulation (EU) No 167/2013, the follo wing subparagraph is added:
‘When adopting delegat ed acts pursuant to the first subparagraph concer ning artificial intellig ence syste ms which are safety 
compo nents within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the 
requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into account. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’ .EN OJ L, 12.7.2024
118/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojArticle 104
Amendment to Regulation (EU) No 168/2013
In Article 22(5) of Regulation (EU) No 168/2013, the follo wing subparagraph is added:
‘When adopting deleg ated acts pursuant to the first subparagraph concer ning Artificial Intell igence syste ms which are safety 
compo nents within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the 
requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into account. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’ .
Article 105
Amendment to Directiv e 2014/90/EU
In Article 8 of Directive 2014/90/EU, the following paragraph is added:
‘5. For Artificial Intelligence syste ms which are safety comp onents within the meaning of Regulation (EU) 2024/1689 of 
the European Parliament and of the Council (*), when carrying out its activities pursuant to paragraph 1 and when adop ting 
technical specif ications and testing standards in accordance with paragraphs 2 and 3, the Commission shall take into 
account the requirements set out in Chap ter III, Section 2, of that Regulation. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’ .
Article 106
Amendment to Directiv e (EU) 2016/797
In Article 5 of Directive (EU) 2016/797, the follo wing paragraph is added:
‘12. When adop ting delegat ed acts pursuant to paragraph 1 and implementing acts pursuant to paragraph 11 
concer ning Artificial Intellig ence systems which are safety components within the meaning of Regulation (EU) 2024/1689 
of the European Parliament and of the Council (*), the requirements set out in Chapt er III, Section 2, of that Regulation shall 
be take n into account. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’ .OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 119/144Article 107
Amendment to Regulation (EU) 2018/858
In Article 5 of Regulation (EU) 2018/858 the followi ng paragraph is added:
‘4. When adop ting deleg ated acts pursuant to paragraph 3 concer ning Artificial Intellig ence syste ms which are safety 
compo nents within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the 
requirements set out in Chap ter III, Section 2, of that Regulation shall be taken into account. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’ .
Article 108
Amendments to Regulation (EU) 2018/1139
Regulation (EU) 2018/1139 is amended as follo ws:
(1) in Article 17, the following paragraph is added:
‘3. Without prejudice to paragraph 2, when adop ting implementing acts pursuant to paragraph 1 concer ning 
Artificial Intellig ence systems which are safety comp onents within the meaning of Regulation (EU) 2024/1689 of the 
European Parliame nt and of the Council (*), the requirements set out in Chapt er III, Section 2, of that Regulation shall be 
taken into account. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down 
harmonised rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) 
No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 
and (EU) 2020/1828 (Artificial Intellig ence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa. 
eu/eli/reg/2024/1689/oj).’;
(2) in Article 19, the following paragraph is added:
‘4. When adop ting deleg ated acts pursuant to paragraphs 1 and 2 concer ning Artificial Intel ligence syste ms which 
are safety comp onents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III, 
Section 2, of that Regulation shall be taken into account.’;
(3) in Article 43, the following paragraph is added:
‘4. When adop ting implementing acts pursuant to paragraph 1 concer ning Artificial Intelligence systems which are 
safety compo nents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III, 
Section 2, of that Regulation shall be taken into account.’;
(4) in Article 47, the following paragraph is added:
‘3. When adop ting deleg ated acts pursuant to paragraphs 1 and 2 concer ning Artificial Intel ligence syste ms which 
are safety comp onents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III, 
Section 2, of that Regulation shall be taken into account.’;
(5) in Article 57, the following subparagraph is added:
‘When adop ting those imp lementing acts concer ning Artificial Intelligence syste ms which are safety comp onents within 
the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III, Section 2, of that Regulation shall 
be take n into account.’;EN OJ L, 12.7.2024
120/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(6) in Article 58, the following paragraph is added:
‘3. When adop ting deleg ated acts pursuant to paragraphs 1 and 2 concer ning Artificial Intel ligence syste ms which 
are safety comp onents within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapt er III, 
Section 2, of that Regulation shall be taken into account.’ .
Article 109
Amendment to Regulation (EU) 2019/2144
In Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:
‘3. When adop ting the imp lementing acts pursuant to paragraph 2, concer ning artificial intellig ence systems which are 
safety comp onents within the meaning of Regulation (EU) 2024/1689 of the European Parliame nt and of the Council (*), 
the requirements set out in Chapt er III, Section 2, of that Regulation shall be take n into account. 
(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intellig ence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’ .
Article 110
Amendment to Directiv e (EU) 2020/1828
In Annex I to Directive (EU) 2020/1828 of the European Parliament and of the Council (58), the following point is added:
‘(68) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonised 
rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, 
(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 
2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: http://data.europa.eu/eli/reg/ 
2024/1689/oj).’.
Article 111
AI systems already placed on the mark et or put into service and general-pur pose AI models already placed on the 
marked
1. Without prejudice to the application of Article 5 as referred to in Article 113(3), point (a), AI syste ms which are 
compo nents of the large-scale IT systems established by the legal acts listed in Annex X that have been placed on the marke t 
or put into service before 2 Augu st 2027 shall be brought into compliance with this Regulation by 31 December 2030.
The requirements laid down in this Regulation shall be taken into account in the evaluation of each large-scale IT system 
established by the legal acts listed in Annex X to be under take n as provid ed for in those lega l acts and where those lega l acts 
are replaced or amended.
2. Without prejudice to the application of Article 5 as refer red to in Article 113(3), point (a), this Regulation shall apply 
to operators of high-r isk AI systems, other than the syste ms referred to in paragraph 1 of this Article, that have been placed 
on the mark et or put into service before 2 August 2026, only if, as from that date, those syste ms are subject to significant 
changes in their designs. In any case, the providers and deplo yers of high-r isk AI systems intended to be used by public 
author ities shall take the necessar y steps to comply with the requirements and obliga tions of this Regulation by 2 August 
2030.
3. Providers of general-purpo se AI models that have been placed on the marke t before 2 August 2025 shall take the 
necessar y steps in order to comp ly with the obliga tions laid down in this Regulation by 2 Augu st 2027.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 121/144(58) Directive (EU) 2020/1828 of the European Parliament and of the Council of 25 November 2020 on representative actions for the 
prot ection of the collective interests of consumers and repealing Directive 2009/22/EC (OJ L 409, 4.12.2020, p. 1).Article 112
Evaluation and review
1. The Commission shall assess the need for amendment of the list set out in Annex III and of the list of prohibite d AI 
practices laid down in Article 5, once a year following the entr y into force of this Regulation, and until the end of the period 
of the deleg ation of power laid down in Article 97. The Commission shall submit the findi ngs of that assessment to the 
European Parliament and the Council.
2. By 2 August 2028 and ever y four years thereaf ter, the Commission shall evaluate and repor t to the European 
Parliame nt and to the Council on the following:
(a)the need for amendments extending existing area headings or adding new area headings in Annex III;
(b)amendments to the list of AI systems requir ing additional transparency measures in Article 50;
(c)amendments enhancing the effectiveness of the super vision and gove rnance system.
3. By 2 August 2029 and ever y four years thereaf ter, the Commission shall submit a repor t on the evaluation and review 
of this Regulation to the European Parliament and to the Council. The repor t shall include an assessment with regard to the 
structure of enforcement and the possible need for a Union agency to resolve any identified shor tcomings. On the basis of 
the find ings, that repor t shall, where appropr iate, be accom panied by a proposal for amendment of this Regulation. The 
repor ts shall be made public.
4. The repor ts refer red to in paragraph 2 shall pay specif ic attention to the following:
(a)the status of the financial, technical and human resources of the national comp etent author ities in order to effectively 
perfo rm the tasks assigned to them under this Regulation;
(b)the state of penalties, in particular administrative fines as refer red to in Article 99(1), applied by Member States for 
infringements of this Regulation;
(c)adop ted harmonised standards and common specifications developed to suppor t this Regulation;
(d)the number of under takings that enter the market after the entr y into application of this Regulation, and how many of 
them are SMEs.
5. By 2 August 2028, the Commission shall evaluate the functioning of the AI Offi ce, whether the AI Office has been 
given sufficient powers and comp etences to fulfil its tasks, and whether it would be relevant and needed for the proper 
imp lementation and enforcement of this Regulation to upgrade the AI Offi ce and its enforcement compet ences and to 
increase its resources. The Commission shall submit a repor t on its evaluation to the European Parliame nt and to the 
Council.
6. By 2 Augu st 2028 and ever y four years thereaf ter, the Commission shall submit a repor t on the review of the progress 
on the development of standardisation deliverables on the energy-eff icient development of general-pur pose AI models, and 
asses the need for further measures or actions, including binding measures or actions. The repor t shall be submitted to the 
European Parliament and to the Council, and it shall be made public.
7. By 2 August 2028 and ever y three years thereaf ter, the Commission shall evaluate the imp act and effectiveness of 
voluntar y codes of conduct to foste r the application of the requirements set out in Chapt er III, Section 2 for AI syste ms 
other than high-r isk AI syste ms and possibly other additional requirements for AI syste ms other than high-r isk AI systems, 
including as regards environmental sustainability .
8. For the purposes of paragraphs 1 to 7, the Board, the Member States and national compet ent author ities shall provide 
the Commission with information upon its request and without undue dela y.
9. In carrying out the evaluations and reviews refer red to in paragraphs 1 to 7, the Commission shall take into account 
the positions and find ings of the Board, of the European Parliame nt, of the Council, and of other relevant bodies or sources.EN OJ L, 12.7.2024
122/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj10. The Commission shall, if necessar y, submit appropr iate proposals to amend this Regulation, in particular taking into 
account developments in technology , the effect of AI syste ms on health and safety , and on fundamental rights, and in light 
of the state of progress in the information society .
11. To guide the evaluations and reviews referred to in paragraphs 1 to 7 of this Article, the AI Office shall under take to 
develop an objective and participative methodology for the evaluation of risk levels based on the criteria outlined in the 
relevant Articles and the inclusion of new syste ms in:
(a)the list set out in Annex III, including the extension of existing area headings or the addition of new area headings in 
that Annex;
(b)the list of prohibited practices set out in Article 5; and
(c)the list of AI syste ms requir ing additional transparency measures pursuant to Article 50.
12. Any amendment to this Regulation pursuant to paragraph 10, or relevant deleg ated or imp lementing acts, which 
concer ns sectoral Union harmonisation legislation listed in Section B of Annex I shall take into account the regulator y 
specific ities of each secto r, and the existing gover nance, conf ormity assessment and enforcement mechanisms and 
author ities established therein.
13. By 2 August 2031, the Commission shall carry out an assessment of the enforcement of this Regulation and shall 
repor t on it to the European Parliament, the Council and the European Economic and Social Committee, taking into 
account the first years of application of this Regulation. On the basis of the findings, that repor t shall, where appropr iate, be 
accom panied by a proposal for amendment of this Regulation with regard to the structure of enforcement and the need for 
a Union agency to resolve any identifie d shor tcomings.
Article 113
Entr y into force and application
This Regulation shall enter into force on the twentieth day following that of its publication in the Official Journal of the 
Europe an Union.
It shall apply from 2 August 2026.
How ever:
(a)Chapt ers I and II shall apply from 2 Febr uary 2025;
(b)Chapt er III Section 4, Chap ter V, Chapt er VII and Chap ter XII and Article 78 shall apply from 2 Augu st 2025, with the 
exception of Article 101;
(c)Article 6(1) and the corresponding obligations in this Regulation shall apply from 2 August 2027.
This Regulation shall be binding in its entirety and directly applicable in all Member States.
Done at Brussels, 13 June 2024.
For the European Parliament
The President
R. METSOL AFor the Council
The President
M. MICHELOJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 123/144ANNEX I
List of Union harmonisation legislation
Section A. List of Union harmonisation legislation based on the New Legislative Framewo rk
1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machi nery, and amending 
Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);
2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJ 
L 170, 30.6.2009, p. 1);
3. Directive 2013/53/EU of the European Parliame nt and of the Council of 20 November 2013 on recreational craft 
and personal watercraf t and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);
4. Directive 2014/33/EU of the European Parliament and of the Council of 26 Febr uary 2014 on the harmonisation of 
the laws of the Member States relating to lifts and safety comp onents for lifts (OJ L 96, 29.3.2014, p. 251);
5. Directive 2014/34/EU of the European Parliament and of the Council of 26 Febr uary 2014 on the harmonisation of 
the laws of the Member States relating to equipment and protect ive systems intende d for use in pote ntially explosive 
atmospheres (OJ L 96, 29.3.2014, p. 309);
6. Directive 2014/53/EU of the European Parliament and of the Council of 16 Apr il 2014 on the harmonisation of the 
laws of the Member States relating to the making available on the marke t of radio equipment and repealing Directive 
1999/5/EC (OJ L 153, 22.5.2014, p. 62);
7. Directive 2014/68/EU of the European Parliament and of the Council of 15 May 2014 on the harmonisation of the 
laws of the Member States relating to the making available on the marke t of pressure equipment (OJ L 189, 
27.6.2014, p. 164);
8. Regulation (EU) 2016/424 of the European Parliament and of the Council of 9 Marc h 2016 on cablewa y 
installations and repealing Directive 2000/9/EC (OJ L 81, 31.3.2016, p. 1);
9. Regulation (EU) 2016/425 of the European Parliament and of the Council of 9 March 2016 on personal prot ective 
equipment and repealing Council Directive 89/686/EEC (OJ L 81, 31.3.2016, p. 51);
10. Regulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016 on appliances burning 
gaseous fuels and repealing Directive 2009/142/EC (OJ L 81, 31.3.2016, p. 99);
11. Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 Apr il 2017 on medical devices, 
amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing 
Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1);
12. Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 Apr il 2017 on in vitro diagnostic 
medical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, 
p. 176).
Section B. List of other Union harmonisation legislation
13. Regulation (EC) No 300/2008 of the European Parliame nt and of the Council of 11 March 2008 on common rules 
in the field of civil aviation secur ity and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72);
14. Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 Januar y 2013 on the approval 
and marke t surveillance of two- or three-wheel vehicles and quadr icycles (OJ L 60, 2.3.2013, p. 52);
15. Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 Febr uary 2013 on the approval 
and marke t surveillance of agricultural and forestr y vehicles (OJ L 60, 2.3.2013, p. 1);EN OJ L, 12.7.2024
124/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj16. Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on mar ine equipment and 
repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146);
17. Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interop erability of 
the rail syste m within the European Union (OJ L 138, 26.5.2016, p. 44);
18. Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and 
marke t surveillance of motor vehicles and their trailers, and of systems, compo nents and separate technical units 
intende d for such vehicles, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 
2007/46/EC (OJ L 151, 14.6.2018, p. 1);
19. Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-appro val 
requirements for motor vehicles and their trailers, and syste ms, compo nents and separate technical units intended 
for such vehicles, as rega rds their general safety and the prot ection of vehicle occupants and vulnerable road users, 
amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC) 
No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and 
Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010, 
(EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU) 
No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012 
and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1);
20. Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the 
field of civil aviat ion and establishing a European Union Aviation Safety Agency , and amending Regulations (EC) 
No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 
2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC) 
No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91 (OJ L 212, 
22.8.2018, p. 1), in so far as the design, production and placing on the market of aircraf ts referred to in Article 2(1), 
points (a) and (b) thereof, where it concer ns unmanned aircraf t and their engines, propellers, parts and equipment to 
control them remotely , are concer ned.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 125/144ANNEX II
List of criminal offences referred to in Article 5(1), first subparag raph, point (h)(iii)
Criminal offences referred to in Article 5(1), first subparagraph, point (h)(iii):
—terrorism,
—trafficking in human beings,
—sexual exploitation of children, and child pornography ,
—illicit traffi cking in narcotic drugs or psychotropic substances,
—illicit traffi cking in weapons, munitions or explosives,
—murder , grievous bodily injur y,
—illicit trade in human organs or tissue,
—illicit traffi cking in nuclear or radioactive materi als,
—kidnapping, illegal restraint or hostage -taking,
—crimes within the jurisdiction of the International Criminal Cour t,
—unla wful seizure of aircraf t or ships,
—rape,
—environmental crime,
—organised or armed robber y,
—sabotag e,
—participation in a criminal organisation involved in one or more of the offences listed above .EN OJ L, 12.7.2024
126/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX III
High-r isk AI systems referred to in Article 6(2)
High-r isk AI systems pursuant to Article 6(2) are the AI systems listed in any of the follo wing areas:
1. Biometr ics, in so far as their use is permitted under relevant Union or national law:
(a)remote biometr ic identifica tion syste ms.
This shall not include AI systems intende d to be used for biometr ic verification the sole purpose of which is to 
confir m that a specific natural person is the person he or she claims to be;
(b)AI systems intended to be used for biometr ic cate gorisation, according to sensitive or protect ed attributes or 
character istics based on the inference of those attribut es or characteristics ;
(c)AI systems intended to be used for emotion recognition.
2. Critical infrastr ucture: AI systems intended to be used as safety comp onents in the managem ent and operation of 
critical digital infrastr ucture, road traff ic, or in the supply of wate r, gas, heating or electr icity .
3. Education and vocational training:
(a)AI syste ms intended to be used to determine access or admission or to assign natural persons to educational and 
vocational training institutions at all levels;
(b)AI syste ms intended to be used to evaluate learning outcomes, including when those outcomes are used to steer 
the learning process of natural persons in educational and vocational training institutions at all levels;
(c)AI syste ms intended to be used for the purpose of assessing the appropr iate level of education that an individual 
will receive or will be able to access, in the context of or within educational and vocational training institutions 
at all levels;
(d)AI systems intended to be used for monitoring and detecting prohibite d behavi our of students during tests in the 
cont ext of or within educational and vocational training institutions at all levels.
4. Emplo yment, workers’ managem ent and access to self-emplo yment:
(a)AI syste ms intended to be used for the recr uitment or selection of natural persons, in particular to place targe ted 
job adver tisements, to analyse and filter job applications, and to evaluate candidates;
(b)AI systems intended to be used to mak e decisions affecting terms of work -relate d relationships, the promotion or 
termination of work-related contractual relationships, to allocat e tasks based on individual behavi our or personal 
traits or character istics or to monito r and evaluate the perf ormance and behavi our of persons in such 
relationships.
5. Access to and enjo yment of essential private services and essential public services and benefits:
(a)AI syste ms intended to be used by public author ities or on behalf of public author ities to evaluate the eligibility 
of natural persons for essential public assistance benefits and services, including healthcare services, as well as to 
grant, reduce, revok e, or reclaim such benefi ts and services;
(b)AI syste ms intended to be used to evaluate the creditwo rthiness of natural persons or establish their credit score, 
with the exception of AI systems used for the purpose of detect ing financ ial fraud;
(c)AI syste ms intended to be used for risk assessment and pricing in relation to natural persons in the case of life 
and health insurance;OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 127/144(d)AI systems intended to evaluate and classify emerge ncy calls by natural persons or to be used to dispatch , or to 
establish priority in the dispatching of, emergency first response services, including by police, firef ighters and 
medical aid, as well as of emerg ency healthcare patient triage systems.
6. Law enforcement, in so far as their use is permitted under relevant Union or national law:
(a)AI syste ms intended to be used by or on behalf of law enforcement author ities, or by Union institutions, bodies, 
offices or agencies in suppor t of law enforcement author ities or on their behalf to assess the risk of a natural 
person becoming the victim of criminal offences;
(b)AI syste ms intended to be used by or on behalf of law enforcement author ities or by Union institutions, bodies, 
offices or agencies in suppor t of law enforcement author ities as polygraphs or similar tools;
(c)AI syste ms intended to be used by or on behalf of law enforcement author ities, or by Union institutions, bodies, 
offices or agencies, in suppor t of law enforcement author ities to evaluate the reliability of evidence in the course 
of the invest igation or prosecution of criminal offences;
(d)AI syste ms intende d to be used by law enforcement author ities or on their behalf or by Union institutions, 
bodies, offices or agencies in suppor t of law enforcement author ities for assessing the risk of a natural person 
offending or re-offending not solely on the basis of the profil ing of natural persons as referred to in Article 3(4) 
of Directive (EU) 2016/680, or to assess personality traits and character istics or past criminal behavi our of 
natural persons or groups;
(e)AI syste ms intended to be used by or on behalf of law enforcement author ities or by Union institutions, bodies, 
offices or agencies in suppor t of law enforcement author ities for the profil ing of natural persons as refer red to in 
Article 3(4) of Directive (EU) 2016/680 in the course of the detection, invest igation or prosecution of criminal 
offenc es.
7. Migration, asylum and border control management, in so far as their use is permitte d under relevant Union or 
national law:
(a)AI systems intended to be used by or on behalf of compet ent public author ities or by Union institutions, bodies, 
offices or agencies as polygraphs or similar tools;
(b)AI systems intended to be used by or on behalf of compet ent public author ities or by Union institutions, bodies, 
offices or agencies to assess a risk, including a secur ity risk, a risk of irregular migration, or a health risk, posed 
by a natural person who intends to enter or who has entered into the territory of a Member State;
(c)AI systems intended to be used by or on behalf of compet ent public author ities or by Union institutions, bodies, 
offices or agencies to assist compet ent public author ities for the examination of applications for asylum, visa or 
residence permits and for associate d complaints with rega rd to the eligibility of the natural persons applying for 
a status, including relate d assessments of the reliability of evidence;
(d)AI systems intended to be used by or on behalf of comp etent public author ities, or by Union institutions, bodies, 
offices or agencies, in the cont ext of migration, asylum or border control management, for the purpose of 
detect ing, recognising or identifying natural persons, with the exception of the verificati on of trave l documents.
8. Administration of justice and democratic processes:
(a)AI syste ms intended to be used by a judicial author ity or on their behalf to assist a judicial author ity in 
researchi ng and inter preting facts and the law and in applying the law to a concrete set of facts, or to be used in 
a similar way in alternative dispute resolution;EN OJ L, 12.7.2024
128/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(b)AI syste ms intended to be used for influencing the outcome of an election or refere ndum or the voting 
behavio ur of natural persons in the exercise of their vote in elections or referenda. This does not include AI 
syste ms to the output of which natural persons are not directly exposed, such as tools used to organise, opti mise 
or structure political camp aigns from an administrative or logistical point of view .OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 129/144ANNEX IV
Technical document ation referred to in Article 11(1)
The technical documentation referred to in Article 11(1) shall contain at least the following information, as applicable to 
the relevant AI syste m:
1. A general descr iption of the AI syste m including:
(a)its intended purpose, the name of the provider and the version of the syste m reflecting its relation to previous 
versions;
(b)how the AI system interacts with, or can be used to interac t with, hardware or software, including with other AI 
syste ms, that are not part of the AI syste m itself, where applicable;
(c)the versions of relevant software or firmware, and any requirements related to version update s;
(d)the descr iption of all the forms in which the AI system is placed on the marke t or put into service, such as 
software pack ages embedded into hardware, downloads, or APIs;
(e)the descr iption of the hardware on which the AI system is intended to run;
(f)where the AI system is a comp onent of products, photographs or illustrations showing exte rnal features, the 
marking and internal layout of those products;
(g)a basic descr iption of the user -interface provided to the deplo yer;
(h)instr uctions for use for the deplo yer, and a basic descr iption of the user -inte rface provided to the deplo yer, where 
applicable;
2. A detailed descr iption of the elements of the AI syste m and of the process for its development, including:
(a)the methods and steps perfo rmed for the development of the AI syste m, including, where relevant, recourse to 
pre-trained systems or tools provided by third parties and how those were used, integrat ed or modifi ed by the 
provid er;
(b)the design specif ications of the syste m, namely the general logic of the AI system and of the algor ithms; the key 
design choices including the rationale and assump tions made, including with regard to persons or groups of 
persons in respect of who, the system is intended to be used; the main classific ation choices; what the syste m is 
designed to optimise for, and the relevance of the different parameters; the descr iption of the expected output 
and output quality of the system; the decisions about any possible trade-off made regard ing the technical 
solutions adopt ed to comply with the requirements set out in Chapt er III, Section 2;
(c)the descr iption of the system arch itecture explaining how software comp onents build on or feed into each other 
and integrat e into the overall processing; the comp utational resources used to develop, train, test and validat e the 
AI system;
(d)where relevant, the data requirements in terms of datasheets descr ibing the training methodologies and 
techniques and the training data sets used, including a general descr iption of these data sets, information about 
their prove nance, scope and main character istics; how the data was obtained and selected; labelling procedures 
(e.g. for super vised learning), data cleaning methodologies (e.g. outliers detection);
(e)assessment of the human oversight measures needed in accordance with Article 14, including an assessment of 
the technical measures needed to facilitate the interpretation of the outputs of AI syste ms by the deplo yers, in 
accordance with Article 13(3), point (d);
(f)where applicable, a detailed descr iption of pre-deter mined chang es to the AI system and its perfo rmance, 
together with all the relevant information relate d to the technical solutions adop ted to ensure continuous 
complia nce of the AI system with the relevant requirements set out in Chapt er III, Section 2;
(g)the validation and testing procedures used, including information about the validation and testing data used and 
their main characteristics ; metr ics used to measure accuracy , robustness and compliance with other relevant 
requirements set out in Chap ter III, Section 2, as well as pote ntially discr iminat ory imp acts; test logs and all test 
repor ts dated and signed by the responsible persons, including with regard to pre-determ ined changes as referred 
to under point (f);EN OJ L, 12.7.2024
130/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj(h)cybersecur ity measures put in place;
3. Detailed information about the monitoring, functioning and control of the AI syste m, in particular with regard to: 
its capabilities and limitations in perf ormance, including the degrees of accuracy for specif ic persons or groups of 
persons on which the system is intended to be used and the overall expected level of accuracy in relation to its 
intende d purpose; the foreseeable unintended outcomes and sources of risks to health and safety , fundamental rights 
and discr imination in view of the intended purpose of the AI system; the human oversight measures needed in 
accordance with Article 14, including the technical measures put in place to facilitate the interpretation of the 
outputs of AI syste ms by the deplo yers; specifications on input data, as appropr iate;
4. A descr iption of the appropr iateness of the perf ormance metr ics for the specific AI syste m;
5. A detailed descr iption of the risk managem ent syste m in accordance with Article 9;
6. A descr iption of relevant changes made by the provid er to the syste m through its lifecycle;
7. A list of the harmonised standards applied in full or in part the refere nces of which have been published in the 
Official Journal of the European Union; where no such harmonised standards have been applied, a detailed descr iption 
of the solutions adop ted to meet the requirements set out in Chapt er III, Section 2, including a list of other relevant 
standards and technical specifications applied;
8. A copy of the EU declaration of conf ormity refer red to in Article 47;
9. A detailed descr iption of the system in place to evaluate the AI system perf ormance in the post-mark et phase in 
accordance with Article 72, including the post-mark et monito ring plan refer red to in Article 72(3).OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 131/144ANNEX V
EU declaration of confor mity
The EU declaration of conf ormity refer red to in Article 47, shall contain all of the followi ng information:
1. AI syste m name and type and any additional unambiguous reference allowi ng the identifi cation and traceability of 
the AI syste m;
2. The name and address of the provid er or, where applicable, of their author ised representative;
3. A statem ent that the EU declaration of conf ormity referred to in Article 47 is issued under the sole responsibility of 
the provid er;
4. A statement that the AI syste m is in conf ormity with this Regulation and, if applicable, with any other relevant 
Union law that provides for the issuing of the EU declaration of conf ormity referred to in Article 47;
5. Where an AI syste m involves the processing of personal data, a statement that that AI syste m comp lies with 
Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680;
6. References to any relevant harmonised standards used or any other common specification in relation to which 
conf ormity is declared;
7. Where applicable, the name and identifica tion number of the notified body , a descr iption of the conf ormity 
assessment procedure perf ormed, and identification of the certificate issued;
8. The place and date of issue of the declaration, the name and function of the person who signed it, as well as an 
indication for, or on behalf of whom, that person signed, a signature.EN OJ L, 12.7.2024
132/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX VI
Conform ity assessment procedure based on inter nal control
1. The conf ormity assessment procedure based on internal control is the conf ormity assessment procedure based on 
points 2, 3 and 4.
2. The provider verifies that the established quality management system is in comp liance with the requirements of 
Article 17.
3. The provider examines the information contained in the technical documentation in order to assess the compliance 
of the AI system with the relevant essential requirements set out in Chapt er III, Section 2.
4. The provider also verifies that the design and development process of the AI system and its post-mark et monitoring 
as refer red to in Article 72 is consiste nt with the technical documentation.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 133/144ANNEX VII
Conf ormity based on an assessment of the quality management system and an assessment of the 
technical documentation
1. Introduction
Conf ormity based on an assessment of the quality management syste m and an assessment of the technical 
documentation is the conf ormity assessment procedure based on points 2 to 5.
2. Over view
The approved quality managem ent syste m for the design, development and testing of AI systems pursuant to 
Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5. 
The technical documentation of the AI system shall be examined in accordance with point 4.
3. Quality managem ent syste m
3.1. The application of the provider shall include:
(a)the name and address of the provid er and, if the application is lodged by an author ised representative, also their 
name and address;
(b)the list of AI syste ms covered under the same quality managem ent syste m;
(c)the technical documentation for each AI system covered under the same quality managem ent syste m;
(d)the documentation concer ning the quality managem ent syste m which shall cover all the aspects listed under 
Article 17;
(e)a descr iption of the procedures in place to ensure that the quality management system remains adequate and 
effective;
(f)a written declaration that the same application has not been lodg ed with any other notified body .
3.2. The quality managem ent system shall be assessed by the notified body , which shall determine whether it satisfies the 
requirements referred to in Article 17.
The decision shall be notif ied to the provider or its author ised representative.
The notification shall contain the conclusions of the assessment of the quality management syste m and the reasoned 
assessment decision.
3.3. The quality manag ement syste m as approved shall continue to be implement ed and maintained by the provid er so 
that it remains adequate and efficient.
3.4. Any intended chang e to the approve d quality manag ement system or the list of AI syste ms covered by the latter shall 
be brought to the attention of the notified body by the provider .
The proposed changes shall be examined by the notified body , which shall decide whether the modif ied quality 
managem ent system continues to satisfy the requirements referred to in point 3.2 or whether a reassessment is 
necessar y.
The notif ied body shall notify the provider of its decision. The notif ication shall contain the conclusions of the 
examination of the changes and the reasoned assessment decision.
4. Control of the technical documentation.
4.1. In addition to the application refer red to in point 3, an application with a notif ied body of their choice shall be 
lodg ed by the provider for the assessment of the technical documentation relating to the AI system which the 
provid er intends to place on the market or put into service and which is covered by the quality managem ent system 
referred to under point 3.
4.2. The application shall include:
(a)the name and address of the provider;
(b)a written declaration that the same application has not been lodg ed with any other notified body ;
(c)the technical documentation refer red to in Annex IV.EN OJ L, 12.7.2024
134/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj4.3. The technical documentation shall be examined by the notified body . Where relevant, and limited to what is 
necessar y to fulfil its tasks, the notif ied body shall be granted full access to the training, validation, and testing data 
sets used, including, where appropr iate and subject to secur ity safegua rds, through API or other relevant technical 
means and tools enabling remote access.
4.4. In examining the technical documentation, the notif ied body may require that the provider supply further evidence 
or carry out further tests so as to enable a proper assessment of the conf ormity of the AI syste m with the 
requirements set out in Chapt er III, Section 2. Where the notif ied body is not satisf ied with the tests carried out by 
the provid er, the notif ied body shall itself directly carry out adequat e tests, as appropr iate.
4.5. Where necessar y to assess the conf ormity of the high-r isk AI syste m with the requirements set out in Chapt er III, 
Section 2, after all other reasonable means to verify conf ormity have been exhaust ed and have proven to be 
insuffic ient, and upon a reasoned request, the notif ied body shall also be grante d access to the training and trained 
models of the AI system, including its relevant parameters. Such access shall be subject to existing Union law on the 
protect ion of intellectual proper ty and trade secrets.
4.6. The decision of the notif ied body shall be notif ied to the provider or its author ised representative. The notif ication 
shall contain the conclusions of the assessment of the technical documentation and the reasoned assessment 
decision.
Where the AI system is in conf ormity with the requirements set out in Chapt er III, Section 2, the notif ied body shall 
issue a Union technical documentation assessment certificate. The certificate shall indicate the name and address of 
the provider , the conclusions of the examination, the conditions (if any) for its validity and the data necessar y for the 
identifica tion of the AI syste m.
The certificate and its annexe s shall contain all relevant information to allow the conf ormity of the AI system to be 
evaluated, and to allow for control of the AI system while in use, where applicable.
Where the AI syste m is not in conf ormity with the requirements set out in Chap ter III, Section 2, the notif ied body 
shall refuse to issue a Union technical documentation assessment certificate and shall inform the applicant 
according ly, giving detailed reasons for its refusal.
Where the AI syste m does not meet the requirement relating to the data used to train it, re-training of the AI system 
will be needed prior to the application for a new conf ormity assessment. In this case, the reasoned assessment 
decision of the notif ied body refusing to issue the Union technical documentation assessment certificate shall 
contain specific considerations on the quality data used to train the AI syste m, in particular on the reasons for 
non-comp liance.
4.7. Any chang e to the AI system that could affect the comp liance of the AI system with the requirements or its intended 
purpose shall be assessed by the notified body which issued the Union technical documentation assessment 
certificate. The provider shall inform such notif ied body of its intention to introduce any of the abovementioned 
changes, or if it other wise becomes aware of the occur rence of such changes. The intended changes shall be assessed 
by the notif ied body , which shall decide whether those changes require a new conf ormity assessment in accordance 
with Article 43(4) or whether they could be addressed by means of a supplement to the Union technical 
documentation assessment certificate. In the latter case, the notif ied body shall assess the chang es, notify the 
provid er of its decision and, where the change s are approve d, issue to the provider a supplement to the Union 
technical documentation assessment certificate.
5. Surveillance of the approve d quality managem ent syste m.
5.1. The purpose of the surveillance carried out by the notified body referred to in Point 3 is to mak e sure that the 
provid er duly complie s with the terms and conditions of the approve d quality managem ent syste m.
5.2. For assessment purposes, the provider shall allow the notif ied body to access the premises where the design, 
development, testing of the AI systems is taking place. The provid er shall further share with the notif ied body all 
necessar y information.
5.3. The notified body shall carry out periodic audits to mak e sure that the provid er maintains and applies the quality 
managem ent system and shall provide the provid er with an audit repor t. In the context of those audits, the notif ied 
body may carry out additional tests of the AI syste ms for which a Union technical documentation assessment 
certificate was issued.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 135/144ANNEX VIII
Infor mation to be submitted upon the registration of high-r isk AI systems in accordance with 
Article 49
Section A — Information to be submitted by provid ers of high-r isk AI syste ms in accordance with Article 49(1)
The following information shall be provid ed and thereaf ter kept up to date with rega rd to high-r isk AI systems to be 
register ed in accordance with Article 49(1):
1. The name, address and contact details of the provid er;
2. Where submission of information is carried out by another person on behalf of the provider , the name, address and 
contact details of that person;
3. The name, address and contact details of the author ised representative, where applicable;
4. The AI system trade name and any additional unambiguous reference allowing the identification and traceability of 
the AI syste m;
5. A descr iption of the intended purpose of the AI system and of the comp onents and functions suppor ted through 
this AI system;
6. A basic and concise descr iption of the information used by the syste m (data, inputs) and its operating logic;
7. The status of the AI system (on the mark et, or in service; no longer placed on the market/in service, recalled);
8. The type, number and expir y date of the certificate issued by the notif ied body and the name or identification 
number of that notif ied body , where applicable;
9. A scanned copy of the certificate referred to in point 8, where applicable;
10. Any Member States in which the AI syste m has been placed on the marke t, put into service or made available in the 
Union;
11. A copy of the EU declaration of conf ormity refer red to in Article 47;
12. Electronic instr uctions for use; this information shall not be provid ed for high-r isk AI syste ms in the areas of law 
enforcement or migration, asylum and border control management referred to in Annex III, points 1, 6 and 7;
13. A URL for additional information (optional).
Section B — Information to be submitt ed by provider s of high-r isk AI syste ms in accordance with Article 49(2)
The following information shall be provided and thereaf ter kept up to date with regard to AI syste ms to be regist ered in 
accordance with Article 49(2):
1. The name, address and contact details of the provid er;
2. Where submission of information is carried out by another person on behalf of the provider , the name, address and 
contact details of that person;
3. The name, address and contact details of the author ised representative, where applicable;
4. The AI system trade name and any additional unambiguous reference allowing the identification and traceability of 
the AI syste m;
5. A descr iption of the intended purpose of the AI system;
6. The condition or conditions under Article 6(3)based on which the AI syste m is considered to be not-high-r isk;
7. A shor t summar y of the grounds on which the AI system is considered to be not-high-r isk in application of the 
procedure under Article 6(3);
8. The status of the AI system (on the mark et, or in service; no longer placed on the market/in service, recalled);
9. Any Member States in which the AI syste m has been placed on the marke t, put into service or made available in the 
Union.EN OJ L, 12.7.2024
136/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojSection C — Information to be submitt ed by deplo yers of high-r isk AI systems in accordance with Article 49(3)
The following information shall be provid ed and thereaf ter kept up to date with rega rd to high-r isk AI systems to be 
register ed in accordance with Article 49(3):
1. The name, address and contact details of the deplo yer;
2. The name, address and contact details of the person submitting information on behalf of the deplo yer;
3. The URL of the entr y of the AI syste m in the EU database by its provider;
4. A summar y of the findings of the fundamental rights impact assessment conducted in accordance with Article 27;
5. A summar y of the data protect ion imp act assessment carried out in accordance with Article 35 of Regulation (EU) 
2016/679 or Article 27 of Directive (EU) 2016/680 as specified in Article 26(8) of this Regulation, where 
applicable.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 137/144ANNEX IX
Information to be submitted upon the registration of high-r isk AI systems listed in Annex III in 
relation to testi ng in real world conditions in accordance with Article 60
The following information shall be provid ed and thereaf ter kept up to date with regard to testing in real world conditions to 
be registered in accordance with Article 60:
1. A Union-wide unique sing le identifica tion number of the testing in real world conditions;
2. The name and contact details of the provider or prospective provid er and of the deplo yers involved in the testing in 
real world conditions;
3. A brief descr iption of the AI syste m, its intended purpose, and other information necessar y for the identification of 
the syste m;
4. A summar y of the main charact eristics of the plan for testing in real world conditions;
5. Information on the suspension or termination of the testing in real world conditions.EN OJ L, 12.7.2024
138/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX X
Union legislative acts on large-scale IT systems in the area of Freedom, Secur ity and Justice
1. Schengen Information System
(a)Regulation (EU) 2018/1860 of the European Parliament and of the Council of 28 November 2018 on the use of 
the Schengen Information System for the retur n of illegally staying third-countr y nationals (OJ L 312, 
7.12.2018, p. 1).
(b)Regulation (EU) 2018/1861 of the European Parliament and of the Council of 28 November 2018 on the 
establishment, operation and use of the Schengen Information System (SIS) in the field of border checks, and 
amending the Convention implementing the Scheng en Agreement, and amending and repealing Regulation (EC) 
No 1987/2006 (OJ L 312, 7.12.2018, p. 14).
(c)Regulation (EU) 2018/1862 of the European Parliament and of the Council of 28 November 2018 on the 
establishment, operation and use of the Sche ngen Information Syste m (SIS) in the field of police cooperation and 
judicial cooperation in criminal matt ers, amending and repealing Council Decision 2007/533/JHA, and 
repealing Regulation (EC) No 1986/2006 of the European Parliame nt and of the Council and Commission 
Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56).
2. Visa Information Syst em
(a)Regulation (EU) 2021/1133 of the European Parliament and of the Council of 7 July 2021 amending 
Regulations (EU) No 603/2013, (EU) 2016/794, (EU) 2018/1862, (EU) 2019/816 and (EU) 2019/818 as regard s 
the establishment of the conditions for accessing other EU information syste ms for the purposes of the Visa 
Information Syste m (OJ L 248, 13.7.2021, p. 1).
(b)Regulation (EU) 2021/1134 of the European Parliament and of the Council of 7 July 2021 amending 
Regulations (EC) No 767/2008, (EC) No 810/2009, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 
2018/1860, (EU) 2018/1861, (EU) 2019/817 and (EU) 2019/1896 of the European Parliament and of the 
Council and repealing Council Decisions 2004/512/EC and 2008/633/JHA, for the purpose of reforming the 
Visa Information Syst em (OJ L 248, 13.7.2021, p. 11).
3. Eurodac
Regulation (EU) 2024/1358 of the European Parliament and of the Council of 14 May 2024 on the establishment of 
‘Eurodac’ for the comp arison of biometr ic data in order to effectively apply Regulations (EU) 2024/1315 and (EU) 
2024/1350 of the European Parliament and of the Council and Council Directive 2001/55/EC and to identify 
illegally staying third-countr y nationals and stateless persons and on requests for the compari son with Eurodac data 
by Member States’ law enforcement author ities and Europol for law enforcement purposes, amending Regulations 
(EU) 2018/1240 and (EU) 2019/818 of the European Parliament and of the Council and repealing Regulation (EU) 
No 603/2013 of the European Parliament and of the Council (OJ L, 2024/1358, 22.5.2024, ELI: http://data.europa. 
eu/eli/reg/2024/1358/oj).
4. Entr y/Exit System
Regulation (EU) 2017/2226 of the European Parliament and of the Council of 30 November 2017 establishing an 
Entr y/Exit System (EES) to regist er entr y and exit data and refusal of entr y data of third-countr y nationals crossing 
the exte rnal borders of the Member States and determining the conditions for access to the EES for law enforcement 
purposes, and amending the Conve ntion implementing the Schengen Agreement and Regulations (EC) 
No 767/2008 and (EU) No 1077/2011 (OJ L 327, 9.12.2017, p. 20).
5. European Travel Information and Author isation Syste m
(a)Regulation (EU) 2018/1240 of the European Parliament and of the Council of 12 September 2018 establishing 
a European Travel Information and Author isation Syst em (ETIAS) and amending Regulations (EU) 
No 1077/2011, (EU) No 515/2014, (EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236, 
19.9.2018, p. 1).
(b)Regulation (EU) 2018/1241 of the European Parliame nt and of the Council of 12 Sept ember 2018 amending 
Regulation (EU) 2016/794 for the purpose of establishing a European Trave l Information and Author isation 
Syste m (ETIAS) (OJ L 236, 19.9.2018, p. 72).OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 139/1446. European Criminal Records Information Syst em on third-countr y nationals and stateless persons
Regulation (EU) 2019/816 of the European Parliament and of the Council of 17 Apr il 2019 establishing 
a centralised system for the identifica tion of Member States holding conviction information on third-countr y 
nationals and statele ss persons (ECRIS- TCN) to supplement the European Criminal Records Information Syste m and 
amending Regulation (EU) 2018/1726 (OJ L 135, 22.5.2019, p. 1).
7. Interop erability
(a)Regulation (EU) 2019/817 of the European Parliament and of the Council of 20 May 2019 on establishing 
a framework for interoperability between EU information syste ms in the field of borders and visa and amending 
Regulations (EC) No 767/2008, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 2018/1726 and (EU) 
2018/1861 of the European Parliament and of the Council and Council Decisions 2004/512/EC and 
2008/633/JHA (OJ L 135, 22.5.2019, p. 27).
(b)Regulation (EU) 2019/818 of the European Parliament and of the Council of 20 May 2019 on establishing 
a framework for interoperability between EU information syste ms in the field of police and judicial cooperation, 
asylum and migration and amending Regulations (EU) 2018/1726, (EU) 2018/1862 and (EU) 2019/816 (OJ 
L 135, 22.5.2019, p. 85).EN OJ L, 12.7.2024
140/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX XI
Technical document ation referred to in Article 53(1), point (a) — technical document ation for 
providers of general-pur pose AI models
Section 1
Information to be provided by all providers of general-pur pose AI models
The technical documentation refer red to in Article 53(1), point (a) shall contain at least the following information as 
appropr iate to the size and risk prof ile of the model:
1. A general descr iption of the general-pur pose AI model including:
(a)the tasks that the model is intended to perf orm and the type and nature of AI syste ms in which it can be 
integrat ed;
(b)the accep table use policies applicable;
(c)the date of release and methods of distr ibution;
(d)the arch itecture and number of parameter s;
(e)the modality (e.g. text, imag e) and format of inputs and outputs;
(f)the licence.
2. A detailed descr iption of the elements of the model referred to in point 1, and relevant information of the process 
for the development, including the followi ng elements:
(a)the technical means (e.g. instr uctions of use, infrastr ucture, tools) required for the general-pur pose AI model to 
be integrat ed in AI systems;
(b)the design specific ations of the model and training process, including training methodologies and techniques, 
the key design choices including the rationale and assum ptions made; what the model is designed to optimise for 
and the relevance of the diffe rent parameters, as applicable;
(c)information on the data used for training, testing and validation, where applicable, including the type and 
prove nance of data and curation methodologies (e.g. cleaning, filter ing, etc.), the number of data points, their 
scope and main charact eristics; how the data was obtained and selecte d as well as all other measures to detect the 
unsuitability of data sources and methods to detect identifia ble biases, where applicable;
(d)the computational resources used to train the model (e.g. number of floating point operations), training time, 
and other relevant details related to the training;
(e)kno wn or estimated energy consump tion of the model.
With regard to point (e), where the energy consump tion of the model is unknow n, the energy consump tion may be 
based on information about comp utational resources used.
Section 2
Additional information to be provided by provid ers of general-pur pose AI models with syste mic risk
1. A detailed descr iption of the evaluation strategies, including evaluation results, on the basis of available public 
evaluation prot ocols and tools or other wise of other evaluation methodologies. Evaluation strategies shall include 
evaluation criteria, metr ics and the methodology on the identifica tion of limitations.
2. Where applicable, a detailed descr iption of the measures put in place for the purpose of conducting inter nal and/or 
external adversar ial testing (e.g. red teaming), model adap tations, including alignment and fine-tuning.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 141/1443. Where applicable, a detailed descr iption of the system arch itecture explaining how software com ponents build or 
feed into each other and integrat e into the overa ll processing.EN OJ L, 12.7.2024
142/144 ELI: http://data.europa.eu/eli/reg/2024/1689/ojANNEX XII
Transparency infor mation refe rred to in Article 53(1), point (b) — technical document ation for 
providers of general-pur pose AI models to downstre am providers that integ rate the model into their 
AI system
The information refer red to in Article 53(1), point (b) shall contain at least the following:
1. A general descr iption of the general-pur pose AI model including:
(a)the tasks that the model is intended to perfor m and the type and nature of AI systems into which it can be 
integrat ed;
(b)the accep table use policies applicable;
(c)the date of release and methods of distr ibution;
(d)how the model interac ts, or can be used to interact, with hardware or software that is not part of the model 
itself, where applicable;
(e)the versions of relevant software related to the use of the general-pur pose AI model, where applicable;
(f)the arch itecture and number of parameter s;
(g)the modality (e.g. text, imag e) and format of inputs and outputs;
(h)the licence for the model.
2. A descr iption of the elements of the model and of the process for its development, including:
(a)the technical means (e.g. instr uctions for use, infrastr ucture, tools) required for the general-pur pose AI model to 
be integrat ed into AI syste ms;
(b)the modality (e.g. text, image, etc.) and format of the inputs and outputs and their maximum size (e.g. cont ext 
window length, etc.);
(c)information on the data used for training, testing and validation, where applicable, including the type and 
prove nance of data and curation methodologies.OJ L, 12.7.2024 EN
ELI: http://data.europa.eu/eli/reg/2024/1689/oj 143/144ANNEX XIII
Criter ia for the designation of general-pur pose AI models with systemic risk refe rred to in Article 51
For the purpose of determining that a general-pur pose AI model has capabilities or an imp act equivalent to those set out in 
Article 51(1), point (a), the Commission shall take into account the following criteria:
(a) the number of parameter s of the model;
(b) the quality or size of the data set, for example measured through tokens;
(c) the amount of comp utation used for training the model, measured in floating point operations or indicate d by 
a combination of other variables such as estimated cost of training, estimat ed time required for the training, or 
estimated energy consump tion for the training;
(d) the input and output modalities of the model, such as text to text (large languag e models), text to imag e, 
multi-modality , and the state of the art thresholds for determi ning high-impact capabilities for each modality , and 
the specific type of inputs and outputs (e.g. biological sequences);
(e) the benchmarks and evaluations of capabilities of the model, including consider ing the number of tasks without 
additional training, adap tability to learn new , distinct tasks, its level of autonom y and scalability , the tools it has 
access to;
(f) whether it has a high impact on the internal marke t due to its reac h, which shall be presumed when it has been 
made available to at least 10 000 registered business users established in the Union;
(g) the number of register ed end-users.EN OJ L, 12.7.2024
144/144 ELI: http://data.europa.eu/eli/reg/2024/1689/oj