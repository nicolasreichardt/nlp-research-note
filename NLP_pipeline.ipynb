{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Themes in AI Regulation: A Comparative NLP Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Imports\n",
    "\n",
    "Loading the libraries needed for tokenization, stopword removal, and topic modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.extract_pdf_contents import process_pdfs\n",
    "from utils.clean_texts import clean_EU_legal_text, clean_US_legal_text\n",
    "from utils.TFIDF_model import load_text, compute_tfidf, save_tfidf_values\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to: data/extracted_text/EU_AI_Act_English.txt\n",
      "Extracted text saved to: data/extracted_text/USA_AI_Executive_Order_English.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output directories\n",
    "input_directory = \"data/raw\" \n",
    "output_directory = \"data/extracted_text\" \n",
    "\n",
    "\n",
    "# Call the function to process the PDFs\n",
    "process_pdfs(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Wrangling and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the EU Act\n",
    "\n",
    "# Read the file content and pass it to clean_legal_text\n",
    "with open(\"data/extracted_text/EU_AI_Act_English.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "clean_EU_text = clean_EU_legal_text(text)\n",
    "\n",
    "# Save cleaned text to /data/cleaned_text directory\n",
    "with open(\"data/cleaned_text/EU_AI_Act_English_Cleaned.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(clean_EU_text)\n",
    "\n",
    "\n",
    "# Second, the USA AI Executive Order\n",
    "\n",
    "# Read the file content and pass it to clean_legal_text\n",
    "with open(\"data/extracted_text/USA_AI_Executive_Order_English.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    clean_US_text = clean_US_legal_text(text)\n",
    "\n",
    "# Save cleaned text to /data/cleaned_text directory\n",
    "with open(\"data/cleaned_text/USA_AI_Executive_Order_English_Cleaned.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(clean_US_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the cleaned text files\n",
    "text_file_1 = \"data/cleaned_text/EU_AI_Act_English_Cleaned.txt\"\n",
    "text_file_2 = \"data/cleaned_text/USA_AI_Executive_Order_English_Cleaned.txt\"\n",
    "\n",
    "# Load the cleaned texts\n",
    "text1 = load_text(text_file_1)\n",
    "text2 = load_text(text_file_2)\n",
    "\n",
    "if not spacy.util.is_package(\"en_core_web_sm\"):\n",
    "    silent_spacy_download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\",\n",
    "                                                \"lemmatizer\", \"attibute_ruler\"]) \n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words_en = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Custom tokenization function\n",
    "def custom_tokenizer(text):\n",
    "    tokenized_text = nlp(text)\n",
    "    return [tok.text.strip() for tok in tokenized_text if tok.text.strip() != '' and not tok.is_punct]\n",
    "\n",
    "# Manual tokenization using the custom tokenizer (for inspection purposes)\n",
    "tokens_EU_AI = custom_tokenizer(text1)\n",
    "tokens_US_AI = custom_tokenizer(text2)\n",
    "\n",
    "# Saving the tokens to files for inspection\n",
    "with open(\"data/tokens/EU_AI_Act_Tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in tokens_EU_AI:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "with open(\"data/tokens/USA_AI_Executive_Order_Tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in tokens_US_AI:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "\n",
    "# Load NLTK stopwords\n",
    "stop_words_en = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Add missing tokenized forms\n",
    "additional_stopwords = [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", \n",
    "                        \"could\", \"might\", \"must\", \"n't\", \"need\", \n",
    "                        \"sha\", \"wo\", \"would\"]\n",
    "stop_words_en = list(set(stop_words_en + additional_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Matrix Dimensions EU AI Act: (1, 3842)\n",
      "BoW Matrix Dimensions USA AI Executive Order: (1, 2912)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer Configuration for BoW (required for LDA)\n",
    "tokenizer_bow = CountVectorizer(analyzer=\"word\",\n",
    "                                tokenizer=custom_tokenizer,\n",
    "                                lowercase=True,\n",
    "                                stop_words=stop_words_en)\n",
    "text1_bow = tokenizer_bow.fit_transform([text1])\n",
    "print(f\"BoW Matrix Dimensions EU AI Act: {text1_bow.shape}\")\n",
    "\n",
    "text2_bow = tokenizer_bow.fit_transform([text2])\n",
    "print(f\"BoW Matrix Dimensions USA AI Executive Order: {text2_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF values saved to: data/tfidf_values/tfidf_doc_1.txt\n",
      "TF-IDF values saved to: data/tfidf_values/tfidf_doc_2.txt\n"
     ]
    }
   ],
   "source": [
    "if text1 and text2:\n",
    "    # Compute TF-IDF\n",
    "    feature_names, tfidf_matrix = compute_tfidf([text1, text2])\n",
    "    \n",
    "    # Save TF-IDF tokens\n",
    "    output_directory = \"data/tfidf_values\"\n",
    "    save_tfidf_values(output_directory, feature_names, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'AI': [-0.14866187  0.5967451  -0.05086897 -0.10289579  0.436231   -0.6889783\n",
      "  0.18889795  1.3001897  -0.279441   -0.28250644 -0.21833727 -0.8097837\n",
      "  0.067187    0.025595   -0.10132525 -0.16360286  0.6211519  -0.60986346\n",
      " -0.24158323 -1.1999272   0.53285295  0.03784047  1.3550559  -0.5035113\n",
      "  0.04096508 -0.23053542 -0.6991654   0.11641812 -0.18338339  0.25613323\n",
      "  1.1824374   0.03249398  0.08220202 -0.91154486 -0.35080025  0.61015403\n",
      "  0.16533157 -0.6529913  -0.10011443 -0.69825554  0.4912114  -0.8590844\n",
      " -0.28641987  0.10619823  0.40091848 -0.3081667   0.01335941 -0.2026129\n",
      "  0.46878043  0.3251462   0.46861973 -0.73733926 -0.3925527  -0.26308906\n",
      " -0.59892565  0.17427142  0.43829775  0.22641993 -0.61892223  0.39143327\n",
      " -0.00461397 -0.07669893  0.3320811   0.0182785  -0.42427236  0.8801778\n",
      "  0.46869823  0.997699   -0.9337702   0.82464653 -0.48314092  0.38475496\n",
      "  1.178694   -0.01402633  0.39689547  0.12208439  0.22020152 -0.14543808\n",
      " -0.26899588  0.43343204 -0.3602049  -0.38176307 -0.11224393  1.3270965\n",
      " -0.5601077   0.14767663  0.02319055  0.82816577  0.92894846  0.24600548\n",
      "  0.90406406  0.37323976  0.40764412 -0.39167634  1.2861702   0.29254955\n",
      "  0.08155119 -0.6396875   0.15004633 -0.02271548]\n"
     ]
    }
   ],
   "source": [
    "# Import the Word2VecModel class\n",
    "from utils.word2vec_module import Word2VecModel\n",
    "\n",
    "# Create and train the Word2Vec model\n",
    "embedding_documents = [tokens_EU_AI, tokens_US_AI]\n",
    "word2vec = Word2VecModel(vector_size=100, # size of the embedding vectors\n",
    "                         window=5, # context window size\n",
    "                         min_count=2, # minimum frequency for a word to be included\n",
    "                         workers=4) # number of CPU cores to use\n",
    "\n",
    "word2vec.train(embedding_documents)\n",
    "\n",
    "# Save the trained model\n",
    "word2vec.save(\"data/embeddings/word2vec\")\n",
    "\n",
    "# Example: Get embedding for a specific word\n",
    "get_vector = word2vec.get_vector(\"AI\")\n",
    "print(f\"Vector for 'AI': {get_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Mathematical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Similarity Measurement (Cosine Similiarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'AI': [('to', 0.9998730421066284), ('or', 0.999854564666748), ('including', 0.999853789806366), ('for', 0.9998424053192139), ('in', 0.9998355507850647)]\n",
      "Similarity between 'AI' and 'artificial': 0.9890989065170288\n",
      "Similarity between EU AI Act and USA AI Executive Order: 0.999997615814209\n",
      "Words most similar to EU AI Act: [('and', 0.9999622702598572), ('or', 0.9999439716339111), ('in', 0.9999430775642395), ('a', 0.9999312162399292), ('including', 0.9999299049377441)]\n",
      "\n",
      "'regulation' - 'law' + 'policy' = [('30', 0.9132196307182312), ('European', 0.9121867418289185), ('Parliament', 0.9120994806289673), ('nologies', 0.9118667244911194), ('Council', 0.9118284583091736)]\n",
      "\n",
      "Analogy - 'risk':'assessment' :: 'data':? = [('those', 0.9983376860618591), ('including', 0.9983199238777161), ('related', 0.9983078241348267)]\n",
      "\n",
      "Outlier in ['AI', 'algorithm', 'technology', 'banana']: AI\n",
      "\n",
      "Words closest to centroid of ['AI', 'data', 'model', 'system']: [('AI', 0.9999215006828308), ('to', 0.9999130368232727), ('or', 0.9999090433120728), ('that', 0.9998970627784729), ('including', 0.9998916387557983)]\n",
      "\n",
      "Euclidean distance between documents: 0.0312\n",
      "\n",
      "Words more unique to EU document (sample): []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# WORK IN PROGRESS\n",
    "\n",
    "\n",
    "# Example: Find similar words\n",
    "similar_words = word2vec.most_similar(\"AI\", topn=5)\n",
    "\n",
    "print(f\"Words similar to 'AI': {similar_words}\")\n",
    "\n",
    "# Example: Compute similarity between two words\n",
    "similarity = word2vec.similarity(\"AI\", \"artificial\")\n",
    "\n",
    "print(f\"Similarity between 'AI' and 'artificial': {similarity}\")\n",
    "\n",
    "# Example: Compute similarity between two documents\n",
    "doc1_vector = word2vec.document_vector(tokens_EU_AI)\n",
    "doc2_vector = word2vec.document_vector(tokens_US_AI)\n",
    "doc_similarity = word2vec.cosine_similarity(doc1_vector, doc2_vector)\n",
    "print(f\"Similarity between EU AI Act and USA AI Executive Order: {doc_similarity}\")\n",
    "\n",
    "# Example: Find most similar words to a document\n",
    "most_similar_to_doc = word2vec.most_similar_to_document(tokens_EU_AI, topn=5)\n",
    "print(f\"Words most similar to EU AI Act: {most_similar_to_doc}\")\n",
    "\n",
    "### TESTING \n",
    "# Arithmetic operations with word embeddings\n",
    "# Example: \"regulation\" - \"law\" + \"policy\" = ?\n",
    "try:\n",
    "    result = word2vec.model.wv.most_similar(\n",
    "        positive=[\"regulation\", \"policy\"],\n",
    "        negative=[\"law\"],\n",
    "        topn=5\n",
    "    )\n",
    "    print(f\"\\n'regulation' - 'law' + 'policy' = {result}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Word not found in vocabulary: {e}\")\n",
    "\n",
    "# Analogy: \"risk\" is to \"assessment\" as \"data\" is to ?\n",
    "try:\n",
    "    analogy_result = word2vec.model.wv.most_similar(\n",
    "        positive=[\"data\", \"assessment\"],\n",
    "        negative=[\"risk\"],\n",
    "        topn=3\n",
    "    )\n",
    "    print(f\"\\nAnalogy - 'risk':'assessment' :: 'data':? = {analogy_result}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Word not found in vocabulary: {e}\")\n",
    "\n",
    "# Find outlier word in a list\n",
    "try:\n",
    "    outlier = word2vec.model.wv.doesnt_match([\"AI\", \"algorithm\", \"technology\", \"banana\"])\n",
    "    print(f\"\\nOutlier in ['AI', 'algorithm', 'technology', 'banana']: {outlier}\")\n",
    "except KeyError as e:\n",
    "    print(f\"Word not found in vocabulary: {e}\")\n",
    "\n",
    "# Compute centroid of multiple words (semantic center)\n",
    "concept_words = [\"AI\", \"data\", \"model\", \"system\"]\n",
    "valid_vectors = [word2vec.get_vector(word) for word in concept_words \n",
    "                 if word in word2vec.model.wv]\n",
    "if valid_vectors:\n",
    "    centroid = np.mean(valid_vectors, axis=0)\n",
    "    similar_to_centroid = word2vec.model.wv.similar_by_vector(centroid, topn=5)\n",
    "    print(f\"\\nWords closest to centroid of {concept_words}: {similar_to_centroid}\")\n",
    "\n",
    "# Measure semantic distance between two documents (Euclidean distance)\n",
    "euclidean_dist = np.linalg.norm(doc1_vector - doc2_vector)\n",
    "print(f\"\\nEuclidean distance between documents: {euclidean_dist:.4f}\")\n",
    "\n",
    "# Find words unique to each document (low cosine similarity)\n",
    "eu_unique = []\n",
    "us_unique = []\n",
    "for word in tokens_EU_AI[:100]:  # Sample first 100 tokens\n",
    "    if word in word2vec.model.wv:\n",
    "        sim = word2vec.model.wv.n_similarity([word], tokens_US_AI[:100])\n",
    "        if sim < 0.3:  # Low similarity threshold\n",
    "            eu_unique.append((word, sim))\n",
    "\n",
    "print(f\"\\nWords more unique to EU document (sample): {eu_unique[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Dimensionality Reduction (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t-SNE Visualization\n",
    "# from utils.tsne_visualization import plot_tsne_embeddings\n",
    "# import numpy as np\n",
    "\n",
    "# # Prepare embeddings for visualization\n",
    "# words = list(word2vec.model.wv.index_to_key) # Get all words in the vocabulary\n",
    "# embeddings = np.array([word2vec.get_vector(word) for word in words])  # Convert to NumPy array for t-SNE function to work\n",
    "\n",
    "# # Plot t-SNE embeddings and save\n",
    "# plot_tsne_embeddings(embeddings, words, \"plots/tsne_embeddings.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n",
      "Topic 1: ai, shall, systems, system, regulation, data, article, use, union, including\n",
      "Topic 2: ai, systems, regulation, system, article, shall, eu, union, higherisk, data\n",
      "Topic 3: ai, shall, secretary, use, order, including, appropriate, security, within, days\n",
      "Topic 4: ai, regulation, systems, shall, eu, article, law, system, authorities, union\n",
      "Topic 5: ai, shall, systems, use, system, including, article, regulation, secretary, appropriate\n",
      "\n",
      "Topic distribution for EU AI Act: [4.49848979e-06 9.99981954e-01 4.55218449e-06 4.48661941e-06\n",
      " 4.50873182e-06]\n",
      "Topic distribution for USA AI Executive Order: [1.46590352e-05 1.48643754e-05 9.99941155e-01 1.46203212e-05\n",
      " 1.47016379e-05]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Combine both documents for topic modeling\n",
    "combined_bow = tokenizer_bow.fit_transform([text1, text2])\n",
    "feature_names = tokenizer_bow.get_feature_names_out()\n",
    "\n",
    "# Initialize and fit LDA model\n",
    "n_topics = 5  # Number of topics to extract\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                       max_iter=10,\n",
    "                                       learning_method='online',\n",
    "                                       random_state=42)\n",
    "lda_model.fit(combined_bow)\n",
    "\n",
    "# Display top words for each topic\n",
    "def display_topics(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"LDA Topics:\")\n",
    "display_topics(lda_model, feature_names)\n",
    "\n",
    "# Get topic distribution for each document\n",
    "doc_topics = lda_model.transform(combined_bow)\n",
    "print(f\"\\nTopic distribution for EU AI Act: {doc_topics[0]}\")\n",
    "print(f\"Topic distribution for USA AI Executive Order: {doc_topics[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. LDA with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics (using TF-IDF):\n",
      "Topic 1: deciding, 1560, presence, equivalent, infor-, improvements, societies, considered, inves-, managed\n",
      "Topic 2: ai, shall, systems, including, regulation, secretary, data, article, system, eu\n",
      "Topic 3: wheel, actors, overfitting, states, fitting, infringe, want, facilitating, person, strict\n",
      "Topic 4: helps, 2006, substantiated, navigating, rather, gagements, essary, explain, entrepreneurs, 5.2017\n",
      "Topic 5: obligation, last, illustrations, abusive, december, accelerate, 14111, work-, advisors, appeal\n",
      "\n",
      "Topic distribution for EU AI Act (TF-IDF): [0.01122038 0.95512512 0.01121686 0.01121364 0.011224  ]\n",
      "Topic distribution for USA AI Executive Order (TF-IDF): [0.01032565 0.95871889 0.01031669 0.01031425 0.01032452]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TF-IDF vectorizer with custom tokenizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer,\n",
    "                                    lowercase=True,\n",
    "                                    stop_words=stop_words_en)\n",
    "\n",
    "# Fit and transform the texts\n",
    "tfidf_for_lda = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Initialize and fit LDA model with TF-IDF\n",
    "n_topics_tfidf = 5\n",
    "lda_tfidf_model = LatentDirichletAllocation(n_components=n_topics_tfidf,\n",
    "                                             max_iter=10,\n",
    "                                             learning_method='online',\n",
    "                                             random_state=42)\n",
    "lda_tfidf_model.fit(tfidf_for_lda)\n",
    "\n",
    "# Display top words for each topic\n",
    "print(\"LDA Topics (using TF-IDF):\")\n",
    "display_topics(lda_tfidf_model, tfidf_feature_names)\n",
    "\n",
    "# Get topic distribution for each document\n",
    "doc_topics_tfidf = lda_tfidf_model.transform(tfidf_for_lda)\n",
    "print(f\"\\nTopic distribution for EU AI Act (TF-IDF): {doc_topics_tfidf[0]}\")\n",
    "print(f\"Topic distribution for USA AI Executive Order (TF-IDF): {doc_topics_tfidf[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-research-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
