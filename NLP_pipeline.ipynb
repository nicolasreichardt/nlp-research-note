{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlp/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils.extract_pdf_contents import process_pdfs\n",
    "from utils.clean_texts import clean_EU_legal_text, clean_US_legal_text\n",
    "from utils.tokenize_TFIDF import load_text, compute_tfidf, save_tfidf_values\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "import nltk\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research paper NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract the text from the two pdf documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text saved to: data/extracted_text/EU_AI_Act_English.txt\n",
      "Extracted text saved to: data/extracted_text/USA_AI_Executive_Order_English.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output directories\n",
    "input_directory = \"data/raw\"  # Directory containing the PDF files\n",
    "output_directory = \"data/extracted_text\"  # Directory to save the extracted text files\n",
    "\n",
    "\n",
    "# Call the function to process the PDFs\n",
    "process_pdfs(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleanup the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the EU Act\n",
    "\n",
    "# Read the file content and pass it to clean_legal_text\n",
    "with open(\"data/extracted_text/EU_AI_Act_English.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "clean_EU_text = clean_EU_legal_text(text)\n",
    "\n",
    "# Save cleaned text to /data/cleaned_text directory\n",
    "with open(\"data/cleaned_text/EU_AI_Act_English_Cleaned.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(clean_EU_text)\n",
    "\n",
    "\n",
    "# Second, the USA AI Executive Order\n",
    "\n",
    "# Read the file content and pass it to clean_legal_text\n",
    "with open(\"data/extracted_text/USA_AI_Executive_Order_English.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    clean_US_text = clean_US_legal_text(text)\n",
    "\n",
    "# Save cleaned text to /data/cleaned_text directory\n",
    "with open(\"data/cleaned_text/USA_AI_Executive_Order_English_Cleaned.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(clean_US_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.9)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.11.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/nlp/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nicolasreichardt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Paths to the cleaned text files\n",
    "text_file_1 = \"data/cleaned_text/EU_AI_Act_English_Cleaned.txt\"\n",
    "text_file_2 = \"data/cleaned_text/USA_AI_Executive_Order_English_Cleaned.txt\"\n",
    "\n",
    "# Load the cleaned texts\n",
    "text1 = load_text(text_file_1)\n",
    "text2 = load_text(text_file_2)\n",
    "\n",
    "\n",
    "# JUSTIFY THE CHOICES\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\",\n",
    "                                                \"lemmatizer\", \"attibute_ruler\"]) \n",
    "nltk.download(\"stopwords\")\n",
    "stop_words_en = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Custom tokenization function\n",
    "def custom_tokenizer(text): # wrap tokenizer in custom function\n",
    "    tokenized_text = nlp(text)\n",
    "    # Remove empty tokens\n",
    "    return [tok.text.strip() for tok in tokenized_text if tok.text.strip() !='']\n",
    "\n",
    "# Manual tokenization using the custom tokenizer (for inspection purposes)\n",
    "tokens_EU_AI = custom_tokenizer(text1)\n",
    "tokens_US_AI = custom_tokenizer(text2)\n",
    "\n",
    "# Saving the tokens to files for inspection\n",
    "with open(\"data/tokens/EU_AI_Act_Tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in tokens_EU_AI:\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "with open(\"data/tokens/USA_AI_Executive_Order_Tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in tokens_US_AI:\n",
    "        f.write(f\"{token}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/nlp/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Matrix Dimensions EU AI Act: (1, 4823)\n",
      "BoW Matrix Dimensions USA AI Executive Order: (1, 2933)\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer Configuration for BoW (required for LDA)\n",
    "tokenizer_bow = CountVectorizer(analyzer=\"word\",\n",
    "                                       tokenizer=custom_tokenizer,\n",
    "                                       lowercase=True,\n",
    "                                       stop_words=stop_words_en)\n",
    "                                      # max_df= 0.80, # Ignore terms that appear in more than 80% of the documents\n",
    "                                      # min_df= 0.01) # Ignore terms that appear in less than 1% of the documents\n",
    "text1_bow = tokenizer_bow.fit_transform([text1])\n",
    "print(f\"BoW Matrix Dimensions EU AI Act: {text1_bow.shape}\")\n",
    "\n",
    "text2_bow = tokenizer_bow.fit_transform([text2])\n",
    "print(f\"BoW Matrix Dimensions USA AI Executive Order: {text2_bow.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF values saved to: data/tfidf_values/tfidf_doc_1.txt\n",
      "TF-IDF values saved to: data/tfidf_values/tfidf_doc_2.txt\n"
     ]
    }
   ],
   "source": [
    "if text1 and text2:\n",
    "    # Compute TF-IDF\n",
    "    feature_names, tfidf_matrix = compute_tfidf([text1, text2])\n",
    "    \n",
    "    # Save TF-IDF tokens\n",
    "    output_directory = \"data/tfidf_values\"\n",
    "    save_tfidf_values(output_directory, feature_names, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Embeddings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. (Optional) Text representation through OpenAI contextualised embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Mathematical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Similarity Measurement (Cosine Similiarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Dimensionality Reduction (t-SNE and/or PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Other techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
